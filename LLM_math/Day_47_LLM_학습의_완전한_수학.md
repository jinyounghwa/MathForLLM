# Day 47: LLM í•™ìŠµì˜ ì™„ì „í•œ ìˆ˜í•™ (2ì‹œê°„) â­

## ðŸ“š í•™ìŠµ ëª©í‘œ
- **LLM í•™ìŠµì˜ ì „ì²´ íë¦„ ì™„ë²½ížˆ ì´í•´í•˜ê¸°**
- ëª¨ë“  ìˆ˜í•™ ê°œë…ì˜ í†µí•© íŒŒì•…í•˜ê¸°
- Forwardì™€ Backwardì˜ ì „ì²´ ê³¼ì • ì´í•´í•˜ê¸°
- ì‹¤ì œ í•™ìŠµ ì½”ë“œ êµ¬í˜„í•˜ê¸°

---

## ðŸŽ¯ ê°•ì˜ ì£¼ì œ
**"ëª¨ë“  ìˆ˜í•™ì´ í•˜ë‚˜ë¡œ - LLM í•™ìŠµ"**

---

## ðŸ“– ì „ì²´ íë¦„

### 1. Forward Pass (ì˜ˆì¸¡)

```
ìž…ë ¥ í† í°: [1, 5, 23, 67]
    â†“
1. Embedding
   í† í° â†’ ë²¡í„° (ì„ í˜•ëŒ€ìˆ˜)
   E[token_id] âˆˆ â„^d_model

2. Positional Encoding
   ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
   PE = sin/cos (ì‚¼ê°í•¨ìˆ˜)

3. Transformer Layers (Nê°œ)
   ê° ì¸µ:
   a. Multi-Head Attention
      - Q, K, V íˆ¬ì˜ (í–‰ë ¬ê³±)
      - QK^T (ë‚´ì , ìœ ì‚¬ë„)
      - / âˆšd_k (ì •ê·œí™”)
      - Softmax (í™•ë¥ , ì§€ìˆ˜/ë¡œê·¸)
      - Ã— V (ê°€ì¤‘í•©)

   b. Feed Forward
      - ReLU(xWâ‚)Wâ‚‚ (ë¯¸ë¶„ ê°€ëŠ¥ í™œì„±í™”)

   c. Layer Norm + Residual
      - ì •ê·œí™”, ê¸°ìš¸ê¸° ì „ë‹¬

4. Output Layer
   logits = hidden @ W_out
   P(token|context) = softmax(logits)

5. Loss Calculation
   L = CrossEntropy(ì‹¤ì œ, ì˜ˆì¸¡)
   L = -log P(ì‹¤ì œ_í† í°|context)
   â†’ ì •ë³´ì´ë¡ !
```

---

### 2. Backward Pass (í•™ìŠµ)

```
ì†ì‹¤ Lì—ì„œ ì‹œìž‘
    â†“
1. Output Layer
   dL/dW_out (íŽ¸ë¯¸ë¶„)

2. Transformer Layers (ì—­ìˆœ)
   ê° ì¸µ:
   a. Layer Norm + Residual
      ì—°ì‡„ë²•ì¹™ ì ìš©

   b. Feed Forward
      dL/dWâ‚‚, dL/dWâ‚
      ReLU ë¯¸ë¶„ (x>0: 1, x<0: 0)

   c. Multi-Head Attention
      dL/dW_O, dL/dW_V, dL/dW_K, dL/dW_Q
      Softmax ë¯¸ë¶„, í–‰ë ¬ ì „ì¹˜

3. Embedding
   dL/dE (ê° í† í° ìž„ë² ë”© ì—…ë°ì´íŠ¸)

4. Gradient Descent
   Î¸_new = Î¸_old - Î± Ã— dL/dÎ¸
   â†’ ìµœì í™”!
```

---

### 3. ì‚¬ìš©ëœ ëª¨ë“  ìˆ˜í•™

**ê¸°ì´ˆ (Day 1-10)**:
```
- ë²¡í„°, í–‰ë ¬
- ì§€ìˆ˜, ë¡œê·¸
- í•¨ìˆ˜, ê·¸ëž˜í”„
```

**ì„ í˜•ëŒ€ìˆ˜ (Day 11-20)**:
```
- ë‚´ì : QK^T
- ì •ê·œí™”: v/||v||
- í–‰ë ¬ê³±: ëª¨ë“  ì„ í˜• ì¸µ
- ì „ì¹˜: K^T
```

**ë¯¸ì ë¶„ (Day 21-27)**:
```
- ë¯¸ë¶„: ê¸°ìš¸ê¸° ê³„ì‚°
- ì—°ì‡„ë²•ì¹™: Backpropagation
- íŽ¸ë¯¸ë¶„: íŒŒë¼ë¯¸í„°ë³„ ê¸°ìš¸ê¸°
- ê²½ì‚¬í•˜ê°•ë²•: ìµœì í™”
```

**í™•ë¥ ê³¼ ì •ë³´ì´ë¡  (Day 28-38)**:
```
- í™•ë¥ : Softmax ì¶œë ¥
- Cross Entropy: ì†ì‹¤ í•¨ìˆ˜
- Perplexity: ëª¨ë¸ í‰ê°€
- ì •ë³´ ì´ë“: BPE
```

**LLM í•µì‹¬ (Day 39-46)**:
```
- BPE: í† í°í™”
- Attention: ë¬¸ë§¥ íŒŒì•…
- Transformer: ì „ì²´ ì•„í‚¤í…ì²˜
```

---

## ðŸ’» ì™„ì „í•œ í•™ìŠµ ë£¨í”„ êµ¬í˜„

### ì „ì²´ í•™ìŠµ ì½”ë“œ
```python
import numpy as np

class TinyLM:
    """ì™„ì „í•œ ì–¸ì–´ ëª¨ë¸ (êµìœ¡ìš©)"""

    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers):
        self.vocab_size = vocab_size
        self.d_model = d_model

        # Embedding
        self.embedding = np.random.randn(vocab_size, d_model) * 0.01

        # Positional Encoding
        self.pos_encoding = self._get_positional_encoding(1000, d_model)

        # Transformer Layers (ê°„ì†Œí™”)
        self.layers = []
        for _ in range(num_layers):
            layer = {
                'W_Q': np.random.randn(d_model, d_model) * 0.01,
                'W_K': np.random.randn(d_model, d_model) * 0.01,
                'W_V': np.random.randn(d_model, d_model) * 0.01,
                'W_O': np.random.randn(d_model, d_model) * 0.01,
                'W1': np.random.randn(d_model, d_ff) * 0.01,
                'W2': np.random.randn(d_ff, d_model) * 0.01,
            }
            self.layers.append(layer)

        # Output
        self.W_out = np.random.randn(d_model, vocab_size) * 0.01

    def _get_positional_encoding(self, max_len, d_model):
        PE = np.zeros((max_len, d_model))
        for pos in range(max_len):
            for i in range(0, d_model, 2):
                PE[pos, i] = np.sin(pos / (10000 ** (i / d_model)))
                if i + 1 < d_model:
                    PE[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))
        return PE

    def forward(self, token_ids, store_activations=False):
        """Forward pass"""
        seq_len = len(token_ids)

        # 1. Embedding + Positional
        x = self.embedding[token_ids] + self.pos_encoding[:seq_len]

        activations = {'input': x} if store_activations else None

        # 2. Transformer Layers (ê°„ì†Œí™”ëœ ë²„ì „)
        for i, layer in enumerate(self.layers):
            # Self-Attention (ê°„ì†Œí™”)
            Q = x @ layer['W_Q']
            K = x @ layer['W_K']
            V = x @ layer['W_V']

            scores = (Q @ K.T) / np.sqrt(self.d_model)
            attn = self._softmax(scores)
            attn_out = attn @ V
            attn_out = attn_out @ layer['W_O']

            # Residual (Layer Norm ìƒëžµ)
            x = x + attn_out

            # FFN
            ffn_out = np.maximum(0, x @ layer['W1']) @ layer['W2']
            x = x + ffn_out

            if store_activations:
                activations[f'layer_{i}'] = x

        # 3. Output
        logits = x @ self.W_out

        return logits, activations

    def _softmax(self, x):
        """Softmax (ìˆ˜ì¹˜ ì•ˆì •ì„±)"""
        exp_x = np.exp(x - x.max(axis=-1, keepdims=True))
        return exp_x / exp_x.sum(axis=-1, keepdims=True)

    def compute_loss(self, token_ids, target_ids):
        """Cross Entropy Loss"""
        logits, _ = self.forward(token_ids)

        # Softmax
        probs = self._softmax(logits)

        # Cross Entropy
        loss = 0
        for i, target in enumerate(target_ids):
            loss += -np.log(probs[i, target] + 1e-10)

        return loss / len(target_ids)

    def train_step(self, token_ids, target_ids, learning_rate):
        """í•œ ìŠ¤í… í•™ìŠµ (ê°„ì†Œí™”)"""
        # Forward
        loss_before = self.compute_loss(token_ids, target_ids)

        # Backward (ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ ê·¼ì‚¬)
        # ì‹¤ì œë¡œëŠ” ì—­ì „íŒŒ ì‚¬ìš©
        epsilon = 1e-5

        # Embedding ì—…ë°ì´íŠ¸ (ì˜ˆì‹œ)
        for idx in token_ids:
            grad = np.zeros_like(self.embedding[idx])

            for j in range(self.d_model):
                self.embedding[idx, j] += epsilon
                loss_plus = self.compute_loss(token_ids, target_ids)
                self.embedding[idx, j] -= epsilon

                grad[j] = (loss_plus - loss_before) / epsilon

            self.embedding[idx] -= learning_rate * grad

        # Forward again
        loss_after = self.compute_loss(token_ids, target_ids)

        return loss_before, loss_after


# ì‚¬ìš© ì˜ˆ
print("=== Tiny Language Model í•™ìŠµ ===\n")

# ì´ˆê¸°í™”
vocab_size = 50
d_model = 32
num_heads = 4
d_ff = 64
num_layers = 2

model = TinyLM(vocab_size, d_model, num_heads, d_ff, num_layers)

# í•™ìŠµ ë°ì´í„°
# "The cat sat" â†’ ë‹¤ìŒ í† í° ì˜ˆì¸¡
sequences = [
    ([5, 12, 23], [12, 23, 7]),   # "The cat sat" â†’ "cat sat ."
    ([5, 8, 15], [8, 15, 7]),      # ...
]

print(f"ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
print(f"- Vocab: {vocab_size}")
print(f"- d_model: {d_model}")
print(f"- Layers: {num_layers}\n")

# í•™ìŠµ
epochs = 5
learning_rate = 0.001

print("í•™ìŠµ ì‹œìž‘...\n")

for epoch in range(epochs):
    total_loss = 0

    for token_ids, target_ids in sequences:
        loss, _ = model.train_step(token_ids, target_ids, learning_rate)
        total_loss += loss

    avg_loss = total_loss / len(sequences)

    if epoch % 1 == 0:
        print(f"Epoch {epoch}: Loss = {avg_loss:.4f}")

print("\ní•™ìŠµ ì™„ë£Œ!")

# ì˜ˆì¸¡
print("\n=== ì˜ˆì¸¡ ===\n")
test_input = [5, 12]  # "The cat"
logits, _ = model.forward(test_input)

# ë§ˆì§€ë§‰ í† í°ì˜ ì˜ˆì¸¡
last_probs = model._softmax(logits[-1])
predicted_token = last_probs.argmax()

print(f"ìž…ë ¥: {test_input}")
print(f"ì˜ˆì¸¡ëœ ë‹¤ìŒ í† í°: {predicted_token}")
print(f"í™•ë¥ : {last_probs[predicted_token]:.4f}")
```

---

## ðŸ“Š ìˆ˜í•™ ê°œë… í†µí•© ë§µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LLM í•™ìŠµ ê³¼ì •                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Forward    â”‚  Loss       â”‚  Backward    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“             â†“              â†“
  ì„ í˜•ëŒ€ìˆ˜      ì •ë³´ì´ë¡         ë¯¸ì ë¶„
  - ë‚´ì         - ì—”íŠ¸ë¡œí”¼     - íŽ¸ë¯¸ë¶„
  - í–‰ë ¬ê³±      - Cross Ent   - ì—°ì‡„ë²•ì¹™
  - ì •ê·œí™”      - Perplexity  - ê²½ì‚¬í•˜ê°•ë²•
```

---

## ðŸ”— ì‹¤ì œ LLMê³¼ì˜ ë¹„êµ

### GPT-3
```
- 175B íŒŒë¼ë¯¸í„°
- 96 layers
- d_model = 12288
- num_heads = 96
- Vocab = 50257 (BPE)

í•™ìŠµ:
- ë°°ì¹˜ í¬ê¸°: 3.2M í† í°
- Adam ìµœì í™”
- Learning rate scheduling
- Gradient clipping
```

### ìš°ë¦¬ì˜ Tiny LM
```
- ~10K íŒŒë¼ë¯¸í„°
- 2 layers
- d_model = 32
- num_heads = 4

â†’ ê°™ì€ ì›ë¦¬, ìž‘ì€ ê·œëª¨!
```

---

## âœ… ìµœì¢… ì²´í¬í¬ì¸íŠ¸

- [ ] **Forward passì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ì„¤ëª…í•  ìˆ˜ ìžˆë‚˜ìš”?**

- [ ] **Backward passì™€ ì—°ì‡„ë²•ì¹™ì„ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©ëœ ìˆ˜í•™ì„ ë§í•  ìˆ˜ ìžˆë‚˜ìš”?**

- [ ] **ì†ì‹¤ í•¨ìˆ˜ì—ì„œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ê¹Œì§€ íë¦„ì„ ì•„ë‚˜ìš”?**

- [ ] **ì‹¤ì œ LLMê³¼ì˜ ì°¨ì´ë¥¼ ì´í•´í–ˆë‚˜ìš”?**

---

## ðŸŽ“ í•µì‹¬ ìš”ì•½

**LLM í•™ìŠµ = ëª¨ë“  ìˆ˜í•™ì˜ í†µí•©**

1. **ìž…ë ¥**: í† í° â†’ ìž„ë² ë”© (ì„ í˜•ëŒ€ìˆ˜)
2. **ì²˜ë¦¬**: Attention + FFN (í–‰ë ¬, ë¯¸ë¶„)
3. **ì¶œë ¥**: Softmax (í™•ë¥ )
4. **ì†ì‹¤**: Cross Entropy (ì •ë³´ì´ë¡ )
5. **í•™ìŠµ**: Backprop + GD (ë¯¸ì ë¶„)

**ë‹¹ì‹ ì€ ì´ì œ LLMì˜ ìˆ˜í•™ì„ ì™„ì „ížˆ ì´í•´í–ˆìŠµë‹ˆë‹¤!**

### ë‹¤ìŒ í•™ìŠµ
- **Day 48**: ìµœì¢… í”„ë¡œì íŠ¸
  - Tiny Language Model êµ¬í˜„!

---

**ì¶•í•˜í•©ë‹ˆë‹¤!** ðŸŽ‰

**ëª¨ë“  ìˆ˜í•™ì´ í•˜ë‚˜ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!**
