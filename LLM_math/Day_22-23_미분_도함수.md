# Day 22-23: 미분 (도함수) (2시간)

## 📚 학습 목표
- 미분의 정의 완벽히 이해하기
- 기본 미분 공식 익히기
- 신경망의 활성화 함수 미분하기
- Backpropagation의 기초 다지기

---

## 🎯 강의 주제
**"변화의 속도를 측정하기"**

---

## 📖 핵심 개념

### 1. 미분의 정의

**도함수 (Derivative)**:
```
f'(x) = lim_{h→0} (f(x+h) - f(x)) / h
```

**다른 표기법**:
```
f'(x) = df/dx = dy/dx = Df(x)
```

**의미**:
```
x에서 f의 순간 변화율
그래프에서 접선의 기울기
```

---

### 2. 기본 미분 공식

**상수 함수**:
```
f(x) = c  →  f'(x) = 0
```

**거듭제곱**:
```
f(x) = x^n  →  f'(x) = nx^{n-1}
```

**예시**:
```
f(x) = x²    →  f'(x) = 2x
f(x) = x³    →  f'(x) = 3x²
f(x) = √x    →  f'(x) = 1/(2√x)
f(x) = 1/x   →  f'(x) = -1/x²
```

**지수/로그**:
```
f(x) = e^x   →  f'(x) = e^x
f(x) = ln(x) →  f'(x) = 1/x
```

---

### 3. 미분 규칙

**1. 합의 미분**:
```
(f + g)' = f' + g'
```

**2. 차의 미분**:
```
(f - g)' = f' - g'
```

**3. 상수배**:
```
(cf)' = c f'
```

**4. 곱의 미분 (곱미분법)**:
```
(fg)' = f'g + fg'
```

**5. 몫의 미분**:
```
(f/g)' = (f'g - fg') / g²
```

---

### 4. 신경망 활성화 함수의 미분

**Sigmoid**:
```
σ(x) = 1 / (1 + e^{-x})

σ'(x) = σ(x)(1 - σ(x))

매우 중요한 성질!
```

**증명**:
```
σ(x) = (1 + e^{-x})^{-1}

σ'(x) = -(1 + e^{-x})^{-2} × (-e^{-x})
      = e^{-x} / (1 + e^{-x})²
      = [1/(1 + e^{-x})] × [e^{-x}/(1 + e^{-x})]
      = σ(x) × [1 - σ(x)]
```

**Tanh**:
```
tanh(x) = (e^x - e^{-x}) / (e^x + e^{-x})

tanh'(x) = 1 - tanh²(x)
```

**ReLU**:
```
ReLU(x) = max(0, x)

ReLU'(x) = {1  if x > 0
           {0  if x < 0
           {undefined if x = 0 (실제로는 0 또는 1 사용)
```

---

### 5. 예제: 손 계산

**예제 1**: f(x) = 3x² + 2x - 1
```
f'(x) = 3(2x) + 2(1) - 0
      = 6x + 2
```

**예제 2**: f(x) = x² × e^x
```
곱미분법:
f'(x) = (x²)' × e^x + x² × (e^x)'
      = 2x × e^x + x² × e^x
      = e^x(2x + x²)
```

---

## 💻 Python 실습

### 실습 1: 미분의 정의로 계산
```python
import numpy as np
import matplotlib.pyplot as plt

def f(x):
    """f(x) = x²"""
    return x**2

def numerical_derivative(f, x, h=1e-5):
    """수치 미분"""
    return (f(x + h) - f(x)) / h

# x = 2에서 미분
x = 2

print("=== 미분 계산 (f(x) = x²) ===\n")

# h를 점점 작게
h_values = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]

for h in h_values:
    derivative = (f(x + h) - f(x)) / h
    print(f"h = {h:>8.5f}  →  f'({x}) ≈ {derivative:.10f}")

print(f"\n정확한 값: f'(x) = 2x → f'({x}) = {2*x}")
```

### 실습 2: 활성화 함수와 도함수
```python
import numpy as np
import matplotlib.pyplot as plt

# Sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# Tanh
def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2

# ReLU
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

# 시각화
x = np.linspace(-5, 5, 200)

fig, axes = plt.subplots(3, 2, figsize=(14, 12))

# Sigmoid
axes[0, 0].plot(x, sigmoid(x), 'b-', linewidth=2)
axes[0, 0].set_title('Sigmoid(x)', fontsize=12)
axes[0, 0].grid(True, alpha=0.3)

axes[0, 1].plot(x, sigmoid_derivative(x), 'r-', linewidth=2)
axes[0, 1].set_title("Sigmoid'(x)", fontsize=12)
axes[0, 1].grid(True, alpha=0.3)

# Tanh
axes[1, 0].plot(x, tanh(x), 'b-', linewidth=2)
axes[1, 0].set_title('Tanh(x)', fontsize=12)
axes[1, 0].grid(True, alpha=0.3)

axes[1, 1].plot(x, tanh_derivative(x), 'r-', linewidth=2)
axes[1, 1].set_title("Tanh'(x)", fontsize=12)
axes[1, 1].grid(True, alpha=0.3)

# ReLU
axes[2, 0].plot(x, relu(x), 'b-', linewidth=2)
axes[2, 0].set_title('ReLU(x)', fontsize=12)
axes[2, 0].grid(True, alpha=0.3)

axes[2, 1].plot(x, relu_derivative(x), 'r-', linewidth=2)
axes[2, 1].set_title("ReLU'(x)", fontsize=12)
axes[2, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('activation_derivatives.png', dpi=150)
print("활성화 함수 도함수 시각화 저장 완료!")

# 수치 확인
print("\n=== 활성화 함수 미분 확인 ===\n")
x_test = 1.0

print(f"x = {x_test}")
print()

# Sigmoid
s = sigmoid(x_test)
ds = sigmoid_derivative(x_test)
print(f"Sigmoid({x_test}) = {s:.6f}")
print(f"Sigmoid'({x_test}) = {ds:.6f}")
print(f"σ(x)(1-σ(x)) = {s * (1-s):.6f} ✓")
print()

# Tanh
t = tanh(x_test)
dt = tanh_derivative(x_test)
print(f"Tanh({x_test}) = {t:.6f}")
print(f"Tanh'({x_test}) = {dt:.6f}")
print(f"1 - tanh²(x) = {1 - t**2:.6f} ✓")
```

### 실습 3: 간단한 Backpropagation
```python
import numpy as np

# 단순한 신경망: x → σ(wx + b) → y
# 손실: L = (y - target)²

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# 초기값
x = 2.0
w = 0.5
b = 0.1
target = 0.9

print("=== 간단한 Backpropagation ===\n")
print(f"입력: x = {x}")
print(f"가중치: w = {w}, b = {b}")
print(f"목표: target = {target}\n")

# Forward pass
z = w * x + b
y = sigmoid(z)
loss = (y - target)**2

print("Forward pass:")
print(f"  z = wx + b = {z:.4f}")
print(f"  y = σ(z) = {y:.4f}")
print(f"  loss = (y - target)² = {loss:.4f}\n")

# Backward pass (Chain rule!)
# dL/dw = dL/dy × dy/dz × dz/dw

dL_dy = 2 * (y - target)          # 손실에 대한 y의 미분
dy_dz = sigmoid_derivative(z)      # σ'(z)
dz_dw = x                          # z에 대한 w의 미분

dL_dw = dL_dy * dy_dz * dz_dw     # Chain rule!

dz_db = 1
dL_db = dL_dy * dy_dz * dz_db

print("Backward pass (Chain rule):")
print(f"  dL/dy = 2(y - target) = {dL_dy:.4f}")
print(f"  dy/dz = σ'(z) = {dy_dz:.4f}")
print(f"  dz/dw = x = {dz_dw:.4f}")
print(f"  dL/dw = {dL_dw:.4f}")
print(f"  dL/db = {dL_db:.4f}\n")

# Gradient descent
learning_rate = 0.1
w_new = w - learning_rate * dL_dw
b_new = b - learning_rate * dL_db

print(f"Gradient descent (lr={learning_rate}):")
print(f"  w: {w:.4f} → {w_new:.4f}")
print(f"  b: {b:.4f} → {b_new:.4f}\n")

# 새로운 손실
z_new = w_new * x + b_new
y_new = sigmoid(z_new)
loss_new = (y_new - target)**2

print(f"새로운 손실: {loss:.4f} → {loss_new:.4f}")
print(f"개선: {loss - loss_new:.4f} ✓")
```

---

## ✍️ 손 계산 연습

### 연습 1: 기본 미분
```
f(x) = x³

f'(x) = 3x²

x = 2:  f'(2) = 3(2²) = 12
```

### 연습 2: 합의 미분
```
f(x) = x² + 3x + 5

f'(x) = 2x + 3 + 0 = 2x + 3
```

### 연습 3: 곱미분법
```
f(x) = x² × e^x

f'(x) = (x²)' × e^x + x² × (e^x)'
      = 2x × e^x + x² × e^x
      = e^x(2x + x²)
```

### 연습 4: Sigmoid 미분
```
x = 0:
σ(0) = 1/(1+1) = 0.5
σ'(0) = 0.5 × (1-0.5) = 0.25
```

---

## 🔗 LLM 연결점

### 1. Backpropagation
```
모든 층의 미분을 연쇄법칙으로 계산:
dL/dw₁ = dL/dw₂ × dw₂/dw₁ × ...

미분 = Backpropagation의 핵심!
```

### 2. 활성화 함수 선택
```
Sigmoid: 기울기 소실 (gradient vanishing)
ReLU: 죽은 뉴런 (dying ReLU)
GELU: LLM에서 선호

미분 성질이 학습 성능 결정!
```

### 3. 최적화
```
경사하강법:
θ_new = θ_old - α × dL/dθ

미분으로 방향 결정!
```

---

## ✅ 체크포인트

- [ ] **미분의 정의를 설명할 수 있나요?**

- [ ] **기본 미분 공식을 사용할 수 있나요?**

- [ ] **Sigmoid의 미분을 유도할 수 있나요?**

- [ ] **Backpropagation에서 미분의 역할을 이해했나요?**

---

## 🎓 핵심 요약

1. **미분**: f'(x) = lim_{h→0} (f(x+h)-f(x))/h
2. **거듭제곱**: (x^n)' = nx^{n-1}
3. **Sigmoid**: σ'(x) = σ(x)(1-σ(x))
4. **ReLU**: ReLU'(x) = {1 if x>0, 0 otherwise}
5. **Backprop**: 연쇄법칙으로 기울기 계산

### 다음 학습
- **Day 24**: 연쇄법칙 (Chain Rule)
  - Backpropagation의 수학적 기초

---

**수고하셨습니다!** 🎉

**미분은 신경망 학습의 심장입니다!**
