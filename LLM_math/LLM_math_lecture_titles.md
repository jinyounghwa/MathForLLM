# LLM 개발을 위한 수학 강의 - 전체 강의 제목 목록

## 📋 강의 구성

### 기초 다지기 (1-7주차)

1. **Day 1: 수의 체계** (1시간)
   - 수의 진화와 각 단계에서 해결된 문제 이해
   - 스칼라와 벡터의 차이 명확히 파악

2. **Day 2-3: 지수와 로그** (3시간)
   - 지수와 로그의 본질적 관계
   - 로그를 이용한 정보량 측정
   - Log-Softmax의 수치적 안정성

3. **Day 4: 함수와 그래프** (1시간)
   - 함수의 본질: 입력 → 계산 → 출력
   - 주요 함수들의 특징과 합성함수

4. **Day 5: 집합과 논리** (1시간)
   - 집합의 기본 개념과 연산
   - 논리 연산과 집합 연산의 관계
   - 조건부 확률의 기초

5. **Day 6-7: 변수, 방정식, 연립방정식** (3시간)
   - 변수와 방정식의 본질
   - 일차방정식과 이차방정식 풀이
   - 연립방정식의 행렬 표현 (Ax = b)

---

### 벡터와 선형대수 (8-11주차)

6. **Day 8-10: 좌표평면과 벡터 기초** (3시간)
   - 2D 좌표계 이해와 시각화
   - 벡터의 더하기와 스칼라배
   - 벡터 연산의 기하학적 의미

7. **Day 11: 벡터의 길이와 거리** (1시간)
   - L2 노름과 피타고라스 정리
   - 거리 개념의 정량화

8. **Day 12: 내적** (1.5시간)
   - 두 벡터의 관계를 수치로 표현
   - Attention 메커니즘의 기초
   - 코사인 유사도 (cosine similarity)

9. **Day 13: 정규화** (1.5시간)
   - 벡터를 길이 1로 만들기
   - 코사인 유사도를 이용한 임베딩 검색
   - RAG 시스템의 원리

10. **Day 14: 행렬과 행렬곱셈** (1.5시간)
    - 벡터 여러 개를 한 번에 다루기
    - 행렬 곱셈의 규칙과 의미
    - 신경망의 기초: y = Wx + b

11. **Day 15: 중간 복습** (1시간)
    - 벡터와 선형대수의 모든 개념 종합

12. **Day 16: 전치행렬** (1시간)
    - 행과 열을 바꾸기
    - 전치의 성질과 응용

13. **Day 17: 역행렬과 단위행렬** (1시간)
    - 행렬의 곱셈 역원
    - 연립방정식 풀이의 다른 방법

14. **Day 18: 고유값과 고유벡터** (1.5시간)
    - 행렬의 본질적 성질
    - 선형변환의 축과 배수

15. **Day 19: 행렬식과 노름** (1시간)
    - 행렬식의 기하학적 의미
    - 다양한 노름의 개념

16. **Day 20: 선형대수 최종 프로젝트** (2시간)
    - 임베딩 공간 분석 프로젝트

---

### 미적분 (12-13주차)

17. **Day 21: 극한과 연속성** (1시간)
    - 함수의 극한 개념 이해
    - 연속성의 정의와 의미

18. **Day 22-23: 미분과 도함수** (2시간)
    - 변화의 속도 측정하기
    - 기본 미분 규칙
    - Backpropagation의 기초

19. **Day 24: 연쇄법칙** (1.5시간)
    - 합성함수의 미분
    - 신경망의 역전파 (Backpropagation)

20. **Day 25: 편미분과 기울기** (1.5시간)
    - 여러 변수일 때의 미분
    - 기울기 벡터와 방향 미분

21. **Day 26: 경사하강법** (1.5시간)
    - 기울기를 따라 최솟값 찾기
    - 학습률의 중요성
    - 모델 최적화의 원리

22. **Day 27: 미적분 종합 복습** (1시간)
    - 모든 미분 개념의 통합

---

### 확률과 정보이론 (14-16주차)

23. **Day 28: 확률론 기초** (1시간)
    - 불확실성의 정량화
    - 기본 확률 개념과 조건부 확률
    - 확률적 독립성

24. **Day 29: 베이즈 정리** (1.5시간)
    - 사건의 연결 관계
    - 사전확률, 가능도, 사후확률
    - LLM의 토큰 예측 원리

25. **Day 30: 확률분포** (1.5시간)
    - 확률이 어떻게 분포하는가?
    - 정규분포 (Normal Distribution)
    - 임베딩 초기화의 원리

26. **Day 31: 중간 복습 확률** (1시간)
    - 확률 개념의 종합 정리

27. **Day 32-33: 엔트로피** (2시간)
    - 불확실성의 정량화
    - 엔트로피 계산과 해석
    - Perplexity = 2^H

28. **Day 34: 상호정보량** (1.5시간)
    - 두 변수의 연관성 측정
    - 정보 이득 개념
    - 변수 선택의 원리

29. **Day 35: 정보 이득과 의사결정** (2시간)
    - 최선의 결정 내리기
    - 의사결정 트리와 정보 이득
    - BPE 토크나이저의 원리

30. **Day 36: 정보이론 종합** (2시간)
    - 엔트로피, 상호정보량, 정보이득의 통일

31. **Day 37-38: 확률과 정보이론 최종 복습** (2시간)
    - 기초, 선형대수, 미적분, 확률의 통합

---

### LLM 핵심 수학 (17-20주차)

32. **Day 39: BPE 알고리즘** (2시간)
    - 텍스트를 효율적으로 토큰화하기
    - 자주 나오는 바이트 쌍 병합
    - 정보 이득 최대화를 통한 압축

33. **Day 40: WordPiece와 SentencePiece** (1.5시간)
    - BPE의 변형들
    - 확률 기반 접근법
    - 다국어 지원 방식

34. **Day 41: 한국어 토크나이저** (1.5시간)
    - 한국어 특화 토크나이저
    - 음절 vs 자모 vs 형태소
    - 엔트로피 기반 어휘 크기 결정

35. **Day 42: 토크나이저 프로젝트** (2시간)
    - 간단한 BPE 토크나이저 구현

36. **Day 43: Scaled Dot-Product Attention** (2시간)
    - 단어들의 관계 찾기
    - Query, Key, Value의 의미
    - Temperature scaling (√d_k)

37. **Day 44: Multi-Head Attention** (1.5시간)
    - 여러 관점에서 동시에 보기
    - 병렬 어텐션 헤드
    - 부분공간에서의 패턴 학습

38. **Day 45-46: Transformer 아키텍처** (2시간)
    - LLM의 기본 설계도
    - 인코더와 디코더
    - Positional Encoding
    - Residual Connection과 Layer Normalization

39. **Day 47: LLM 학습의 완전한 수학** (2시간)
    - 모든 개념의 통합
    - 토크나이저 → Embedding → Attention → 출력
    - 학습 과정의 전체 흐름

40. **Day 48: 최종 프로젝트** (3시간)
    - Tiny Language Model (NumPy 기반 구현)
    - 전체 학습 내용의 실전 적용

---

## 📊 학습 요약

| 단계 | 기간 | 강의 수 | 핵심 목표 |
|------|------|--------|---------|
| 기초 다지기 | 1-7주 | 7개 | 수, 함수, 방정식의 이해 |
| 선형대수 | 8-11주 | 14개 | 벡터, 행렬, 고유값 완벽 이해 |
| 미적분 | 12-13주 | 6개 | 미분과 최적화 개념 |
| 확률과 정보이론 | 14-16주 | 9개 | 엔트로피와 정보 이득 |
| LLM 수학 | 17-20주 | 9개 | 토크나이저, Attention, Transformer |

**총 강의: 48개** | **총 기간: 약 4-5개월**

---

## 🎯 각 단계별 성취 목표

### 기초 다지기 후
- 로그와 지수를 능숙하게 다룰 수 있다
- 함수를 손으로 그릴 수 있다
- 연립방정식을 여러 방법으로 풀 수 있다

### 선형대수 후
- 내적의 기하학적 의미를 설명할 수 있다
- 정규화의 목적을 이해할 수 있다
- 코사인 유사도로 RAG를 이해할 수 있다

### 미적분 후
- 연쇄법칙을 손으로 계산할 수 있다
- Backpropagation의 원리를 설명할 수 있다
- 경사하강법으로 함수의 최솟값을 찾을 수 있다

### 확률과 정보이론 후
- 엔트로피를 계산할 수 있다
- 정보 이득으로 의사결정 트리를 구축할 수 있다
- Perplexity로 모델을 평가할 수 있다

### LLM 수학 후
- BPE 토크나이저를 구현할 수 있다
- Attention의 각 단계를 설명할 수 있다
- 간단한 LLM의 흐름을 완전히 이해할 수 있다

---

**이 강의를 완료하면 LLM의 핵심 수학을 완벽히 이해할 수 있습니다! 💪**
