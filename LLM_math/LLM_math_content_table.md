# 🎓 LLM 개발을 위한 수학 강의
## 매일 학습하는 실용적 커리큘럼

**강의 제작자**: 진영화 (AI 개발 준비 중)  
**학습 방식**: AI와 함께 강의 파일 제작 + 실습  
**목표**: LLM의 핵심 영역(토크나이저, 어텐션, 임베딩) 개발 가능 수준  
**총 기간**: 약 4-5개월 (매일 1-2시간, 주 5일)  
**학습 환경**: 한국 여름 고려 (7월-9월 속도 조절)

---

# 📋 전체 구조

```
기초 다지기 (2주)
    ↓
벡터와 선형대수 기초 (3주)
    ↓
미적분 기초 (2주)
    ↓
확률과 정보이론 (3주)
    ↓
LLM 핵심 수학 (4주)
    ↓
실전 구현으로 이해하기 (2주)
```

---

# 📚 학습 방식 설명

## 매일의 학습 구조

### 1. 강의 제작 (40분)
- AI와 함께 강의 내용 작성
- 수식 유도 및 설명
- 실습 예제 작성

### 2. 실습 코드 (30분)
- 제공된 Python 코드 직접 실행
- 결과 해석하기
- 변수 조정해보기

### 3. 손 계산 (30분)
- 간단한 예제를 손으로 계산
- 공식 유도 직접 해보기
- 이해도 확인

### 4. 정리 (20분)
- 오늘 배운 내용 요약
- 다음 내용과의 연결성 파악
- 모르는 부분 표시

---

# 🔷 기초 다지기 (1~2주차)

## 학습 목표
- 수와 연산의 본질 이해
- 함수와 변수의 개념 정확히 하기
- 로그와 지수의 관계 깊이 있게 이해

### Day 1: 수의 체계 (1시간)
**강의 주제**: 왜 다양한 수가 필요했을까?

**핵심 개념**
1. 자연수 → 정수 → 유리수 → 실수 → 복소수의 진화
2. 각 단계에서 해결한 문제 (음수, 소수, 제곱근)
3. 벡터: "여러 수를 한 번에 표현하는 방법"

**체크포인트**
- [ ] 음수가 왜 필요한지 설명할 수 있나?
- [ ] 벡터와 스칼라의 차이를 말할 수 있나?

---

### Day 2-3: 지수와 로그 (3시간)
**강의 주제**: "2를 3번 곱하면 8" vs "8을 2로 몇 번 나누면 1?"

**핵심 내용**
- 지수: 2³ = 8의 의미
- 로그: log₂(8) = 3의 의미
- 밑 변환 공식: log_a(x) = ln(x) / ln(a)
- 수치 안정성: Log-softmax

**LLM 연결점**
- 정보량 측정: log₂로 정보의 크기를 비트 단위로 측정
- 엔트로피: 불확실성을 log로 표현
- Softmax: log-softmax는 왜 안정적?

**체크포인트**
- [ ] 2³과 log₂(8)의 관계를 설명할 수 있나?
- [ ] Log-softmax가 왜 수치적으로 안정적인지 이해했나?

---

### Day 4: 함수와 그래프 (1시간)
**강의 주제**: 함수 = 입력을 받아서 출력을 내는 기계

**핵심 개념**
- 함수의 본질: f(x) = y
- 중요한 함수들: 선형, 지수, 로그
- 합성함수: f(g(x))

**체크포인트**
- [ ] 함수를 "입력→계산→출력" 구조로 설명할 수 있나?
- [ ] 지수, 로그, 선형 함수의 모양을 그릴 수 있나?

---

### Day 5: 집합과 논리 (1시간)
**강의 주제**: 데이터와 연산을 다루기 위한 언어

**핵심 내용**
- 집합의 기본 연산: ∪, ∩, ⊆
- 논리 연산과 집합 연산의 관계
- 조건부 확률의 기초

**체크포인트**
- [ ] 집합 기호를 이해했나?
- [ ] 논리 연산과 확률의 연결을 알나?

---

### Day 6-7: 변수, 방정식, 연립방정식 (3시간)
**강의 주제**: "모르는 수를 x라고 하자"

**핵심 내용**
- 일차 방정식 풀기
- 이차 방정식과 근의 공식
- 연립방정식과 행렬 표현: Ax = b
- 가감법, 대입법

**체크포인트**
- [ ] 연립방정식을 가감법으로 풀 수 있나?
- [ ] Ax = b의 의미를 이해했나?

---

### Day 8-10: 좌표평면과 벡터 기초 (3시간)
**강의 주제**: 수를 평면에 그려보기

**핵심 내용**
1. 2D 좌표계 이해
2. 벡터의 시각화와 연산
3. 벡터의 더하기, 스칼라배

**체크포인트**
- [ ] 벡터를 화살표로 그릴 수 있나?
- [ ] 벡터의 덧셈을 시각적으로 이해했나?

---

# 🔷 벡터와 선형대수 (3~5주차)

## 학습 목표
- 벡터/행렬 연산 완벽히 이해
- 내적(dot product) 의미 파악
- 정규화(normalization) 실전 활용

### Day 11: 벡터의 길이와 거리 (1시간)
**핵심**: L2 노름과 피타고라스 정리
- ||v|| = √(x² + y²)
- 거리 = "얼마나 다른가"를 숫자로 표현

### Day 12: 내적(Dot Product) (1.5시간)
**핵심**: 두 벡터가 얼마나 같은 방향인가?
- a · b = a₁b₁ + a₂b₂ + ...
- **LLM 연결**: Attention 메커니즘의 핵심
- a · b = ||a|| × ||b|| × cos(θ)

### Day 13: 정규화(Normalization) (1.5시간)
**핵심**: 벡터를 길이 1로 만들기
- v_normalized = v / ||v||
- **코사인 유사도**: (a · b) / (||a|| × ||b||)
- **RAG 시스템**: 정규화된 벡터로 임베딩 검색

### Day 14: 행렬과 행렬곱셈 (1.5시간)
**핵심**: 벡터 여러 개를 한 번에 다루기
- (m × n) × (n × p) = (m × p)
- **신경망**: y = Wx + b

### Day 15: 중간 복습 (1시간)
**정리**: 벡터와 선형대수의 모든 개념

---

### Day 16-20: 행렬의 성질과 변환 (5시간)
**주제들**:
- 전치(Transpose): A^T
- 역행렬과 단위행렬
- 고유값과 고유벡터: Av = λv
- 행렬식과 노름
- **최종 프로젝트**: 임베딩 공간 분석

---

# 🔷 미적분 (6~7주차)

## 학습 목표
- 변화율(미분) 개념 이해
- 연쇄법칙 (backpropagation의 기초)
- 경사하강법 (최적화)

### Day 21: 극한과 연속성 (1시간)
**핵심**: lim_{x→a} f(x) = L

### Day 22-23: 미분(도함수) (2시간)
**핵심**: 변화의 속도를 측정하기
- f'(x) = lim_{h→0} (f(x+h) - f(x)) / h
- 기본 미분 규칙: d/dx(x^n) = nx^(n-1)
- **LLM**: Backpropagation의 기초

### Day 24: 연쇄법칙(Chain Rule) (1.5시간)
**핵심**: 신경망의 영혼
- df/dx = df/dg × dg/dx
- **역전파**: Loss → 마지막 층 → 첫 층

### Day 25: 편미분과 기울기 (1.5시간)
**핵심**: 여러 변수일 때의 미분
- ∂f/∂x, ∂f/∂y
- **기울기 벡터**: ∇f = (∂f/∂x, ∂f/∂y, ...)

### Day 26: 경사하강법(Gradient Descent) (1.5시간)
**핵심**: 기울기를 따라 최솟값 찾기
- θ_new = θ_old - α × ∇f(θ)
- 학습률의 중요성

### Day 27: 미분 종합 복습 (1시간)

---

# 🔷 확률과 정보이론 (8~10주차)

## 학습 목표
- 확률의 다양한 개념
- 엔트로피 완벽 이해
- 정보 이득과 의사결정

### Day 28: 확률론 기초 (1시간)
**핵심**: 불확실성을 숫자로 표현하기
- P(A) = 0 ~ 1
- 조건부 확률: P(A|B)
- 독립성

### Day 29: 베이즈 정리 (1.5시간)
**핵심**: 사건을 연결하기
- P(A|B) = P(B|A) × P(A) / P(B)
- 사전확률, 가능도, 사후확률
- **LLM**: 토큰 예측 = 베이즈 정리

### Day 30: 확률분포 (1.5시간)
**핵심**: 확률이 어떻게 분포하는가?
- 정규분포(Normal Distribution)
- μ (평균), σ (표준편차)
- **LLM**: 임베딩 초기화

### Day 31: 중간 복습 및 연결 (1시간)
**전환**: 불확실성을 측정할 수 있을까? → 엔트로피!

---

### Day 32-33: 엔트로피 (2시간)
**핵심**: 불확실성의 정량화
- H(X) = -Σ p(x) × log₂(p(x))
- **해석**: 낮은 엔트로피 = 확실성, 높은 엔트로피 = 불확실성
- **LLM**: Perplexity = 2^H

### Day 34: 상호정보량(Mutual Information) (1.5시간)
**핵심**: 두 변수가 얼마나 연관있나?
- I(X;Y) = H(X) - H(X|Y)
- **정보 이득**: Y를 알면 X의 불확실성이 줄어들 정도

### Day 35: 정보 이득과 의사결정 (2시간)
**핵심**: 최선의 결정 내리기
- IG = H(부모) - Σ(비율 × H(자식))
- **의사결정 트리**: 정보 이득이 가장 큰 특성 선택
- **토크나이저**: BPE에서 정보 이득이 가장 큰 쌍을 선택

### Day 36: 정보이론 종합 (2시간)
**최종 정리**: 엔트로피, 상호정보량, 정보이득의 통일

### Day 37-38: 중간 종합 복습 (2시간)
**통합**: 기초, 선형대수, 미적분, 확률이 모두 연결되는 그림

---

# 🔷 LLM 핵심 수학 (11~14주차)

### Day 39: BPE 알고리즘 (2시간)
**핵심**: 텍스트를 효율적으로 나누기
- 자주 나오는 바이트 쌍을 병합
- **정보 이득 최대화**
- 압축과 정보의 관계

### Day 40: WordPiece와 SentencePiece (1.5시간)
**핵심**: BPE의 변형들
- WordPiece: 확률 기반
- SentencePiece: 다국어 지원
- 각각의 장단점

### Day 41: 한국어 토크나이저 (1.5시간)
**당신의 목표 - 한국어 특화**
- 음절 vs 자모
- 형태소 분석과의 결합
- 엔트로피 기반 어휘 크기

### Day 42: 토크나이저 프로젝트 (2시간)
**프로젝트**: 간단한 BPE 토크나이저 구현

---

### Day 43: Scaled Dot-Product Attention (2시간)
**핵심**: 단어들의 관계 찾기
- Attention(Q, K, V) = softmax(QK^T / √d_k) V
- Q, K, V의 의미
- √d_k로 나누는 이유

### Day 44: Multi-Head Attention (1.5시간)
**핵심**: 여러 관점에서 보기
- h개의 병렬 어텐션 헤드
- 다양한 패턴 학습
- 부분공간에서의 작동

### Day 45-46: Transformer 아키텍처 (2시간)
**핵심**: LLM의 기본 설계도
- 인코더와 디코더
- Positional Encoding: PE(pos, 2i) = sin(pos/10000^(2i/d))
- 전체 흐름
- Residual Connection
- Layer Normalization

### Day 47: LLM 학습의 완전한 수학 (2시간)
**통합**: 모든 개념이 어떻게 함께 작동하는가?

### Day 48: 최종 프로젝트 (3시간)
**프로젝트**: Tiny Language Model (NumPy 기반)

---

# 📊 성공의 지표

## 각 단계별 목표 달성 여부

### 기초 다지기 후
- [ ] 로그와 지수를 능숙하게 다룬다
- [ ] 함수를 손으로 그릴 수 있다
- [ ] 연립방정식을 여러 방법으로 풀 수 있다

### 선형대수 후
- [ ] 내적의 기하학적 의미를 설명할 수 있다
- [ ] 정규화의 목적을 알 수 있다
- [ ] 코사인 유사도로 RAG를 이해할 수 있다

### 미적분 후
- [ ] 연쇄법칙을 손으로 계산할 수 있다
- [ ] Backpropagation의 원리를 설명할 수 있다
- [ ] 경사하강법으로 함수의 최솟값을 찾을 수 있다

### 정보이론 후
- [ ] 엔트로피를 계산할 수 있다
- [ ] 정보 이득으로 의사결정 트리를 구축할 수 있다
- [ ] Perplexity로 모델을 평가할 수 있다

### LLM 수학 후
- [ ] BPE 토크나이저를 구현할 수 있다
- [ ] Attention의 각 단계를 설명할 수 있다
- [ ] 간단한 LLM의 흐름을 이해할 수 있다

---

# 🚀 다음 단계

## 이 목차를 완료한 후

### 실전 학습 (Week 49+)
1. **PyTorch로 구현**
   - NumPy 코드를 실제 프레임워크로
   - GPU 가속 활용

2. **한국어 토크나이저 개발**
   - SentencePiece 사용
   - 한국어 말뭉치로 학습

3. **작은 LLM 학습**
   - 1B~3B 파라미터 모델
   - M4 Mac에서 실행 가능

4. **Rust 토크나이저**
   - Python 프로토타입 → Rust 최적화
   - 2027년 이후 단계

---

## 💪 학습 팁

1. **매일 기록하기**: "오늘 배운 것" 정리
2. **손으로 계산하기**: 공식 유도 직접 해보기
3. **코드 실행하기**: 모든 실습 코드 직접 돌려보기
4. **연결 짓기**: "이게 왜 필요한가?"
5. **질문하기**: 모르는 것은 AI와 함께 풀어보기

---

**이 목차가 당신의 출발점입니다.**

매일 차분하게, 한 발씩 나아가세요.
LLM을 만드는 경험이 당신을 바꿀 것입니다.

화이팅! 💪
