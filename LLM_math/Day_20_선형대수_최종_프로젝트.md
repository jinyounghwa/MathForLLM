# Day 20: ì„ í˜•ëŒ€ìˆ˜ ìµœì¢… í”„ë¡œì íŠ¸ - ì„ë² ë”© ê³µê°„ ë¶„ì„ (1ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ì„ í˜•ëŒ€ìˆ˜ ê°œë… ì¢…í•© í™œìš©í•˜ê¸°
- ì‹¤ì œ ì„ë² ë”© ë°ì´í„° ë¶„ì„í•˜ê¸°
- ì½”ì‚¬ì¸ ìœ ì‚¬ë„, PCA, í´ëŸ¬ìŠ¤í„°ë§ ì ìš©í•˜ê¸°

---

## ğŸ¯ í”„ë¡œì íŠ¸ ì£¼ì œ
**"ë‹¨ì–´ ì„ë² ë”© ê³µê°„ íƒí—˜í•˜ê¸°"**

---

## ğŸ“– í”„ë¡œì íŠ¸ ê°œìš”

### êµ¬í˜„í•  ë‚´ìš©
1. ê°„ë‹¨í•œ ë‹¨ì–´ ì„ë² ë”© ìƒì„±
2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°
3. PCAë¡œ 2D ì‹œê°í™”
4. í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì˜ë¯¸ ê·¸ë£¹ ì°¾ê¸°

### ì‚¬ìš©í•  ê°œë…
- ë‚´ì , ì •ê·œí™”, ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- ê³ ìœ ê°’, ê³ ìœ ë²¡í„°, PCA
- í–‰ë ¬ ì—°ì‚°, ê±°ë¦¬ ê³„ì‚°

---

## ğŸ’» ìµœì¢… í”„ë¡œì íŠ¸ ì½”ë“œ

### í”„ë¡œì íŠ¸: ë‹¨ì–´ ì„ë² ë”© ë¶„ì„

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import font_manager
from sklearn.decomposition import PCA

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ===== 1. ë‹¨ì–´ ì„ë² ë”© ìƒì„± =====
print("=" * 50)
print("1. ë‹¨ì–´ ì„ë² ë”© ìƒì„±")
print("=" * 50 + "\n")

# ê°„ë‹¨í•œ ì„ë² ë”© (ì‹¤ì œë¡œëŠ” í•™ìŠµë¨)
# ì°¨ì›: 5
words = ['king', 'queen', 'man', 'woman', 'apple', 'banana', 'car', 'truck']

# ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë°˜ì˜í•œ ì„ë² ë”©
embeddings = {
    'king':   [0.9, 0.8, 0.1, 0.1, 0.0],
    'queen':  [0.85, 0.9, 0.05, 0.15, 0.0],
    'man':    [0.7, 0.5, 0.2, 0.0, 0.0],
    'woman':  [0.65, 0.6, 0.1, 0.2, 0.0],
    'apple':  [0.0, 0.0, 0.9, 0.8, 0.1],
    'banana': [0.0, 0.0, 0.85, 0.9, 0.15],
    'car':    [0.0, 0.0, 0.1, 0.0, 0.9],
    'truck':  [0.0, 0.0, 0.15, 0.05, 0.85]
}

# NumPy ë°°ì—´ë¡œ ë³€í™˜
embedding_matrix = np.array([embeddings[w] for w in words])

print(f"ë‹¨ì–´ ìˆ˜: {len(words)}")
print(f"ì„ë² ë”© ì°¨ì›: {embedding_matrix.shape[1]}")
print(f"ì„ë² ë”© í–‰ë ¬ í˜•íƒœ: {embedding_matrix.shape}\n")

# ===== 2. ì •ê·œí™” =====
print("=" * 50)
print("2. ì„ë² ë”© ì •ê·œí™”")
print("=" * 50 + "\n")

# L2 ì •ê·œí™”
norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)
normalized_embeddings = embedding_matrix / norms

print("ì •ê·œí™” ì „ ë…¸ë¦„:")
for i, word in enumerate(words):
    print(f"  ||{word}|| = {norms[i, 0]:.4f}")

print("\nì •ê·œí™” í›„ ë…¸ë¦„ (ëª¨ë‘ 1.0):")
new_norms = np.linalg.norm(normalized_embeddings, axis=1)
for i, word in enumerate(words):
    print(f"  ||{word}|| = {new_norms[i]:.4f}")

# ===== 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ =====
print("\n" + "=" * 50)
print("3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°")
print("=" * 50 + "\n")

def cosine_similarity(v1, v2):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
    return np.dot(v1, v2)  # ì´ë¯¸ ì •ê·œí™”ë¨

def find_most_similar(word, embeddings_dict, normalized_emb, words_list, top_k=3):
    """ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°"""
    word_idx = words_list.index(word)
    word_vec = normalized_emb[word_idx]

    similarities = []
    for i, other_word in enumerate(words_list):
        if other_word != word:
            sim = cosine_similarity(word_vec, normalized_emb[i])
            similarities.append((other_word, sim))

    # ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]

# ì˜ˆì‹œ: 'king'ê³¼ ìœ ì‚¬í•œ ë‹¨ì–´
query = 'king'
similar = find_most_similar(query, embeddings, normalized_embeddings, words, top_k=3)

print(f"'{query}'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´:")
for word, sim in similar:
    print(f"  {word}: {sim:.4f}")

print()

# ëª¨ë“  ë‹¨ì–´ ìŒì˜ ìœ ì‚¬ë„ í–‰ë ¬
similarity_matrix = normalized_embeddings @ normalized_embeddings.T

print("ìœ ì‚¬ë„ í–‰ë ¬ (ì¼ë¶€):")
print("       ", "  ".join(f"{w:>6}" for w in words[:4]))
for i in range(4):
    row = "  ".join(f"{similarity_matrix[i, j]:6.3f}" for j in range(4))
    print(f"{words[i]:>6}  {row}")

# ===== 4. PCAë¡œ ì°¨ì› ì¶•ì†Œ =====
print("\n" + "=" * 50)
print("4. PCAë¡œ 2D ì‹œê°í™”")
print("=" * 50 + "\n")

# PCA: 5ì°¨ì› â†’ 2ì°¨ì›
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(normalized_embeddings)

print(f"ì›ë³¸ ì°¨ì›: {normalized_embeddings.shape[1]}")
print(f"ì¶•ì†Œ ì°¨ì›: {embeddings_2d.shape[1]}")
print(f"ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨: {pca.explained_variance_ratio_}")
print(f"ì´ ë¶„ì‚°: {sum(pca.explained_variance_ratio_):.4f}\n")

# ì‹œê°í™”
plt.figure(figsize=(12, 10))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=200, alpha=0.6)

for i, word in enumerate(words):
    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                 fontsize=14, ha='center', va='bottom')

plt.xlabel('PC1', fontsize=12)
plt.ylabel('PC2', fontsize=12)
plt.title('Word Embeddings (PCA)', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('word_embeddings_pca.png', dpi=150)
print("ì‹œê°í™” ì €ì¥: word_embeddings_pca.png")

# ===== 5. ê±°ë¦¬ ê¸°ë°˜ ë¶„ì„ =====
print("\n" + "=" * 50)
print("5. ê±°ë¦¬ ê³„ì‚°")
print("=" * 50 + "\n")

def euclidean_distance(v1, v2):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬"""
    return np.linalg.norm(v1 - v2)

# 'king'ê³¼ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬
query = 'king'
query_idx = words.index(query)
query_vec = normalized_embeddings[query_idx]

print(f"'{query}'ì™€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬:")
distances = []
for i, word in enumerate(words):
    if word != query:
        dist = euclidean_distance(query_vec, normalized_embeddings[i])
        distances.append((word, dist))

distances.sort(key=lambda x: x[1])

for word, dist in distances:
    sim = cosine_similarity(query_vec, normalized_embeddings[words.index(word)])
    print(f"  {word:>8}: dist={dist:.4f}, sim={sim:.4f}")

# ===== 6. ë²¡í„° ì—°ì‚° (King - Man + Woman = ?) =====
print("\n" + "=" * 50)
print("6. ë²¡í„° ì—°ì‚° (Word Analogy)")
print("=" * 50 + "\n")

# King - Man + Woman â‰ˆ Queen?
king_vec = normalized_embeddings[words.index('king')]
man_vec = normalized_embeddings[words.index('man')]
woman_vec = normalized_embeddings[words.index('woman')]

# ë²¡í„° ì—°ì‚°
result_vec = king_vec - man_vec + woman_vec
# ì¬ì •ê·œí™”
result_vec = result_vec / np.linalg.norm(result_vec)

print("King - Man + Woman = ?")
print()

# ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°
similarities = []
for i, word in enumerate(words):
    if word not in ['king', 'man', 'woman']:
        sim = cosine_similarity(result_vec, normalized_embeddings[i])
        similarities.append((word, sim))

similarities.sort(key=lambda x: x[1], reverse=True)

print("ê²°ê³¼ ë²¡í„°ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´:")
for word, sim in similarities[:3]:
    print(f"  {word}: {sim:.4f}")

print("\nâœ… 'queen'ì´ ê°€ì¥ ìœ ì‚¬í•˜ê²Œ ë‚˜ì˜¤ë©´ ì„±ê³µ!")

# ===== 7. ì¢…í•© í†µê³„ =====
print("\n" + "=" * 50)
print("7. ì¢…í•© í†µê³„")
print("=" * 50 + "\n")

# ê³µë¶„ì‚° í–‰ë ¬
cov_matrix = np.cov(normalized_embeddings.T)

print(f"ê³µë¶„ì‚° í–‰ë ¬ í˜•íƒœ: {cov_matrix.shape}")
print(f"ê³µë¶„ì‚° í–‰ë ¬ (ì¼ë¶€):\n{cov_matrix[:3, :3]}\n")

# ê³ ìœ ê°’ ë¶„í•´
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
eigenvalues_sorted = np.sort(eigenvalues)[::-1]

print("ê³ ìœ ê°’ (ë‚´ë¦¼ì°¨ìˆœ):")
for i, ev in enumerate(eigenvalues_sorted):
    print(f"  Î»_{i+1} = {ev:.4f}")

print("\nâœ… í”„ë¡œì íŠ¸ ì™„ë£Œ!")
print("\n" + "=" * 50)
print("ë°°ìš´ ê°œë… í™œìš©:")
print("=" * 50)
print("âœ“ ë‚´ì ê³¼ ì •ê·œí™”")
print("âœ“ ì½”ì‚¬ì¸ ìœ ì‚¬ë„")
print("âœ“ ê±°ë¦¬ ê³„ì‚°")
print("âœ“ PCA (ê³ ìœ ê°’/ê³ ìœ ë²¡í„°)")
print("âœ“ í–‰ë ¬ ì—°ì‚°")
print("âœ“ ë²¡í„° ì—°ì‚°")
print("=" * 50)
```

---

## âœï¸ í”„ë¡œì íŠ¸ í™•ì¥ ì•„ì´ë””ì–´

### 1. ë” ë§ì€ ë‹¨ì–´
```python
# ë™ë¬¼, ê³¼ì¼, êµí†µìˆ˜ë‹¨ ë“± ì¹´í…Œê³ ë¦¬ ì¶”ê°€
# ë” ë³µì¡í•œ ì˜ë¯¸ ê´€ê³„ íƒí—˜
```

### 2. 3D ì‹œê°í™”
```python
# PCA n_components=3
# matplotlibì˜ 3D í”Œë¡¯ ì‚¬ìš©
```

### 3. t-SNE
```python
from sklearn.manifold import TSNE
# PCAë³´ë‹¤ ë” ë‚˜ì€ ì‹œê°í™”
```

---

## ğŸ”— LLM ì—°ê²°ì 

### ì‹¤ì œ LLMì—ì„œëŠ”
```
1. ì„ë² ë”©:
   - ìˆ˜ë°±~ìˆ˜ì²œ ì°¨ì›
   - í•™ìŠµìœ¼ë¡œ íšë“

2. ìœ ì‚¬ë„:
   - RAG: ë¬¸ì„œ ê²€ìƒ‰
   - Attention: í† í° ê°„ ê´€ê³„

3. ì°¨ì› ì¶•ì†Œ:
   - ì‹œê°í™”
   - í•´ì„ ê°€ëŠ¥ì„±

4. ë²¡í„° ì—°ì‚°:
   - ì˜ë¯¸ ì¡°í•©
   - ê´€ê³„ í•™ìŠµ
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **ëª¨ë“  ì½”ë“œë¥¼ ì‹¤í–‰í–ˆë‚˜ìš”?**

- [ ] **ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **PCAì˜ ê²°ê³¼ë¥¼ í•´ì„í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **ë²¡í„° ì—°ì‚°ì˜ ì˜ë¯¸ë¥¼ ì´í•´í–ˆë‚˜ìš”?**

---

## ğŸ“ ì„ í˜•ëŒ€ìˆ˜ ì´ì •ë¦¬

**Day 11-20ì—ì„œ ë°°ìš´ ê²ƒ**:

1. **ë²¡í„°**:
   - ê¸¸ì´, ê±°ë¦¬, ë°©í–¥

2. **ë‚´ì **:
   - ìœ ì‚¬ë„ ì¸¡ì •ì˜ í•µì‹¬

3. **ì •ê·œí™”**:
   - í¬ê¸° ì œê±°, ë°©í–¥ë§Œ

4. **í–‰ë ¬**:
   - ì„ í˜• ë³€í™˜, ì‹ ê²½ë§

5. **ì „ì¹˜**:
   - ì°¨ì› ë§ì¶”ê¸°

6. **ì—­í–‰ë ¬**:
   - ë°©ì •ì‹ í’€ì´

7. **ê³ ìœ ê°’/ë²¡í„°**:
   - PCA, ì£¼ì„±ë¶„ ë¶„ì„

8. **ë…¸ë¦„**:
   - í¬ê¸°, ì•ˆì •ì„±

**ì´ ëª¨ë“  ê²ƒì´ LLMì˜ í† ëŒ€ì…ë‹ˆë‹¤!**

### ë‹¤ìŒ í•™ìŠµ
- **Day 21-27**: ë¯¸ì ë¶„
  - ë³€í™”ìœ¨, ê²½ì‚¬í•˜ê°•ë²•, Backpropagation

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**ì„ í˜•ëŒ€ìˆ˜ ë§ˆìŠ¤í„°ë¥¼ ì¶•í•˜í•©ë‹ˆë‹¤!**
