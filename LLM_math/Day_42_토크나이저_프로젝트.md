# Day 42: í† í¬ë‚˜ì´ì € í”„ë¡œì íŠ¸ (2ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- ì™„ì „í•œ BPE í† í¬ë‚˜ì´ì € êµ¬í˜„í•˜ê¸°
- ì¸ì½”ë”©/ë””ì½”ë”© ê¸°ëŠ¥ ë§Œë“¤ê¸°
- ì••ì¶•ë¥  í‰ê°€í•˜ê¸°

---

## ğŸ¯ í”„ë¡œì íŠ¸
**"ë‚˜ë§Œì˜ BPE í† í¬ë‚˜ì´ì € ë§Œë“¤ê¸°"**

---

## ğŸ’» ìµœì¢… í”„ë¡œì íŠ¸ ì½”ë“œ

```python
import re
from collections import Counter, defaultdict

class BPETokenizer:
    """ì™„ì „í•œ BPE í† í¬ë‚˜ì´ì €"""

    def __init__(self, vocab_size=100):
        self.vocab_size = vocab_size
        self.bpe_codes = {}
        self.vocab = set()

    def get_stats(self, words):
        """ë°”ì´íŠ¸ ìŒ ë¹ˆë„"""
        pairs = defaultdict(int)
        for word, freq in words.items():
            symbols = word.split()
            for i in range(len(symbols)-1):
                pairs[symbols[i], symbols[i+1]] += freq
        return pairs

    def merge_pair(self, pair, words):
        """ìŒ ë³‘í•©"""
        new_words = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)

        pattern = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + pattern + r'(?!\S)')

        for word in words:
            new_word = p.sub(replacement, word)
            new_words[new_word] = words[word]
        return new_words

    def train(self, corpus):
        """í•™ìŠµ"""
        # ë‹¨ì–´ ë¹ˆë„
        words = corpus.lower().split()
        word_freqs = Counter(words)

        # ë¬¸ìë¡œ ë¶„ë¦¬
        vocab_words = {' '.join(word): freq
                      for word, freq in word_freqs.items()}

        # ì´ˆê¸° ì–´íœ˜ (ë¬¸ì)
        for word in vocab_words:
            self.vocab.update(word.split())

        print(f"=== BPE í•™ìŠµ ===\n")
        print(f"ì´ˆê¸° ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")

        # BPE í•™ìŠµ
        for i in range(self.vocab_size - len(self.vocab)):
            pairs = self.get_stats(vocab_words)

            if not pairs:
                break

            best_pair = max(pairs, key=pairs.get)
            vocab_words = self.merge_pair(best_pair, vocab_words)

            self.bpe_codes[best_pair] = i
            self.vocab.add(''.join(best_pair))

            if i % 10 == 0 or i < 5:
                print(f"  {i+1}. {best_pair[0]} + {best_pair[1]} â†’ "
                      f"{''.join(best_pair)} (ë¹ˆë„: {pairs[best_pair]})")

        print(f"\nìµœì¢… ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")

    def encode(self, text):
        """í…ìŠ¤íŠ¸ â†’ í† í°"""
        words = text.lower().split()
        encoded = []

        for word in words:
            word_tokens = ' '.join(word)

            # BPE ì ìš©
            while True:
                pairs = [(word_tokens.split()[i], word_tokens.split()[i+1])
                        for i in range(len(word_tokens.split())-1)]

                if not pairs:
                    break

                # í•™ìŠµëœ ìˆœì„œëŒ€ë¡œ ë³‘í•©
                min_pair = min(pairs,
                              key=lambda p: self.bpe_codes.get(p, float('inf')))

                if min_pair not in self.bpe_codes:
                    break

                first, second = min_pair
                new_word = []
                i = 0
                symbols = word_tokens.split()

                while i < len(symbols):
                    if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:
                        new_word.append(first + second)
                        i += 2
                    else:
                        new_word.append(symbols[i])
                        i += 1

                word_tokens = ' '.join(new_word)

            encoded.extend(word_tokens.split())

        return encoded

    def decode(self, tokens):
        """í† í° â†’ í…ìŠ¤íŠ¸"""
        return ' '.join(''.join(tokens).split())

# ì‚¬ìš© ì˜ˆ
corpus = """
hello hello hello world
hello world
world of warcraft
"""

# í•™ìŠµ
tokenizer = BPETokenizer(vocab_size=50)
tokenizer.train(corpus)

# í…ŒìŠ¤íŠ¸
test_text = "hello world"
tokens = tokenizer.encode(test_text)
decoded = tokenizer.decode(tokens)

print(f"\n=== í…ŒìŠ¤íŠ¸ ===")
print(f"ì›ë³¸: {test_text}")
print(f"í† í°: {tokens}")
print(f"ë””ì½”ë”©: {decoded}")

# ì••ì¶•ë¥ 
original_chars = len(test_text.replace(' ', ''))
num_tokens = len(tokens)
compression = original_chars / num_tokens

print(f"\nì••ì¶•ë¥ : {compression:.2f}x")
```

---

## ğŸ“ í”„ë¡œì íŠ¸ ì™„ë£Œ!

**ë‹¹ì‹ ì€ ì´ì œ í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!**

### ë‹¤ìŒ ë‹¨ê³„
- **Day 43**: Scaled Dot-Product Attention
  - LLMì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜!

---

**ì¶•í•˜í•©ë‹ˆë‹¤!** ğŸ‰
