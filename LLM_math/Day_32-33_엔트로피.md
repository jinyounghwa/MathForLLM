# Day 32-33: ì—”íŠ¸ë¡œí”¼ (2ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- **ì—”íŠ¸ë¡œí”¼ì˜ ì •ì˜ì™€ ì˜ë¯¸ ì™„ë²½íˆ ì´í•´í•˜ê¸°** â­
- ì—”íŠ¸ë¡œí”¼ ê³„ì‚°í•˜ê¸°
- LLMì˜ Perplexity ì´í•´í•˜ê¸°

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"ë¶ˆí™•ì‹¤ì„±ì˜ ì •ëŸ‰í™”"**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. ì—”íŠ¸ë¡œí”¼ (Entropy)

**ì •ì˜**:
```
H(X) = -Î£ P(x) Ã— logâ‚‚ P(x)

ë¶ˆí™•ì‹¤ì„±ì˜ í‰ê·  ì •ë³´ëŸ‰
```

**ì§ê´€**:
```
H = 0: ì™„ì „íˆ í™•ì‹¤ (í•˜ë‚˜ë§Œ P=1)
H í¬ë‹¤: ë§¤ìš° ë¶ˆí™•ì‹¤ (ê· ë“± ë¶„í¬ì— ê°€ê¹Œì›€)
```

**ì˜ˆì‹œ**:
```
ë™ì „ (ê³µì •):
P(H) = 0.5, P(T) = 0.5

H = -0.5 logâ‚‚(0.5) - 0.5 logâ‚‚(0.5)
  = -0.5(-1) - 0.5(-1)
  = 1 bit

â†’ í‰ê·  1ë¹„íŠ¸ì˜ ì •ë³´ í•„ìš”
```

---

### 2. ì—”íŠ¸ë¡œí”¼ì˜ ì„±ì§ˆ

**1. ë¹„ìŒìˆ˜**:
```
H(X) â‰¥ 0
```

**2. ìµœëŒ“ê°’**:
```
H(X) â‰¤ logâ‚‚(n)  (n: ê²½ìš°ì˜ ìˆ˜)

ê· ë“±ë¶„í¬ì¼ ë•Œ ìµœëŒ€!
```

**3. ê²°ì •ë¡ ì **:
```
P(x) = 1ì¸ ê²½ìš° í•˜ë‚˜ë§Œ ìˆìœ¼ë©´:
H(X) = 0
```

---

### 3. êµì°¨ ì—”íŠ¸ë¡œí”¼ (Cross Entropy)

**ì •ì˜**:
```
H(P, Q) = -Î£ P(x) Ã— log Q(x)

P: ì‹¤ì œ ë¶„í¬
Q: ëª¨ë¸ ë¶„í¬
```

**LLM ì†ì‹¤ í•¨ìˆ˜**:
```
Loss = CrossEntropy(ì‹¤ì œ, ì˜ˆì¸¡)

ì—”íŠ¸ë¡œí”¼ë¥¼ ìµœì†Œí™” = ì˜ˆì¸¡ ê°œì„ !
```

---

### 4. Perplexity

**ì •ì˜**:
```
Perplexity = 2^H

H: êµì°¨ ì—”íŠ¸ë¡œí”¼
```

**ì˜ë¯¸**:
```
"ëª¨ë¸ì´ í‰ê· ì ìœ¼ë¡œ ëª‡ ê°œì˜ ì„ íƒì§€ ì¤‘ì— ê³ ë¯¼í•˜ëŠ”ê°€?"

ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ!
```

---

## ğŸ’» Python ì‹¤ìŠµ

### ì‹¤ìŠµ 1: ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
```python
import numpy as np

def entropy(probs):
    """ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
    # 0 log 0 = 0ìœ¼ë¡œ ì²˜ë¦¬
    probs = probs[probs > 0]
    return -np.sum(probs * np.log2(probs))

print("=== ì—”íŠ¸ë¡œí”¼ ===\n")

# ê³µì •í•œ ë™ì „
p_fair = np.array([0.5, 0.5])
h_fair = entropy(p_fair)
print(f"ê³µì • ë™ì „: P={p_fair}, H={h_fair:.4f} bits")

# í¸í–¥ëœ ë™ì „
p_biased = np.array([0.9, 0.1])
h_biased = entropy(p_biased)
print(f"í¸í–¥ ë™ì „: P={p_biased}, H={h_biased:.4f} bits")

# í™•ì‹¤
p_certain = np.array([1.0, 0.0])
h_certain = entropy(p_certain)
print(f"í™•ì‹¤: P={p_certain}, H={h_certain:.4f} bits")

# ì£¼ì‚¬ìœ„
p_dice = np.ones(6) / 6
h_dice = entropy(p_dice)
print(f"ì£¼ì‚¬ìœ„: H={h_dice:.4f} bits (ì´ë¡ : logâ‚‚(6)={np.log2(6):.4f})")
```

### ì‹¤ìŠµ 2: Cross Entropyì™€ Perplexity
```python
import numpy as np

def cross_entropy(p, q):
    """êµì°¨ ì—”íŠ¸ë¡œí”¼"""
    return -np.sum(p * np.log(q))

def perplexity(p, q):
    """Perplexity"""
    ce = cross_entropy(p, q)
    return np.exp(ce)

print("\n=== Cross Entropy & Perplexity ===\n")

# ì‹¤ì œ ë¶„í¬
p = np.array([0.7, 0.2, 0.1])

# ì¢‹ì€ ëª¨ë¸ (ì‹¤ì œì™€ ìœ ì‚¬)
q_good = np.array([0.65, 0.25, 0.10])

# ë‚˜ìœ ëª¨ë¸
q_bad = np.array([0.33, 0.33, 0.34])

ce_good = cross_entropy(p, q_good)
ce_bad = cross_entropy(p, q_bad)

ppl_good = perplexity(p, q_good)
ppl_bad = perplexity(p, q_bad)

print(f"ì¢‹ì€ ëª¨ë¸: CE={ce_good:.4f}, PPL={ppl_good:.4f}")
print(f"ë‚˜ìœ ëª¨ë¸: CE={ce_bad:.4f}, PPL={ppl_bad:.4f}")
print("\nâ†’ CEì™€ PPLì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸!")
```

### ì‹¤ìŠµ 3: LLM ì‹œë®¬ë ˆì´ì…˜
```python
import numpy as np

# í† í° ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜
vocab = ['the', 'a', 'is', 'are', '.']
n_vocab = len(vocab)

# ì‹¤ì œ ë‹¤ìŒ í† í°: 'is'
true_token = 2
p_true = np.zeros(n_vocab)
p_true[true_token] = 1.0

# ëª¨ë¸ ì˜ˆì¸¡
p_model = np.array([0.1, 0.15, 0.6, 0.1, 0.05])

print("\n=== LLM í† í° ì˜ˆì¸¡ ===\n")
print(f"ì–´íœ˜: {vocab}")
print(f"ì‹¤ì œ: {vocab[true_token]}")
print(f"ëª¨ë¸ ì˜ˆì¸¡: {p_model}\n")

# Cross entropy loss
loss = cross_entropy(p_true, p_model)
ppl = perplexity(p_true, p_model)

print(f"Loss (CE): {loss:.4f}")
print(f"Perplexity: {ppl:.4f}")
print(f"\nâ†’ ëª¨ë¸ì´ í‰ê·  {ppl:.2f}ê°œ ì„ íƒì§€ ì¤‘ ê³ ë¯¼")
```

---

## âœï¸ ì† ê³„ì‚° ì—°ìŠµ

### ì—°ìŠµ 1: ì—”íŠ¸ë¡œí”¼
```
P = [0.5, 0.5]

H = -0.5 logâ‚‚(0.5) - 0.5 logâ‚‚(0.5)
  = -0.5(-1) - 0.5(-1)
  = 1 bit
```

### ì—°ìŠµ 2: ë¶ˆê· ë“± ë¶„í¬
```
P = [0.8, 0.2]

H = -0.8 logâ‚‚(0.8) - 0.2 logâ‚‚(0.2)
  â‰ˆ -0.8(-0.322) - 0.2(-2.322)
  â‰ˆ 0.258 + 0.464
  â‰ˆ 0.722 bits
```

---

## ğŸ”— LLM ì—°ê²°ì 

### 1. ì†ì‹¤ í•¨ìˆ˜
```python
# PyTorch
loss = nn.CrossEntropyLoss()
output = model(input)
loss_value = loss(output, target)

# ë‚´ë¶€ì ìœ¼ë¡œ cross entropy ê³„ì‚°!
```

### 2. Perplexity í‰ê°€
```
ë‚®ì€ perplexity = ì¢‹ì€ ì–¸ì–´ ëª¨ë¸

GPT-3: PPL â‰ˆ 20
ì¸ê°„: PPL â‰ˆ 10-12
```

### 3. í† í°í™”ì™€ ì—”íŠ¸ë¡œí”¼
```
BPE: ì •ë³´ëŸ‰(ì—”íŠ¸ë¡œí”¼)ë¥¼ ê³ ë ¤í•œ ë³‘í•©
â†’ íš¨ìœ¨ì ì¸ ì••ì¶•
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **ì—”íŠ¸ë¡œí”¼ì˜ ì˜ë¯¸ë¥¼ ì´í•´í–ˆë‚˜ìš”?**
- [ ] **ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‚˜ìš”?**
- [ ] **Cross Entropyì™€ Perplexityë¥¼ ì•„ë‚˜ìš”?**
- [ ] **LLM ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì´í•´í–ˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

1. **ì—”íŠ¸ë¡œí”¼**: H = -Î£ P log P (ë¶ˆí™•ì‹¤ì„±)
2. **Cross Entropy**: -Î£ P log Q (ì†ì‹¤)
3. **Perplexity**: 2^H (í‰ê°€ ì§€í‘œ)
4. **LLM**: CEë¥¼ ìµœì†Œí™” = í•™ìŠµ!

### ë‹¤ìŒ í•™ìŠµ
- **Day 34**: ìƒí˜¸ì •ë³´ëŸ‰

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**ì—”íŠ¸ë¡œí”¼ëŠ” ì •ë³´ì´ë¡ ì˜ í•µì‹¬ì…ë‹ˆë‹¤!**
