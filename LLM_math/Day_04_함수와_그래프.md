# Day 4: í•¨ìˆ˜ì™€ ê·¸ë˜í”„ (1ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- í•¨ìˆ˜ì˜ ë³¸ì§ˆì„ "ì…ë ¥ â†’ ê³„ì‚° â†’ ì¶œë ¥" êµ¬ì¡°ë¡œ ì´í•´í•˜ê¸°
- ì£¼ìš” í•¨ìˆ˜ë“¤(ì„ í˜•, ì§€ìˆ˜, ë¡œê·¸)ì˜ íŠ¹ì§• íŒŒì•…í•˜ê¸°
- í•©ì„±í•¨ìˆ˜ì˜ ê°œë…ê³¼ ì‹ ê²½ë§ê³¼ì˜ ì—°ê²° ì´í•´í•˜ê¸°

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"í•¨ìˆ˜ = ì…ë ¥ì„ ë°›ì•„ì„œ ì¶œë ¥ì„ ë‚´ëŠ” ê¸°ê³„"**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. í•¨ìˆ˜ì˜ ì •ì˜

#### 1.1 í•¨ìˆ˜ë€?
**ì…ë ¥(x)ì„ ë°›ì•„ ì •í•´ì§„ ê·œì¹™ì— ë”°ë¼ ì¶œë ¥(y)ì„ ë‚´ëŠ” ê´€ê³„**

```
f(x) = y
```

**êµ¬ì„± ìš”ì†Œ**:
- **ì •ì˜ì—­ (Domain)**: ì…ë ¥ ê°€ëŠ¥í•œ ê°’ë“¤ì˜ ì§‘í•©
- **ê³µì—­ (Codomain)**: ì¶œë ¥ ê°€ëŠ¥í•œ ê°’ë“¤ì˜ ì§‘í•©
- **ì¹˜ì—­ (Range)**: ì‹¤ì œë¡œ ì¶œë ¥ë˜ëŠ” ê°’ë“¤ì˜ ì§‘í•©

**ì˜ˆì‹œ**:
```python
def f(x):
    return 2 * x + 1

f(3) = 7   # ì…ë ¥ 3 â†’ ì¶œë ¥ 7
f(5) = 11  # ì…ë ¥ 5 â†’ ì¶œë ¥ 11
```

---

### 2. ì£¼ìš” í•¨ìˆ˜ë“¤

#### 2.1 ì„ í˜• í•¨ìˆ˜ (Linear Function)
```
f(x) = ax + b
```

- **a**: ê¸°ìš¸ê¸° (slope)
- **b**: yì ˆí¸ (intercept)

**íŠ¹ì§•**:
- ì§ì„  ê·¸ë˜í”„
- ì¼ì •í•œ ë³€í™”ìœ¨
- ê°€ì¥ ë‹¨ìˆœí•œ í•¨ìˆ˜

**ì˜ˆì‹œ**:
```
f(x) = 2x + 1
f(0) = 1
f(1) = 3
f(2) = 5
```

**LLM ì—°ê²°**: ì„ í˜• ë³€í™˜ (Linear Layer)
```python
y = Wx + b  # ì‹ ê²½ë§ì˜ ê¸°ë³¸ ì—°ì‚°
```

---

#### 2.2 ì§€ìˆ˜ í•¨ìˆ˜ (Exponential Function)
```
f(x) = aË£  (íŠ¹íˆ eË£)
```

**íŠ¹ì§•**:
- ë¹ ë¥´ê²Œ ì¦ê°€ (í­ë°œì  ì„±ì¥)
- í•­ìƒ ì–‘ìˆ˜
- ë¯¸ë¶„í•´ë„ ìê¸° ìì‹  (eË£ì˜ ê²½ìš°)

**ê·¸ë˜í”„ í˜•íƒœ**:
```
  â†‘
  |     *
  |    *
  |   *
  |  *
  | *
  |*_____________â†’
```

**LLM ì—°ê²°**: Softmax
```python
softmax(x) = exp(x) / Î£ exp(x)
```

---

#### 2.3 ë¡œê·¸ í•¨ìˆ˜ (Logarithmic Function)
```
f(x) = log(x)
```

**íŠ¹ì§•**:
- ì²œì²œíˆ ì¦ê°€
- x > 0ì—ì„œë§Œ ì •ì˜
- ì§€ìˆ˜ í•¨ìˆ˜ì˜ ì—­í•¨ìˆ˜

**ê·¸ë˜í”„ í˜•íƒœ**:
```
  â†‘
  |         *****
  |      ***
  |   **
  | *
  |*
  |_____________â†’
```

**LLM ì—°ê²°**: Log-Softmax, Cross-Entropy
```python
loss = -log(predicted_prob)
```

---

#### 2.4 ì´ì°¨ í•¨ìˆ˜ (Quadratic Function)
```
f(x) = axÂ² + bx + c
```

**íŠ¹ì§•**:
- í¬ë¬¼ì„  ëª¨ì–‘
- ìµœëŒ“ê°’ ë˜ëŠ” ìµœì†Ÿê°’ ì¡´ì¬
- a > 0: ì•„ë˜ë¡œ ë³¼ë¡, a < 0: ìœ„ë¡œ ë³¼ë¡

**LLM ì—°ê²°**: ì†ì‹¤ í•¨ìˆ˜ (Loss Function)
```
L(Î¸) = (y - Å·)Â²  # MSE Loss
```

---

#### 2.5 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (Sigmoid Function)
```
Ïƒ(x) = 1 / (1 + eâ»Ë£)
```

**íŠ¹ì§•**:
- Sì ëª¨ì–‘
- ì¶œë ¥ ë²”ìœ„: (0, 1)
- í™•ë¥ ë¡œ í•´ì„ ê°€ëŠ¥

**ê·¸ë˜í”„**:
```
  1 â†‘     ________
    |    /
0.5 |   *
    |  /
  0 |_/___________â†’
      0
```

**LLM ì—°ê²°**: ì´ì§„ ë¶„ë¥˜, Gate ë©”ì»¤ë‹ˆì¦˜ (LSTM)
```python
gate = sigmoid(Wx + b)
```

---

#### 2.6 ReLU í•¨ìˆ˜
```
ReLU(x) = max(0, x) = {x if x > 0, 0 if x â‰¤ 0}
```

**ê·¸ë˜í”„**:
```
  â†‘
  |    /
  |   /
  |  /
  | /
  |/_____________â†’
  0
```

**íŠ¹ì§•**:
- ë‹¨ìˆœí•˜ê³  ë¹ ë¦„
- ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°
- í˜„ëŒ€ ì‹ ê²½ë§ì˜ í‘œì¤€ í™œì„±í™” í•¨ìˆ˜

**LLM ì—°ê²°**: Transformerì˜ FFN (Feed-Forward Network)
```python
output = ReLU(Wx + b)
```

---

### 3. í•©ì„±í•¨ìˆ˜ (Function Composition)

#### 3.1 ì •ì˜
**í•¨ìˆ˜ë¥¼ ì°¨ë¡€ë¡œ ì ìš©**

```
(f âˆ˜ g)(x) = f(g(x))
```

**ë‹¨ê³„**:
1. g(x) ê³„ì‚°
2. ê·¸ ê²°ê³¼ë¥¼ fì— ë„£ê¸°

**ì˜ˆì‹œ**:
```
f(x) = xÂ²
g(x) = x + 1

(f âˆ˜ g)(3) = f(g(3))
           = f(4)
           = 16
```

---

#### 3.2 ì‹ ê²½ë§ = í•©ì„±í•¨ìˆ˜!

**1ì¸µ ì‹ ê²½ë§**:
```
h = ReLU(Wâ‚x + bâ‚)
y = Wâ‚‚h + bâ‚‚
```

**í•©ì„±í•¨ìˆ˜ë¡œ í‘œí˜„**:
```
y = fâ‚‚(fâ‚(x))
```

**ê¹Šì€ ì‹ ê²½ë§ (Deep Neural Network)**:
```
y = f_n(f_{n-1}(...fâ‚‚(fâ‚(x))))
```

**LLM (Transformer)**:
```
x â†’ Embedding â†’ Layer1 â†’ Layer2 â†’ ... â†’ Layer_N â†’ Output
```

ê° ì¸µì´ í•¨ìˆ˜ì´ê³ , ì „ì²´ê°€ ê±°ëŒ€í•œ í•©ì„±í•¨ìˆ˜!

---

## ğŸ’» Python ì‹¤ìŠµ

### ì‹¤ìŠµ 1: ê¸°ë³¸ í•¨ìˆ˜ ì •ì˜ì™€ ì‚¬ìš©
```python
import numpy as np
import matplotlib.pyplot as plt

# 1. ì„ í˜• í•¨ìˆ˜
def linear(x):
    return 2 * x + 1

# 2. ì§€ìˆ˜ í•¨ìˆ˜
def exponential(x):
    return np.exp(x)

# 3. ë¡œê·¸ í•¨ìˆ˜
def logarithm(x):
    return np.log(x)

# 4. ì´ì°¨ í•¨ìˆ˜
def quadratic(x):
    return x**2 - 4*x + 3

# 5. ì‹œê·¸ëª¨ì´ë“œ
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 6. ReLU
def relu(x):
    return np.maximum(0, x)

# í…ŒìŠ¤íŠ¸
x_test = 2
print(f"x = {x_test}")
print(f"linear({x_test}) = {linear(x_test)}")
print(f"exponential({x_test}) = {exponential(x_test):.4f}")
print(f"logarithm({x_test}) = {logarithm(x_test):.4f}")
print(f"quadratic({x_test}) = {quadratic(x_test)}")
print(f"sigmoid({x_test}) = {sigmoid(x_test):.4f}")
print(f"relu({x_test}) = {relu(x_test)}")
```

### ì‹¤ìŠµ 2: í•¨ìˆ˜ ì‹œê°í™”
```python
import numpy as np
import matplotlib.pyplot as plt

# x ë²”ìœ„ ì„¤ì •
x_exp = np.linspace(-2, 2, 100)
x_log = np.linspace(0.1, 5, 100)
x_sigmoid = np.linspace(-6, 6, 100)
x_relu = np.linspace(-5, 5, 100)

# í•¨ìˆ˜ ê³„ì‚°
y_linear = 2 * x_exp + 1
y_exp = np.exp(x_exp)
y_log = np.log(x_log)
y_sigmoid = 1 / (1 + np.exp(-x_sigmoid))
y_relu = np.maximum(0, x_relu)

# ì‹œê°í™”
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# ì„ í˜• í•¨ìˆ˜
axes[0, 0].plot(x_exp, y_linear, linewidth=2, color='blue')
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].set_title('Linear: f(x) = 2x + 1', fontsize=12)
axes[0, 0].axhline(y=0, color='k', linewidth=0.5)
axes[0, 0].axvline(x=0, color='k', linewidth=0.5)

# ì§€ìˆ˜ í•¨ìˆ˜
axes[0, 1].plot(x_exp, y_exp, linewidth=2, color='red')
axes[0, 1].grid(True, alpha=0.3)
axes[0, 1].set_title('Exponential: f(x) = eË£', fontsize=12)
axes[0, 1].axhline(y=0, color='k', linewidth=0.5)
axes[0, 1].axvline(x=0, color='k', linewidth=0.5)

# ë¡œê·¸ í•¨ìˆ˜
axes[0, 2].plot(x_log, y_log, linewidth=2, color='green')
axes[0, 2].grid(True, alpha=0.3)
axes[0, 2].set_title('Logarithm: f(x) = ln(x)', fontsize=12)
axes[0, 2].axhline(y=0, color='k', linewidth=0.5)
axes[0, 2].axvline(x=0, color='k', linewidth=0.5)

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
axes[1, 0].plot(x_sigmoid, y_sigmoid, linewidth=2, color='purple')
axes[1, 0].grid(True, alpha=0.3)
axes[1, 0].set_title('Sigmoid: Ïƒ(x) = 1/(1+eâ»Ë£)', fontsize=12)
axes[1, 0].axhline(y=0, color='k', linewidth=0.5)
axes[1, 0].axhline(y=1, color='k', linewidth=0.5, linestyle='--')
axes[1, 0].axhline(y=0.5, color='r', linewidth=0.5, linestyle='--')
axes[1, 0].axvline(x=0, color='k', linewidth=0.5)

# ReLU í•¨ìˆ˜
axes[1, 1].plot(x_relu, y_relu, linewidth=2, color='orange')
axes[1, 1].grid(True, alpha=0.3)
axes[1, 1].set_title('ReLU: f(x) = max(0, x)', fontsize=12)
axes[1, 1].axhline(y=0, color='k', linewidth=0.5)
axes[1, 1].axvline(x=0, color='k', linewidth=0.5)

# ë¹„êµ: Sigmoid vs ReLU
axes[1, 2].plot(x_sigmoid, y_sigmoid, linewidth=2, label='Sigmoid', color='purple')
axes[1, 2].plot(x_relu, y_relu / 5, linewidth=2, label='ReLU (scaled)', color='orange')
axes[1, 2].grid(True, alpha=0.3)
axes[1, 2].set_title('Activation Functions', fontsize=12)
axes[1, 2].legend()
axes[1, 2].axhline(y=0, color='k', linewidth=0.5)
axes[1, 2].axvline(x=0, color='k', linewidth=0.5)

plt.tight_layout()
plt.savefig('functions_overview.png', dpi=150, bbox_inches='tight')
print("í•¨ìˆ˜ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ!")
```

### ì‹¤ìŠµ 3: í•©ì„±í•¨ìˆ˜
```python
import numpy as np

# ê¸°ë³¸ í•¨ìˆ˜ ì •ì˜
def f(x):
    """f(x) = xÂ²"""
    return x**2

def g(x):
    """g(x) = x + 1"""
    return x + 1

def h(x):
    """h(x) = 2x"""
    return 2 * x

# í•©ì„±í•¨ìˆ˜
def f_compose_g(x):
    """(f âˆ˜ g)(x) = f(g(x))"""
    return f(g(x))

def g_compose_f(x):
    """(g âˆ˜ f)(x) = g(f(x))"""
    return g(f(x))

def f_g_h(x):
    """f(g(h(x)))"""
    return f(g(h(x)))

# í…ŒìŠ¤íŠ¸
x = 3
print("=== í•©ì„±í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===")
print(f"x = {x}")
print(f"f(x) = xÂ² = {f(x)}")
print(f"g(x) = x + 1 = {g(x)}")
print(f"h(x) = 2x = {h(x)}")
print()
print(f"(f âˆ˜ g)(x) = f(g(x)) = f({g(x)}) = {f_compose_g(x)}")
print(f"(g âˆ˜ f)(x) = g(f(x)) = g({f(x)}) = {g_compose_f(x)}")
print(f"f(g(h(x))) = f(g({h(x)})) = f({g(h(x))}) = {f_g_h(x)}")
print()
print("âš ï¸ í•©ì„±í•¨ìˆ˜ëŠ” ìˆœì„œê°€ ì¤‘ìš”! (f âˆ˜ g) â‰  (g âˆ˜ f)")
```

### ì‹¤ìŠµ 4: ì‹ ê²½ë§ = í•©ì„±í•¨ìˆ˜
```python
import numpy as np

# ê°„ë‹¨í•œ 2ì¸µ ì‹ ê²½ë§
class TwoLayerNetwork:
    def __init__(self, input_dim, hidden_dim, output_dim):
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.01
        self.b1 = np.zeros(hidden_dim)
        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01
        self.b2 = np.zeros(output_dim)

    def layer1(self, x):
        """ì²« ë²ˆì§¸ ì¸µ: fâ‚(x) = ReLU(Wâ‚x + bâ‚)"""
        z = np.dot(x, self.W1) + self.b1
        return np.maximum(0, z)  # ReLU

    def layer2(self, h):
        """ë‘ ë²ˆì§¸ ì¸µ: fâ‚‚(h) = Wâ‚‚h + bâ‚‚"""
        return np.dot(h, self.W2) + self.b2

    def forward(self, x):
        """ì „ì²´ ë„¤íŠ¸ì›Œí¬: y = fâ‚‚(fâ‚(x))"""
        h = self.layer1(x)  # ì¤‘ê°„ì¸µ
        y = self.layer2(h)  # ì¶œë ¥ì¸µ
        return y

# ë„¤íŠ¸ì›Œí¬ ìƒì„±
net = TwoLayerNetwork(input_dim=3, hidden_dim=4, output_dim=2)

# í…ŒìŠ¤íŠ¸ ì…ë ¥
x = np.array([1.0, 2.0, 3.0])

print("=== ì‹ ê²½ë§ = í•©ì„±í•¨ìˆ˜ ===")
print(f"ì…ë ¥ x: {x}")
print(f"ì…ë ¥ ì°¨ì›: {x.shape}")
print()

# ì¸µë³„ ì¶œë ¥ í™•ì¸
h = net.layer1(x)
print(f"Layer 1 ì¶œë ¥ (fâ‚(x)): {h}")
print(f"ì°¨ì›: {h.shape}")
print()

y = net.layer2(h)
print(f"Layer 2 ì¶œë ¥ (fâ‚‚(fâ‚(x))): {y}")
print(f"ì°¨ì›: {y.shape}")
print()

# ì „ì²´ ìˆœì „íŒŒ
output = net.forward(x)
print(f"ì „ì²´ ë„¤íŠ¸ì›Œí¬ ì¶œë ¥: {output}")
print("\nâœ… ì‹ ê²½ë§ = ì—¬ëŸ¬ í•¨ìˆ˜ë¥¼ í•©ì„±í•œ ê²ƒ!")
```

---

## âœï¸ ì† ê³„ì‚° ì—°ìŠµ

### ì—°ìŠµ 1: í•¨ìˆ˜ê°’ ê³„ì‚°
ë‹¤ìŒ í•¨ìˆ˜ë“¤ì˜ ê°’ì„ ê³„ì‚°í•˜ì„¸ìš”:

1. f(x) = 3x + 2, f(4) = ?
   ```
   f(4) = 3(4) + 2 = 12 + 2 = 14
   ```

2. g(x) = xÂ², g(-3) = ?
   ```
   g(-3) = (-3)Â² = 9
   ```

3. h(x) = 2Ë£, h(3) = ?
   ```
   h(3) = 2Â³ = 8
   ```

### ì—°ìŠµ 2: í•©ì„±í•¨ìˆ˜
f(x) = 2x, g(x) = x + 3ì¼ ë•Œ:

1. (f âˆ˜ g)(5) = ?
   ```
   (f âˆ˜ g)(5) = f(g(5))
              = f(5 + 3)
              = f(8)
              = 2(8)
              = 16
   ```

2. (g âˆ˜ f)(5) = ?
   ```
   (g âˆ˜ f)(5) = g(f(5))
              = g(2 Ã— 5)
              = g(10)
              = 10 + 3
              = 13
   ```

3. (f âˆ˜ g)(x) = ?
   ```
   (f âˆ˜ g)(x) = f(g(x))
              = f(x + 3)
              = 2(x + 3)
              = 2x + 6
   ```

### ì—°ìŠµ 3: í•¨ìˆ˜ ê·¸ë˜í”„ ìŠ¤ì¼€ì¹˜
ë‹¤ìŒ í•¨ìˆ˜ì˜ ê·¸ë˜í”„ ê°œí˜•ì„ ê·¸ë ¤ë³´ì„¸ìš”:

1. f(x) = x (ì„ í˜•)
2. f(x) = eË£ (ì§€ìˆ˜)
3. f(x) = ln(x) (ë¡œê·¸)
4. f(x) = 1/(1+eâ»Ë£) (ì‹œê·¸ëª¨ì´ë“œ)

---

## ğŸ”— LLM ì—°ê²°ì 

### 1. Transformer = ê±°ëŒ€í•œ í•©ì„±í•¨ìˆ˜

**Transformer êµ¬ì¡°**:
```
Input â†’ Embedding â†’
  â†’ Layer 1 (Attention + FFN) â†’
  â†’ Layer 2 (Attention + FFN) â†’
  â†’ ... â†’
  â†’ Layer N â†’
  â†’ Output
```

**ìˆ˜í•™ì  í‘œí˜„**:
```
y = f_N(...f_2(f_1(Embed(x))))
```

### 2. í™œì„±í™” í•¨ìˆ˜ì˜ ì—­í• 

**ì„ í˜• ì¸µë§Œ ìŒ“ìœ¼ë©´?**
```
y = Wâ‚‚(Wâ‚x) = (Wâ‚‚Wâ‚)x = W_totalx
```
â†’ ì—¬ëŸ¬ ì¸µ = í•˜ë‚˜ì˜ ì„ í˜• ì¸µ (ì˜ë¯¸ ì—†ìŒ!)

**í™œì„±í™” í•¨ìˆ˜ ì¶”ê°€**:
```
y = Wâ‚‚(ReLU(Wâ‚x))
```
â†’ ë¹„ì„ í˜•ì„± ë„ì…! ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥

### 3. FFN (Feed-Forward Network)
**Transformerì˜ ê° ì¸µ**:
```python
def ffn(x):
    h = ReLU(W1 @ x + b1)  # fâ‚
    y = W2 @ h + b2         # fâ‚‚
    return y                # fâ‚‚(fâ‚(x))
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **í•¨ìˆ˜ë¥¼ "ì…ë ¥â†’ê³„ì‚°â†’ì¶œë ¥" êµ¬ì¡°ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **ì§€ìˆ˜, ë¡œê·¸, ì„ í˜• í•¨ìˆ˜ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **í•©ì„±í•¨ìˆ˜ (f âˆ˜ g)(x)ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **ì‹ ê²½ë§ì´ ì™œ í•©ì„±í•¨ìˆ˜ì¸ì§€ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **í™œì„±í™” í•¨ìˆ˜ì˜ ì—­í• ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

1. **í•¨ìˆ˜**: ì…ë ¥ â†’ ê·œì¹™ â†’ ì¶œë ¥
2. **ì£¼ìš” í•¨ìˆ˜ë“¤**: ì„ í˜•, ì§€ìˆ˜, ë¡œê·¸, ì‹œê·¸ëª¨ì´ë“œ, ReLU
3. **í•©ì„±í•¨ìˆ˜**: f(g(x)), ì—¬ëŸ¬ í•¨ìˆ˜ë¥¼ ìˆœì°¨ ì ìš©
4. **ì‹ ê²½ë§**: ì¸µì¸µì´ ìŒ“ì¸ í•©ì„±í•¨ìˆ˜
5. **í™œì„±í™” í•¨ìˆ˜**: ë¹„ì„ í˜•ì„±ì„ ìœ„í•´ í•„ìˆ˜

### ë‹¤ìŒ í•™ìŠµ
- **Day 5**: ì§‘í•©ê³¼ ë…¼ë¦¬
  - ë°ì´í„°ì™€ í™•ë¥ ì„ ë‹¤ë£¨ëŠ” ê¸°ì´ˆ

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰
