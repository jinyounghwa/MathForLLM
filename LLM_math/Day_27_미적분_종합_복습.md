# Day 27: ë¯¸ì ë¶„ ì¢…í•© ë³µìŠµ (1ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- Day 21-26ì˜ í•µì‹¬ ê°œë… ì´ì •ë¦¬
- ë¯¸ì ë¶„ê³¼ ì‹ ê²½ë§ì˜ ì—°ê²° í™•ì¸
- Backpropagation ì „ì²´ íë¦„ ì´í•´
- ë‹¤ìŒ ë‹¨ê³„(í™•ë¥ ) ì¤€ë¹„

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"ë¯¸ì ë¶„ìœ¼ë¡œ ì‹ ê²½ë§ì„ ì´í•´í•˜ë‹¤"**

---

## ğŸ“– í•µì‹¬ ê°œë… ì •ë¦¬

### 1. ê·¹í•œê³¼ ì—°ì† (Day 21)
```
lim_{xâ†’a} f(x) = L

ì—°ì†: lim_{xâ†’a} f(x) = f(a)

â†’ ë¯¸ë¶„ ê°€ëŠ¥ì˜ ì „ì œ ì¡°ê±´
```

---

### 2. ë¯¸ë¶„ (Day 22-23)
```
f'(x) = lim_{hâ†’0} (f(x+h) - f(x)) / h

ê¸°ë³¸ ê³µì‹:
- (x^n)' = nx^{n-1}
- (e^x)' = e^x
- (ln x)' = 1/x

í™œì„±í™” í•¨ìˆ˜:
- Ïƒ'(x) = Ïƒ(x)(1-Ïƒ(x))
- tanh'(x) = 1 - tanhÂ²(x)
- ReLU'(x) = {1 if x>0, 0 otherwise}
```

---

### 3. ì—°ì‡„ë²•ì¹™ (Day 24)
```
dy/dx = (dy/du)(du/dx)

Backpropagationì˜ í•µì‹¬!

dL/dw = dL/dy Ã— dy/dz Ã— dz/dw
```

---

### 4. í¸ë¯¸ë¶„ê³¼ ê¸°ìš¸ê¸° (Day 25)
```
âˆ‚f/âˆ‚x: xì— ëŒ€í•œ í¸ë¯¸ë¶„

âˆ‡f = [âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚xâ‚™]

ê¸°ìš¸ê¸° = ê°€ì¥ ë¹ ë¥¸ ì¦ê°€ ë°©í–¥
```

---

### 5. ê²½ì‚¬í•˜ê°•ë²• (Day 26)
```
Î¸ = Î¸ - Î±âˆ‡L(Î¸)

Adam:
m = Î²â‚m + (1-Î²â‚)g
v = Î²â‚‚v + (1-Î²â‚‚)gÂ²
Î¸ = Î¸ - Î± Ã— m/âˆšv
```

---

## ğŸ”— ì „ì²´ ì—°ê²°: ì‹ ê²½ë§ í•™ìŠµ

### Forward Pass
```python
# ì…ë ¥
x = [xâ‚, xâ‚‚, ..., xâ‚™]

# Layer 1
zâ‚ = Wâ‚Â·x + bâ‚
aâ‚ = Ïƒ(zâ‚)

# Layer 2
zâ‚‚ = Wâ‚‚Â·aâ‚ + bâ‚‚
y = Ïƒ(zâ‚‚)

# ì†ì‹¤
L = (y - target)Â²
```

### Backward Pass (ì—°ì‡„ë²•ì¹™)
```python
# ì¶œë ¥ì¸µ
dL/dy = 2(y - target)
dy/dzâ‚‚ = Ïƒ'(zâ‚‚)
dL/dzâ‚‚ = dL/dy Ã— dy/dzâ‚‚

dL/dWâ‚‚ = dL/dzâ‚‚ Ã— aâ‚áµ€  (ì™¸ì )
dL/dbâ‚‚ = dL/dzâ‚‚

# ì€ë‹‰ì¸µ
dL/daâ‚ = Wâ‚‚áµ€ Ã— dL/dzâ‚‚
daâ‚/dzâ‚ = Ïƒ'(zâ‚)
dL/dzâ‚ = dL/daâ‚ âŠ™ daâ‚/dzâ‚  (ì›ì†Œë³„ ê³±)

dL/dWâ‚ = dL/dzâ‚ Ã— xáµ€
dL/dbâ‚ = dL/dzâ‚
```

### ì—…ë°ì´íŠ¸ (ê²½ì‚¬í•˜ê°•ë²•)
```python
Wâ‚ = Wâ‚ - Î± Ã— dL/dWâ‚
bâ‚ = bâ‚ - Î± Ã— dL/dbâ‚
Wâ‚‚ = Wâ‚‚ - Î± Ã— dL/dWâ‚‚
bâ‚‚ = bâ‚‚ - Î± Ã— dL/dbâ‚‚
```

---

## ğŸ’» ì¢…í•© ì‹¤ìŠµ

### ì „ì²´ íë¦„ êµ¬í˜„
```python
import numpy as np

class TwoLayerNetwork:
    """2ì¸µ ì‹ ê²½ë§ (ì™„ì „ êµ¬í˜„)"""

    def __init__(self, input_size, hidden_size, output_size):
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (He ì´ˆê¸°í™”)
        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2/input_size)
        self.b1 = np.zeros(hidden_size)
        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2/hidden_size)
        self.b2 = np.zeros(output_size)

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

    def sigmoid_derivative(self, x):
        s = self.sigmoid(x)
        return s * (1 - s)

    def forward(self, x):
        """Forward pass"""
        # Layer 1
        self.z1 = self.W1 @ x + self.b1
        self.a1 = self.sigmoid(self.z1)

        # Layer 2
        self.z2 = self.W2 @ self.a1 + self.b2
        self.y = self.sigmoid(self.z2)

        return self.y

    def backward(self, x, target):
        """Backward pass (Backpropagation)"""
        # ì¶œë ¥ì¸µ
        dL_dy = 2 * (self.y - target)
        dy_dz2 = self.sigmoid_derivative(self.z2)
        dL_dz2 = dL_dy * dy_dz2

        # ê¸°ìš¸ê¸° ê³„ì‚°
        self.dW2 = np.outer(dL_dz2, self.a1)
        self.db2 = dL_dz2

        # ì€ë‹‰ì¸µìœ¼ë¡œ ì „íŒŒ
        dL_da1 = self.W2.T @ dL_dz2
        da1_dz1 = self.sigmoid_derivative(self.z1)
        dL_dz1 = dL_da1 * da1_dz1

        # ê¸°ìš¸ê¸° ê³„ì‚°
        self.dW1 = np.outer(dL_dz1, x)
        self.db1 = dL_dz1

    def update(self, learning_rate):
        """íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ê²½ì‚¬í•˜ê°•ë²•)"""
        self.W1 -= learning_rate * self.dW1
        self.b1 -= learning_rate * self.db1
        self.W2 -= learning_rate * self.dW2
        self.b2 -= learning_rate * self.db2

    def train_step(self, x, target, learning_rate):
        """í•œ ìŠ¤í… í•™ìŠµ"""
        # Forward
        y = self.forward(x)
        loss = np.sum((y - target)**2)

        # Backward
        self.backward(x, target)

        # Update
        self.update(learning_rate)

        return loss

# ì‚¬ìš© ì˜ˆì‹œ
print("=== 2ì¸µ ì‹ ê²½ë§ ì¢…í•© ì‹¤ìŠµ ===\n")

# ë„¤íŠ¸ì›Œí¬ ìƒì„±
net = TwoLayerNetwork(input_size=3, hidden_size=4, output_size=1)

# í•™ìŠµ ë°ì´í„°
X_train = [
    np.array([0.1, 0.2, 0.3]),
    np.array([0.4, 0.5, 0.6]),
    np.array([0.7, 0.8, 0.9]),
]

y_train = [
    np.array([0.2]),
    np.array([0.6]),
    np.array([0.9]),
]

# í•™ìŠµ
epochs = 100
learning_rate = 0.1

print("í•™ìŠµ ì‹œì‘...\n")

for epoch in range(epochs):
    total_loss = 0

    for x, target in zip(X_train, y_train):
        loss = net.train_step(x, target, learning_rate)
        total_loss += loss

    if epoch % 20 == 0 or epoch == epochs - 1:
        avg_loss = total_loss / len(X_train)
        print(f"Epoch {epoch:3d}: Loss = {avg_loss:.6f}")

print("\ní•™ìŠµ ì™„ë£Œ!\n")

# í…ŒìŠ¤íŠ¸
print("í…ŒìŠ¤íŠ¸:")
for i, (x, target) in enumerate(zip(X_train, y_train)):
    y_pred = net.forward(x)
    print(f"  ì…ë ¥: {x}")
    print(f"  ëª©í‘œ: {target[0]:.4f}, ì˜ˆì¸¡: {y_pred[0]:.4f}")
    print()
```

---

## âœï¸ ìê°€ ì§„ë‹¨ ë¬¸ì œ

### ë¬¸ì œ 1: ë¯¸ë¶„
```
f(x) = 3xÂ² + 2e^x

f'(x) = ?
```

<details>
<summary>ì •ë‹µ</summary>

```
f'(x) = 6x + 2e^x
```
</details>

### ë¬¸ì œ 2: ì—°ì‡„ë²•ì¹™
```
y = (2x + 1)Â³

dy/dx = ?
```

<details>
<summary>ì •ë‹µ</summary>

```
u = 2x + 1  â†’  du/dx = 2
y = uÂ³      â†’  dy/du = 3uÂ²

dy/dx = 3uÂ² Ã— 2 = 6(2x + 1)Â²
```
</details>

### ë¬¸ì œ 3: í¸ë¯¸ë¶„
```
f(x, y) = xÂ²y + 3x

âˆ‚f/âˆ‚x = ?
âˆ‚f/âˆ‚y = ?
```

<details>
<summary>ì •ë‹µ</summary>

```
âˆ‚f/âˆ‚x = 2xy + 3
âˆ‚f/âˆ‚y = xÂ²
```
</details>

### ë¬¸ì œ 4: ê²½ì‚¬í•˜ê°•ë²•
```
f(x) = xÂ² - 4x
f'(x) = 2x - 4

ì‹œì‘: x = 0, Î± = 0.5
1ë‹¨ê³„ í›„ x = ?
```

<details>
<summary>ì •ë‹µ</summary>

```
g = 2(0) - 4 = -4
x = 0 - 0.5(-4) = 2
```
</details>

---

## ğŸ“ ë¯¸ì ë¶„ â†’ ì‹ ê²½ë§ ë§¤í•‘

| ë¯¸ì ë¶„ ê°œë… | ì‹ ê²½ë§ ì ìš© |
|------------|-------------|
| í•¨ìˆ˜ | ëª¨ë¸ (ì…ë ¥â†’ì¶œë ¥) |
| ë¯¸ë¶„ | ê¸°ìš¸ê¸° ê³„ì‚° |
| ì—°ì‡„ë²•ì¹™ | Backpropagation |
| í¸ë¯¸ë¶„ | íŒŒë¼ë¯¸í„°ë³„ ê¸°ìš¸ê¸° |
| ê¸°ìš¸ê¸° | âˆ‡L (ì†ì‹¤ì˜ ê¸°ìš¸ê¸°) |
| ê²½ì‚¬í•˜ê°•ë²• | ìµœì í™” (í•™ìŠµ) |

---

## âœ… ìµœì¢… ì²´í¬í¬ì¸íŠ¸

- [ ] **ë¯¸ë¶„ì˜ ì •ì˜ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **ì—°ì‡„ë²•ì¹™ìœ¼ë¡œ í•©ì„±í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **Backpropagationì˜ ì›ë¦¬ë¥¼ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **ê¸°ìš¸ê¸° ë²¡í„°ì˜ ì˜ë¯¸ë¥¼ ì•„ë‚˜ìš”?**

- [ ] **ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì í™”í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **ì‹ ê²½ë§ í•™ìŠµì˜ ì „ì²´ íë¦„ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

**ë¯¸ì ë¶„ì´ ì‹ ê²½ë§ì˜ ì–¸ì–´ì…ë‹ˆë‹¤!**

1. **ë¯¸ë¶„**: ë³€í™”ìœ¨
2. **ì—°ì‡„ë²•ì¹™**: Backpropagation
3. **ê¸°ìš¸ê¸°**: ìµœì  ë°©í–¥
4. **ê²½ì‚¬í•˜ê°•ë²•**: í•™ìŠµ

**ì´ì œ í™•ë¥ ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤!**

### ë‹¤ìŒ í•™ìŠµ
- **Day 28-38**: í™•ë¥ ê³¼ ì •ë³´ì´ë¡ 
  - ë¶ˆí™•ì‹¤ì„±, ì—”íŠ¸ë¡œí”¼, ì •ë³´ ì´ë“

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**ë¯¸ì ë¶„ ë§ˆìŠ¤í„°ë¥¼ ì¶•í•˜í•©ë‹ˆë‹¤!**
