# Day 37-38: í™•ë¥ ê³¼ ì •ë³´ì´ë¡  ìµœì¢… ë³µìŠµ (2ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- Day 28-36ì˜ ëª¨ë“  ê°œë… ì´ì •ë¦¬
- LLMê³¼ì˜ ì—°ê²° ì™„ì „íˆ ì´í•´í•˜ê¸°
- ë‹¤ìŒ ë‹¨ê³„ (LLM í•µì‹¬ ìˆ˜í•™) ì¤€ë¹„

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"í™•ë¥ ì—ì„œ ì •ë³´ì´ë¡ ìœ¼ë¡œ, ê·¸ë¦¬ê³  LLMìœ¼ë¡œ"**

---

## ğŸ“– ì „ì²´ ë³µìŠµ

### 1. í™•ë¥  (Day 28-31)
```
P(A): 0 ~ 1
P(A|B) = P(Aâˆ©B) / P(B)
ë² ì´ì¦ˆ: P(A|B) = P(B|A)P(A) / P(B)
ì •ê·œë¶„í¬: N(Î¼, ÏƒÂ²)
```

### 2. ì—”íŠ¸ë¡œí”¼ (Day 32-33)
```
H(X) = -Î£ P(x) log P(x)
Cross Entropy: H(P, Q) = -Î£ P log Q
Perplexity = 2^H
```

### 3. ì •ë³´ ì¸¡ì • (Day 34-36)
```
ìƒí˜¸ì •ë³´ëŸ‰: I(X;Y) = H(X) - H(X|Y)
ì •ë³´ ì´ë“: IG = H(ì „) - H(í›„)
KL Divergence: D(P||Q) = Î£ P log(P/Q)
```

---

## ğŸ’» ìµœì¢… ì¢…í•© í”„ë¡œì íŠ¸

```python
import numpy as np

class LLM_Math_Probability:
    """í™•ë¥ ê³¼ ì •ë³´ì´ë¡  í†µí•©"""

    def __init__(self):
        self.vocab = ['the', 'a', 'cat', 'dog', '.']
        self.vocab_size = len(self.vocab)

    def simulate_language_model(self):
        """ê°„ë‹¨í•œ ì–¸ì–´ ëª¨ë¸ ì‹œë®¬ë ˆì´ì…˜"""
        print("=== LLM ì‹œë®¬ë ˆì´ì…˜ ===\n")

        # Context: "the"
        # ì‹¤ì œ ë‹¤ìŒ í† í°: "cat"
        true_idx = 2

        # ëª¨ë¸ ì˜ˆì¸¡ (í™•ë¥  ë¶„í¬)
        pred_probs = np.array([0.1, 0.15, 0.5, 0.2, 0.05])

        print(f"Context: '{self.vocab[0]}'")
        print(f"ì‹¤ì œ ë‹¤ìŒ í† í°: '{self.vocab[true_idx]}'\n")

        print("ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥ :")
        for i, (word, prob) in enumerate(zip(self.vocab, pred_probs)):
            mark = " â† ì •ë‹µ" if i == true_idx else ""
            print(f"  {word}: {prob:.2f}{mark}")

        # ì—”íŠ¸ë¡œí”¼ (ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„±)
        h = -np.sum(pred_probs * np.log2(pred_probs))
        print(f"\nëª¨ë¸ ì—”íŠ¸ë¡œí”¼: {h:.4f} bits")

        # Cross Entropy Loss
        true_dist = np.zeros(self.vocab_size)
        true_dist[true_idx] = 1.0
        ce = -np.sum(true_dist * np.log(pred_probs + 1e-10))
        print(f"Cross Entropy Loss: {ce:.4f}")

        # Perplexity
        ppl = np.exp(ce)
        print(f"Perplexity: {ppl:.4f}")
        print(f"  â†’ ëª¨ë¸ì´ í‰ê·  {ppl:.1f}ê°œ ì„ íƒì§€ ì¤‘ ê³ ë¯¼\n")

        return ce, ppl

    def bayesian_update(self):
        """ë² ì´ì¦ˆ ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜"""
        print("=== ë² ì´ì¦ˆ ì—…ë°ì´íŠ¸ ===\n")

        # ì‚¬ì „ í™•ë¥ : í† í° ë¹ˆë„
        prior = np.array([0.3, 0.2, 0.15, 0.25, 0.1])

        # ê´€ì°°: context "the" ë‹¤ìŒ
        # ê°€ëŠ¥ë„ P(context|token)
        likelihood = np.array([0.1, 0.2, 0.5, 0.15, 0.05])

        # ì‚¬í›„ í™•ë¥  (ì •ê·œí™”)
        posterior = prior * likelihood
        posterior /= posterior.sum()

        print("ì‚¬ì „ í™•ë¥  (ë¹ˆë„):")
        for word, prob in zip(self.vocab, prior):
            print(f"  {word}: {prob:.2f}")

        print("\nì‚¬í›„ í™•ë¥  (context ê³ ë ¤):")
        for word, prob in zip(self.vocab, posterior):
            print(f"  {word}: {prob:.2f}")

        print("\nâ†’ 'cat'ì˜ í™•ë¥ ì´ ì¦ê°€!")

# ì‹¤í–‰
model = LLM_Math_Probability()
ce, ppl = model.simulate_language_model()
model.bayesian_update()
```

---

## ğŸ“Š ê°œë… ë§µ

```
í™•ë¥  ê¸°ì´ˆ
    â†“
ë² ì´ì¦ˆ ì •ë¦¬ â†’ LLM í† í° ì˜ˆì¸¡
    â†“
í™•ë¥ ë¶„í¬ â†’ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
    â†“
ì—”íŠ¸ë¡œí”¼ â†’ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
    â†“
Cross Entropy â†’ ì†ì‹¤ í•¨ìˆ˜
    â†“
Perplexity â†’ ëª¨ë¸ í‰ê°€
    â†“
ì •ë³´ ì´ë“ â†’ BPE í† í¬ë‚˜ì´ì €
    â†“
LLM í•™ìŠµ!
```

---

## âœ… ìµœì¢… ì²´í¬í¬ì¸íŠ¸

- [ ] **í™•ë¥ ì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í–ˆë‚˜ìš”?**
- [ ] **ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ LLMì— ì—°ê²°í•  ìˆ˜ ìˆë‚˜ìš”?**
- [ ] **ì—”íŠ¸ë¡œí”¼ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**
- [ ] **Cross Entropy ì†ì‹¤ì„ ì´í•´í–ˆë‚˜ìš”?**
- [ ] **Perplexityë¡œ ëª¨ë¸ì„ í‰ê°€í•  ìˆ˜ ìˆë‚˜ìš”?**
- [ ] **ì •ë³´ ì´ë“ê³¼ BPEì˜ ê´€ê³„ë¥¼ ì•„ë‚˜ìš”?**

---

## ğŸ“ ì§€ê¸ˆê¹Œì§€ì˜ ì—¬ì •

### ë°°ìš´ ê²ƒë“¤
1. **ê¸°ì´ˆ** (Day 1-10): ìˆ˜, í•¨ìˆ˜, ë²¡í„°
2. **ì„ í˜•ëŒ€ìˆ˜** (Day 11-20): ë‚´ì , í–‰ë ¬, PCA
3. **ë¯¸ì ë¶„** (Day 21-27): ë¯¸ë¶„, ì—°ì‡„ë²•ì¹™, ê²½ì‚¬í•˜ê°•ë²•
4. **í™•ë¥ ** (Day 28-38): í™•ë¥ , ì—”íŠ¸ë¡œí”¼, ì •ë³´ì´ë¡ 

### ë‹¤ìŒ ë‹¨ê³„
**Day 39-48: LLM í•µì‹¬ ìˆ˜í•™**
- BPE í† í¬ë‚˜ì´ì €
- Attention ë©”ì»¤ë‹ˆì¦˜
- Transformer ì•„í‚¤í…ì²˜
- ìµœì¢… í†µí•©!

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**ì´ì œ LLMì˜ í•µì‹¬ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤!**
