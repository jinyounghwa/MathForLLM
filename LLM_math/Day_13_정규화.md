# Day 13: ì •ê·œí™” (Normalization) (1.5ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- ë²¡í„° ì •ê·œí™”ì˜ ê°œë…ê³¼ ë°©ë²• ì´í•´í•˜ê¸°
- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì™„ë²½íˆ ì´í•´í•˜ê¸°
- RAG ì‹œìŠ¤í…œì—ì„œì˜ ì •ê·œí™” í™œìš© íŒŒì•…í•˜ê¸°
- L2 ì •ê·œí™”ì™€ Batch Normalization êµ¬ë³„í•˜ê¸°

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"ë²¡í„°ë¥¼ ê¸¸ì´ 1ë¡œ ë§Œë“¤ê¸°"**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. ë²¡í„° ì •ê·œí™” (Vector Normalization)

#### 1.1 ì •ì˜
**ë²¡í„°ë¥¼ ê·¸ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ê¸¸ì´ë¥¼ 1ë¡œ ë§Œë“¤ê¸°**

```
vÌ‚ = v / ||v||
```

**ê²°ê³¼**:
- ë°©í–¥ì€ ìœ ì§€
- í¬ê¸°ëŠ” 1

**ì˜ˆì‹œ**:
```
v = [3, 4]
||v|| = 5

vÌ‚ = [3/5, 4/5] = [0.6, 0.8]
||vÌ‚|| = âˆš(0.36 + 0.64) = 1 âœ“
```

#### 1.2 ë‹¨ìœ„ ë²¡í„° (Unit Vector)
**ì •ê·œí™”ëœ ë²¡í„° = ë‹¨ìœ„ ë²¡í„°**

```
||vÌ‚|| = 1
```

**í‘œì¤€ ê¸°ì € ë²¡í„°ë„ ë‹¨ìœ„ ë²¡í„°**:
```
eâƒ—â‚“ = [1, 0, 0]  â†’ ||eâƒ—â‚“|| = 1
eâƒ—áµ§ = [0, 1, 0]  â†’ ||eâƒ—áµ§|| = 1
eâƒ—_z = [0, 0, 1]  â†’ ||eâƒ—_z|| = 1
```

---

### 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Cosine Similarity) â­

#### 2.1 ì •ì˜
**ë‘ ë²¡í„°ê°€ ê°€ë¦¬í‚¤ëŠ” ë°©í–¥ì˜ ìœ ì‚¬ë„**

```
similarity = cos(Î¸) = (aâƒ— Â· bâƒ—) / (||aâƒ—|| Ã— ||bâƒ—||)
```

**ì •ê·œí™”ëœ ë²¡í„°ë¡œ í‘œí˜„**:
```
similarity = Ã¢ Â· bÌ‚
```

**ë²”ìœ„**: -1 ~ 1
- 1: ì™„ì „íˆ ê°™ì€ ë°©í–¥ (Î¸ = 0Â°)
- 0: ì§êµ (Î¸ = 90Â°)
- -1: ì™„ì „íˆ ë°˜ëŒ€ ë°©í–¥ (Î¸ = 180Â°)

#### 2.2 ì™œ ì½”ì‚¬ì¸ì„ ì‚¬ìš©í•˜ëŠ”ê°€?

**ë¬¸ì œ**: ìœ í´ë¦¬ë“œ ê±°ë¦¬ëŠ” ë²¡í„° í¬ê¸°ì— ë¯¼ê°
```
vâ‚ = [1, 0]
vâ‚‚ = [2, 0]  (ê°™ì€ ë°©í–¥, 2ë°° ê¸¸ì´)

ê±°ë¦¬ = ||vâ‚‚ - vâ‚|| = 1 (ë‹¤ë¥´ë‹¤ê³  íŒë‹¨)
```

**í•´ê²°**: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë°©í–¥ë§Œ ë¹„êµ
```
cos(Î¸) = 1 (ê°™ì€ ë°©í–¥!)
```

#### 2.3 ì˜ˆì‹œ
```
aâƒ— = [1, 2, 3]
bâƒ— = [2, 4, 6]  (aì˜ 2ë°°)

cos(Î¸) = (2 + 8 + 18) / (âˆš14 Ã— âˆš56)
       = 28 / 28
       = 1 (ì™„ì „íˆ ê°™ì€ ë°©í–¥)

câƒ— = [1, 0, 0]
cos(Î¸_ac) = 1 / âˆš14 â‰ˆ 0.27 (ë‹¤ë¥¸ ë°©í–¥)
```

---

### 3. L2 ì •ê·œí™” (L2 Normalization)

#### 3.1 ì •ì˜
**ê° ë²¡í„°ë¥¼ L2 ë…¸ë¦„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°**

```
x_normalized = x / ||x||â‚‚
```

**íŠ¹ì§•**:
- ê° ìƒ˜í”Œì„ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™”
- ë°©í–¥ë§Œ ì¤‘ìš”í•œ ê²½ìš° ì‚¬ìš©

#### 3.2 LLM ì„ë² ë”©ì—ì„œì˜ ì‚¬ìš©
```python
# ì„ë² ë”© ë²¡í„°
embedding = [0.1, 0.2, ..., 0.5]  # 512ì°¨ì›

# L2 ì •ê·œí™”
norm = ||embedding||
embedding_normalized = embedding / norm

# ì´ì œ ||embedding_normalized|| = 1
```

**ì´ì **:
1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë‹¨ìˆœí™”
   ```
   similarity = emb1_norm Â· emb2_norm
   ```

2. ë²¡í„° í¬ê¸° ë¬´ì‹œ, ìˆœìˆ˜ ë°©í–¥ ë¹„êµ

---

### 4. RAGì—ì„œì˜ ì •ê·œí™”

#### 4.1 RAG íŒŒì´í”„ë¼ì¸
```
1. ë¬¸ì„œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
2. ì„ë² ë”©ì„ L2 ì •ê·œí™”
3. ë²¡í„° DBì— ì €ì¥
4. ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
5. ì¿¼ë¦¬ ì„ë² ë”© L2 ì •ê·œí™”
6. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
```

#### 4.2 ì˜ˆì‹œ
```python
# ë¬¸ì„œ ì„ë² ë”©
doc1 = [0.1, 0.2, 0.3, ...]  # ê¸¸ì´: 1.5
doc2 = [0.2, 0.4, 0.6, ...]  # ê¸¸ì´: 3.0 (doc1ì˜ 2ë°°)

# ì •ê·œí™”í•˜ì§€ ì•Šìœ¼ë©´
# doc2ê°€ ë” ê¸¸ì–´ì„œ ìœ ì‚¬ë„ ê³„ì‚°ì— ì˜í–¥

# ì •ê·œí™”í•˜ë©´
doc1_norm = doc1 / ||doc1||  # ê¸¸ì´: 1
doc2_norm = doc2 / ||doc2||  # ê¸¸ì´: 1

# ì´ì œ ìˆœìˆ˜í•˜ê²Œ ë°©í–¥ë§Œ ë¹„êµ!
```

---

### 5. Batch Normalization vs Layer Normalization

#### 5.1 Batch Normalization
**ë°°ì¹˜ ì°¨ì›ì—ì„œ ì •ê·œí™”**

```
x_norm = (x - mean_batch) / std_batch
```

**íŠ¹ì§•**:
- ê° featureë³„ë¡œ ë°°ì¹˜ ì „ì²´ì˜ í‰ê· /í‘œì¤€í¸ì°¨ ì‚¬ìš©
- CNNì—ì„œ ì£¼ë¡œ ì‚¬ìš©

#### 5.2 Layer Normalization
**feature ì°¨ì›ì—ì„œ ì •ê·œí™”**

```
x_norm = (x - mean_features) / std_features
```

**íŠ¹ì§•**:
- ê° ìƒ˜í”Œë³„ë¡œ ëª¨ë“  featureì˜ í‰ê· /í‘œì¤€í¸ì°¨ ì‚¬ìš©
- Transformerì—ì„œ ì‚¬ìš©

**ë¹„êµ**:
```
ì…ë ¥: [batch_size, features]

Batch Norm: ê° featureì— ëŒ€í•´ batch ì „ì²´ ì •ê·œí™”
Layer Norm: ê° ìƒ˜í”Œì— ëŒ€í•´ feature ì „ì²´ ì •ê·œí™”
```

---

## ğŸ’» Python ì‹¤ìŠµ

### ì‹¤ìŠµ 1: ë²¡í„° ì •ê·œí™”
```python
import numpy as np

def normalize(v):
    """L2 ì •ê·œí™”"""
    norm = np.linalg.norm(v)
    if norm == 0:
        return v
    return v / norm

# ì˜ˆì‹œ ë²¡í„°ë“¤
vectors = {
    'v1': np.array([3, 4]),
    'v2': np.array([1, 1, 1]),
    'v3': np.array([0, 0, 5]),
}

print("=== ë²¡í„° ì •ê·œí™” ===\n")

for name, v in vectors.items():
    v_norm = normalize(v)
    norm_before = np.linalg.norm(v)
    norm_after = np.linalg.norm(v_norm)

    print(f"{name} = {v}")
    print(f"  ì •ê·œí™” ì „ í¬ê¸°: {norm_before:.4f}")
    print(f"  ì •ê·œí™” í›„: {v_norm}")
    print(f"  ì •ê·œí™” í›„ í¬ê¸°: {norm_after:.4f}")
    print()
```

### ì‹¤ìŠµ 2: ì½”ì‚¬ì¸ ìœ ì‚¬ë„
```python
import numpy as np

def cosine_similarity(a, b):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    dot = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    return dot / (norm_a * norm_b)

def cosine_similarity_normalized(a_norm, b_norm):
    """ì •ê·œí™”ëœ ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë‹¨ìˆœ ë‚´ì )"""
    return np.dot(a_norm, b_norm)

# ì˜ˆì‹œ ë²¡í„°
v1 = np.array([1, 2, 3])
v2 = np.array([2, 4, 6])  # v1ì˜ 2ë°°
v3 = np.array([1, 0, 0])  # ë‹¤ë¥¸ ë°©í–¥

print("=== ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ===\n")

print(f"v1 = {v1}")
print(f"v2 = {v2} (v1ì˜ 2ë°°)")
print(f"v3 = {v3}")
print()

# ë°©ë²• 1: ì§ì ‘ ê³„ì‚°
sim_12 = cosine_similarity(v1, v2)
sim_13 = cosine_similarity(v1, v3)

print("ë°©ë²• 1: ì§ì ‘ ê³„ì‚°")
print(f"  cos(v1, v2) = {sim_12:.4f}")
print(f"  cos(v1, v3) = {sim_13:.4f}")
print()

# ë°©ë²• 2: ì •ê·œí™” í›„ ë‚´ì 
v1_norm = v1 / np.linalg.norm(v1)
v2_norm = v2 / np.linalg.norm(v2)
v3_norm = v3 / np.linalg.norm(v3)

sim_12_norm = cosine_similarity_normalized(v1_norm, v2_norm)
sim_13_norm = cosine_similarity_normalized(v1_norm, v3_norm)

print("ë°©ë²• 2: ì •ê·œí™” í›„ ë‚´ì ")
print(f"  v1_norm Â· v2_norm = {sim_12_norm:.4f}")
print(f"  v1_norm Â· v3_norm = {sim_13_norm:.4f}")
print()

print("âœ… v2ëŠ” v1ì˜ 2ë°°ì§€ë§Œ, ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” 1 (ê°™ì€ ë°©í–¥!)")
```

### ì‹¤ìŠµ 3: RAG ë¬¸ì„œ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
```python
import numpy as np

# ì‹œë®¬ë ˆì´ì…˜: ê°„ë‹¨í•œ ë¬¸ì„œ ì„ë² ë”©
np.random.seed(42)
dim = 128

# ë¬¸ì„œ ì„ë² ë”© (ì •ê·œí™”ë˜ì§€ ì•ŠìŒ)
documents = {
    "Python ê¸°ì´ˆ": np.random.randn(dim) + np.array([1, 1] + [0]*(dim-2)),
    "Python ê³ ê¸‰": np.random.randn(dim) + np.array([1.2, 1.1] + [0]*(dim-2)),
    "Java ê¸°ì´ˆ": np.random.randn(dim) + np.array([0.8, -0.5] + [0]*(dim-2)),
    "ìš”ë¦¬ ë ˆì‹œí”¼": np.random.randn(dim) + np.array([-1, 0.5] + [0]*(dim-2)),
    "ìš´ë™ ë°©ë²•": np.random.randn(dim) + np.array([-0.5, 1] + [0]*(dim-2))
}

# ì¿¼ë¦¬
query = "Python í”„ë¡œê·¸ë˜ë° ë°°ìš°ê¸°"
query_emb = np.random.randn(dim) + np.array([1.1, 0.9] + [0]*(dim-2))

print("=== RAG ë¬¸ì„œ ê²€ìƒ‰ ===")
print(f"ì¿¼ë¦¬: '{query}'")
print(f"ì„ë² ë”© ì°¨ì›: {dim}")
print()

# L2 ì •ê·œí™”
query_norm = query_emb / np.linalg.norm(query_emb)
docs_norm = {name: emb / np.linalg.norm(emb) for name, emb in documents.items()}

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
similarities = {}
for doc_name, doc_emb_norm in docs_norm.items():
    sim = np.dot(query_norm, doc_emb_norm)
    similarities[doc_name] = sim

# ì •ë ¬ (ìœ ì‚¬ë„ ë†’ì€ ìˆœ)
sorted_docs = sorted(similarities.items(), key=lambda x: x[1], reverse=True)

print("ê²€ìƒ‰ ê²°ê³¼ (ìœ ì‚¬ë„ ìˆœ):")
for rank, (doc_name, sim) in enumerate(sorted_docs, 1):
    print(f"  {rank}. {doc_name:20s}: {sim:.4f}")

print()
print("âœ… 'Python' ê´€ë ¨ ë¬¸ì„œë“¤ì´ ìƒìœ„ì— ë­í¬!")
```

### ì‹¤ìŠµ 4: ì •ê·œí™” ì‹œê°í™”
```python
import numpy as np
import matplotlib.pyplot as plt

# ì—¬ëŸ¬ ë²¡í„° ìƒì„±
np.random.seed(42)
vectors = np.random.randn(10, 2) * 2

# ì •ê·œí™”
vectors_normalized = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)

# ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(16, 8))

for ax, vecs, title in zip(axes, [vectors, vectors_normalized],
                             ['Original Vectors', 'Normalized Vectors (Unit Circle)']):
    ax.set_xlim(-3, 3)
    ax.set_ylim(-3, 3)
    ax.set_aspect('equal')
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0, color='k', linewidth=0.5)
    ax.axvline(x=0, color='k', linewidth=0.5)

    # ë²¡í„° ê·¸ë¦¬ê¸°
    for v in vecs:
        ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,
                  width=0.005, alpha=0.7)
        ax.plot(v[0], v[1], 'ro', markersize=5)

    # ì •ê·œí™”ëœ ê²½ìš° ë‹¨ìœ„ì› ê·¸ë¦¬ê¸°
    if 'Normalized' in title:
        circle = plt.Circle((0, 0), 1, fill=False, color='blue',
                            linewidth=2, linestyle='--', label='Unit Circle')
        ax.add_patch(circle)
        ax.legend()

    ax.set_xlabel('x', fontsize=12)
    ax.set_ylabel('y', fontsize=12)
    ax.set_title(title, fontsize=14)

plt.tight_layout()
plt.savefig('normalization_visualization.png', dpi=150, bbox_inches='tight')
print("ì •ê·œí™” ì‹œê°í™” ì €ì¥ ì™„ë£Œ!")
```

### ì‹¤ìŠµ 5: Layer Normalization
```python
import numpy as np

def layer_norm(x, epsilon=1e-6):
    """Layer Normalization"""
    mean = np.mean(x, axis=-1, keepdims=True)
    std = np.std(x, axis=-1, keepdims=True)
    return (x - mean) / (std + epsilon)

# ì˜ˆì‹œ: [batch_size, features]
batch_size, features = 3, 4
x = np.random.randn(batch_size, features) * 2 + 5

print("=== Layer Normalization ===")
print(f"ì…ë ¥ shape: {x.shape}")
print(f"ì…ë ¥:\n{x}")
print()

# Layer Normalization ì ìš©
x_norm = layer_norm(x)

print(f"ì •ê·œí™” í›„:\n{x_norm}")
print()

# ê° ìƒ˜í”Œì˜ í†µê³„
for i in range(batch_size):
    print(f"ìƒ˜í”Œ {i+1}:")
    print(f"  ì •ê·œí™” ì „: í‰ê· ={np.mean(x[i]):.4f}, í‘œì¤€í¸ì°¨={np.std(x[i]):.4f}")
    print(f"  ì •ê·œí™” í›„: í‰ê· ={np.mean(x_norm[i]):.4f}, í‘œì¤€í¸ì°¨={np.std(x_norm[i]):.4f}")
print()

print("âœ… ì •ê·œí™” í›„ ê° ìƒ˜í”Œì˜ í‰ê· â‰ˆ0, í‘œì¤€í¸ì°¨â‰ˆ1")
```

---

## âœï¸ ì† ê³„ì‚° ì—°ìŠµ

### ì—°ìŠµ 1: ë²¡í„° ì •ê·œí™”
```
v = [3, 4]

Step 1: í¬ê¸° ê³„ì‚°
||v|| = âˆš(9 + 16) = 5

Step 2: ì •ê·œí™”
vÌ‚ = [3/5, 4/5] = [0.6, 0.8]

ê²€ì¦: ||vÌ‚|| = âˆš(0.36 + 0.64) = 1 âœ“
```

### ì—°ìŠµ 2: ì½”ì‚¬ì¸ ìœ ì‚¬ë„
```
aâƒ— = [1, 2]
bâƒ— = [2, 1]

Step 1: ë‚´ì 
aâƒ— Â· bâƒ— = 2 + 2 = 4

Step 2: í¬ê¸°
||aâƒ—|| = âˆš5
||bâƒ—|| = âˆš5

Step 3: ì½”ì‚¬ì¸ ìœ ì‚¬ë„
cos(Î¸) = 4 / (âˆš5 Ã— âˆš5) = 4/5 = 0.8
```

### ì—°ìŠµ 3: ì •ê·œí™” í›„ ë‚´ì 
```
aâƒ— = [3, 4], bâƒ— = [5, 12]

Step 1: ì •ê·œí™”
Ã¢ = [3/5, 4/5] = [0.6, 0.8]
bÌ‚ = [5/13, 12/13] â‰ˆ [0.385, 0.923]

Step 2: ë‚´ì  (= ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
Ã¢ Â· bÌ‚ = 0.6Ã—0.385 + 0.8Ã—0.923
      = 0.231 + 0.738
      â‰ˆ 0.97
```

---

## ğŸ”— LLM ì—°ê²°ì 

### 1. Sentence Embedding + RAG
```python
# 1. ë¬¸ì„œ ì„ë² ë”© & ì •ê·œí™”
doc_emb = model.encode("ë¬¸ì„œ ë‚´ìš©")
doc_emb_norm = doc_emb / ||doc_emb||

# 2. ì €ì¥
vector_db.store(doc_emb_norm)

# 3. ê²€ìƒ‰
query_emb = model.encode("ì§ˆë¬¸")
query_emb_norm = query_emb / ||query_emb||

# 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë‹¨ìˆœ ë‚´ì )
scores = query_emb_norm @ doc_embs_norm.T
top_k = argmax(scores, k=5)
```

### 2. Attentionì—ì„œì˜ ì •ê·œí™”
```python
# Scaled Dot-Product Attention
scores = Q @ K.T / sqrt(d_k)

# âˆšd_kë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ :
# - ë‚´ì  ê°’ì´ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒ ë°©ì§€
# - Softmaxì˜ ìˆ˜ì¹˜ ì•ˆì •ì„± í–¥ìƒ
```

### 3. Layer Normalization in Transformer
```python
# Transformerì˜ ê° ì„œë¸Œì¸µ í›„
x = x + sublayer(x)  # Residual Connection
x = LayerNorm(x)     # Layer Normalization
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **ë²¡í„°ë¥¼ ì •ê·œí™”í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **ì •ê·œí™”ëœ ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì ì„ì„ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **RAGì—ì„œ ì™œ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **L2 ì •ê·œí™”ì™€ Layer Normalizationì˜ ì°¨ì´ë¥¼ ì´í•´í–ˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

1. **ì •ê·œí™”**: vÌ‚ = v / ||v||, í¬ê¸°ë¥¼ 1ë¡œ
2. **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**: cos(Î¸) = (aâƒ— Â· bâƒ—) / (||aâƒ—||||bâƒ—||)
3. **ì •ê·œí™” í›„**: cos(Î¸) = Ã¢ Â· bÌ‚ (ë‹¨ìˆœ ë‚´ì !)
4. **RAG**: ì„ë² ë”© ì •ê·œí™” â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
5. **Layer Norm**: Transformerì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

### ë‹¤ìŒ í•™ìŠµ
- **Day 14**: í–‰ë ¬ê³¼ í–‰ë ¬ ê³±ì…ˆ
  - ì‹ ê²½ë§ì˜ y = Wx + b

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**ì •ê·œí™”ëŠ” RAGì™€ ì„ë² ë”© ê²€ìƒ‰ì˜ í•µì‹¬ì…ë‹ˆë‹¤!**
