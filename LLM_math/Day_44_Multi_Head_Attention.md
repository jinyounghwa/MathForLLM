# Day 44: Multi-Head Attention (1.5ì‹œê°„) â­

## ğŸ“š í•™ìŠµ ëª©í‘œ
- **Multi-Head Attentionì˜ ì›ë¦¬ ì™„ë²½íˆ ì´í•´í•˜ê¸°**
- ì—¬ëŸ¬ í—¤ë“œê°€ ì™œ í•„ìš”í•œì§€ íŒŒì•…í•˜ê¸°
- Parallel Attention ê³„ì‚° ì´í•´í•˜ê¸°
- ì „ì²´ íë¦„ êµ¬í˜„í•˜ê¸°

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ë³´ê¸°"**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. ì™œ Multi-Headì¸ê°€?

**ë¬¸ì œ**:
```
í•˜ë‚˜ì˜ Attention:
- í•œ ê°€ì§€ íŒ¨í„´ë§Œ í•™ìŠµ
- "ê³ ì–‘ì´ê°€ ì¥ë¥¼ ì¡ì•˜ë‹¤"
  â†’ "ê³ ì–‘ì´ - ì¡ì•˜ë‹¤" (ì£¼ì–´-ì„œìˆ ì–´)ë§Œ í¬ì°©

ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ë™ì‹œì— íŒŒì•…í•˜ë ¤ë©´?
```

**í•´ê²°ì±…**:
```
ì—¬ëŸ¬ ê°œì˜ Attentionì„ ë³‘ë ¬ë¡œ ì‹¤í–‰!

Head 1: ì£¼ì–´-ì„œìˆ ì–´ ê´€ê³„
Head 2: ëª©ì ì–´-ë™ì‚¬ ê´€ê³„
Head 3: í˜•ìš©ì‚¬-ëª…ì‚¬ ê´€ê³„
...
```

---

### 2. Multi-Head Attention ê³µì‹

**ì „ì²´ ê³¼ì •**:
```
1. ì…ë ¥ì„ hê°œì˜ í—¤ë“œë¡œ ë¶„í• 
2. ê° í—¤ë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ Attention
3. ê²°ê³¼ë¥¼ ì—°ê²°(concat)
4. ìµœì¢… ì„ í˜• ë³€í™˜
```

**ìˆ˜ì‹**:
```
MultiHead(Q, K, V) = Concat(headâ‚, ..., headâ‚•) W^O

head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i)

W^Q_i, W^K_i, W^V_i: ê° í—¤ë“œì˜ íˆ¬ì˜ í–‰ë ¬
W^O: ì¶œë ¥ íˆ¬ì˜ í–‰ë ¬
```

---

### 3. ì°¨ì› ê´€ë¦¬

**í•µì‹¬ ì•„ì´ë””ì–´**:
```
ì „ì²´ ëª¨ë¸ ì°¨ì›: d_model = 512
í—¤ë“œ ìˆ˜: h = 8

ê° í—¤ë“œì˜ ì°¨ì›: d_k = d_v = d_model / h = 64

â†’ ê³„ì‚°ëŸ‰ì€ ê±°ì˜ ë™ì¼í•˜ë©´ì„œ ë‹¤ì–‘í•œ í‘œí˜„ í•™ìŠµ!
```

**ì°¨ì› ë³€í™”**:
```
ì…ë ¥: (batch, seq_len, d_model)

1. íˆ¬ì˜: (batch, seq_len, d_model) â†’ (batch, seq_len, d_k) Ã— h
2. Reshape: (batch, h, seq_len, d_k)
3. Attention: ê° í—¤ë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ
4. Concat: (batch, seq_len, h Ã— d_k) = (batch, seq_len, d_model)
5. ì¶œë ¥ íˆ¬ì˜: (batch, seq_len, d_model)
```

---

### 4. ì˜ˆì œ: 2-Head Attention

**ì„¤ì •**:
```
seq_len = 3 (ë‹¨ì–´ 3ê°œ)
d_model = 4
h = 2 (í—¤ë“œ 2ê°œ)
d_k = d_model / h = 2
```

**ì…ë ¥**:
```
X = [[1, 2, 3, 4],    # ë‹¨ì–´ 1
     [5, 6, 7, 8],    # ë‹¨ì–´ 2
     [9, 10, 11, 12]] # ë‹¨ì–´ 3

(3, 4)
```

**Head 1**:
```
W^Q_1: (4, 2) - ì²˜ìŒ 2ì°¨ì› íˆ¬ì˜
Q_1 = X @ W^Q_1 â†’ (3, 2)

K_1 = X @ W^K_1 â†’ (3, 2)
V_1 = X @ W^V_1 â†’ (3, 2)

output_1 = Attention(Q_1, K_1, V_1) â†’ (3, 2)
```

**Head 2**:
```
W^Q_2: (4, 2) - ë‹¤ë¥¸ 2ì°¨ì› íˆ¬ì˜
Q_2 = X @ W^Q_2 â†’ (3, 2)

K_2 = X @ W^K_2 â†’ (3, 2)
V_2 = X @ W^V_2 â†’ (3, 2)

output_2 = Attention(Q_2, K_2, V_2) â†’ (3, 2)
```

**Concat**:
```
output = [output_1 | output_2] â†’ (3, 4)
```

**ìµœì¢…**:
```
final_output = output @ W^O â†’ (3, 4)
```

---

## ğŸ’» Python ì‹¤ìŠµ

### ì‹¤ìŠµ 1: Multi-Head Attention êµ¬í˜„
```python
import numpy as np

def scaled_dot_product_attention(Q, K, V, mask=None):
    """Scaled Dot-Product Attention"""
    d_k = K.shape[-1]
    scores = (Q @ K.T) / np.sqrt(d_k)

    if mask is not None:
        scores += (mask * -1e9)

    attn_weights = np.exp(scores)
    attn_weights /= attn_weights.sum(axis=-1, keepdims=True)

    output = attn_weights @ V
    return output, attn_weights


class MultiHeadAttention:
    """Multi-Head Attention"""

    def __init__(self, d_model, num_heads):
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.W_Q = np.random.randn(num_heads, d_model, self.d_k) * 0.1
        self.W_K = np.random.randn(num_heads, d_model, self.d_k) * 0.1
        self.W_V = np.random.randn(num_heads, d_model, self.d_k) * 0.1
        self.W_O = np.random.randn(d_model, d_model) * 0.1

    def forward(self, Q, K, V, mask=None):
        """
        Args:
            Q, K, V: (seq_len, d_model)
            mask: optional

        Returns:
            output: (seq_len, d_model)
            attn_weights: list of (seq_len, seq_len) for each head
        """
        seq_len = Q.shape[0]

        # ê° í—¤ë“œë³„ ì¶œë ¥ ì €ì¥
        head_outputs = []
        all_attn_weights = []

        for i in range(self.num_heads):
            # íˆ¬ì˜
            Q_i = Q @ self.W_Q[i]  # (seq_len, d_k)
            K_i = K @ self.W_K[i]  # (seq_len, d_k)
            V_i = V @ self.W_V[i]  # (seq_len, d_k)

            # Attention
            head_output, attn_weights = scaled_dot_product_attention(
                Q_i, K_i, V_i, mask
            )

            head_outputs.append(head_output)
            all_attn_weights.append(attn_weights)

        # Concat
        concat_output = np.concatenate(head_outputs, axis=-1)  # (seq_len, d_model)

        # ìµœì¢… íˆ¬ì˜
        output = concat_output @ self.W_O  # (seq_len, d_model)

        return output, all_attn_weights


# ì‚¬ìš© ì˜ˆ
print("=== Multi-Head Attention ===\n")

# ì„¤ì •
seq_len = 4
d_model = 8
num_heads = 2

# ì…ë ¥
X = np.random.randn(seq_len, d_model)

print(f"ì…ë ¥ í˜•íƒœ: {X.shape}")
print(f"d_model: {d_model}")
print(f"num_heads: {num_heads}")
print(f"d_k (ê° í—¤ë“œ): {d_model // num_heads}\n")

# Multi-Head Attention
mha = MultiHeadAttention(d_model, num_heads)
output, attn_weights = mha.forward(X, X, X)

print(f"ì¶œë ¥ í˜•íƒœ: {output.shape}")
print(f"Attention í—¤ë“œ ìˆ˜: {len(attn_weights)}")
print(f"ê° í—¤ë“œ attention í˜•íƒœ: {attn_weights[0].shape}\n")

# ê° í—¤ë“œì˜ attention í™•ì¸
for i, attn in enumerate(attn_weights):
    print(f"Head {i+1} attention weights:")
    print(attn)
    print()
```

### ì‹¤ìŠµ 2: í—¤ë“œë³„ Attention íŒ¨í„´ ì‹œê°í™”
```python
import numpy as np
import matplotlib.pyplot as plt

# ë¬¸ì¥
words = ["The", "cat", "sat", "mat"]
seq_len = len(words)
d_model = 8
num_heads = 4

# ì…ë ¥
np.random.seed(42)
X = np.random.randn(seq_len, d_model)

# Multi-Head Attention
mha = MultiHeadAttention(d_model, num_heads)
output, attn_weights = mha.forward(X, X, X)

# ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
axes = axes.flatten()

for i, attn in enumerate(attn_weights):
    ax = axes[i]

    im = ax.imshow(attn, cmap='Blues', aspect='auto')
    ax.set_xticks(range(seq_len))
    ax.set_yticks(range(seq_len))
    ax.set_xticklabels(words)
    ax.set_yticklabels(words)
    ax.set_xlabel('Key')
    ax.set_ylabel('Query')
    ax.set_title(f'Head {i+1} Attention')

    # ê°’ í‘œì‹œ
    for row in range(seq_len):
        for col in range(seq_len):
            text = ax.text(col, row, f'{attn[row, col]:.2f}',
                          ha="center", va="center",
                          color="black", fontsize=9)

    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

plt.tight_layout()
plt.savefig('multi_head_attention.png', dpi=150)
print("Multi-Head Attention ì‹œê°í™” ì €ì¥ ì™„ë£Œ!")
print("\nâ†’ ê° í—¤ë“œê°€ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµí•¨!")
```

### ì‹¤ìŠµ 3: í—¤ë“œ ìˆ˜ì— ë”°ë¥¸ ë¹„êµ
```python
import numpy as np

def test_num_heads(seq_len, d_model, num_heads_list):
    """ë‹¤ì–‘í•œ í—¤ë“œ ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("=== í—¤ë“œ ìˆ˜ ë¹„êµ ===\n")

    X = np.random.randn(seq_len, d_model)

    for num_heads in num_heads_list:
        if d_model % num_heads != 0:
            print(f"num_heads={num_heads}: ë¶ˆê°€ëŠ¥ (d_model % num_heads != 0)")
            continue

        mha = MultiHeadAttention(d_model, num_heads)
        output, _ = mha.forward(X, X, X)

        d_k = d_model // num_heads
        total_params = (
            num_heads * d_model * d_k * 3 +  # W_Q, W_K, W_V
            d_model * d_model                  # W_O
        )

        print(f"num_heads={num_heads}:")
        print(f"  d_k (ê° í—¤ë“œ ì°¨ì›): {d_k}")
        print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {total_params}")
        print(f"  ì¶œë ¥ í˜•íƒœ: {output.shape}")
        print()

# í…ŒìŠ¤íŠ¸
test_num_heads(seq_len=4, d_model=64, num_heads_list=[1, 2, 4, 8, 16])
```

### ì‹¤ìŠµ 4: Self vs Cross Attention
```python
import numpy as np

# Self-Attention: Q=K=V (ê°™ì€ ì‹œí€€ìŠ¤)
print("=== Self-Attention vs Cross-Attention ===\n")

# Encoder ì¶œë ¥ (ë‹¤ë¥¸ ë¬¸ì¥)
encoder_output = np.random.randn(5, 8)  # 5 ë‹¨ì–´

# Decoder ì…ë ¥ (ìƒì„± ì¤‘ì¸ ë¬¸ì¥)
decoder_input = np.random.randn(3, 8)   # 3 ë‹¨ì–´

mha = MultiHeadAttention(d_model=8, num_heads=2)

# 1. Self-Attention (Decoder)
print("1. Decoder Self-Attention:")
self_output, self_attn = mha.forward(decoder_input, decoder_input, decoder_input)
print(f"   Query=Key=Value: {decoder_input.shape}")
print(f"   ì¶œë ¥: {self_output.shape}")
print(f"   Attention: {self_attn[0].shape} (3x3)\n")

# 2. Cross-Attention (Encoder-Decoder)
print("2. Encoder-Decoder Cross-Attention:")
cross_output, cross_attn = mha.forward(decoder_input, encoder_output, encoder_output)
print(f"   Query (Decoder): {decoder_input.shape}")
print(f"   Key,Value (Encoder): {encoder_output.shape}")
print(f"   ì¶œë ¥: {cross_output.shape}")
print(f"   Attention: {cross_attn[0].shape} (3x5)")
print("   â†’ Decoderê°€ Encoderì˜ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë³¼ ìˆ˜ ìˆìŒ!")
```

---

## âœï¸ ì† ê³„ì‚° ì—°ìŠµ

### ì—°ìŠµ: 2-Head Attention (ê°„ì†Œí™”)
```
ì…ë ¥: X = [1, 2]  (1ê°œ ë‹¨ì–´, d_model=2)

h = 2, d_k = 1

Head 1:
  W^Q_1 = [0.5], W^K_1 = [0.5], W^V_1 = [0.5]
  Q_1 = [1,2] @ [0.5] = 1.5
  K_1 = 1.5, V_1 = 1.5
  output_1 = Attention(1.5, 1.5, 1.5) = 1.5

Head 2:
  W^Q_2 = [0.3], W^K_2 = [0.3], W^V_2 = [0.3]
  Q_2 = [1,2] @ [0.3] = 0.9
  K_2 = 0.9, V_2 = 0.9
  output_2 = 0.9

Concat: [1.5, 0.9]

ìµœì¢…: [1.5, 0.9] @ W^O
```

---

## ğŸ”— LLM ì—°ê²°ì 

### 1. GPT-3 êµ¬ì„±
```
- 96 layers
- d_model = 12288
- num_heads = 96
- d_k = 128

â†’ ê° ì¸µì—ì„œ 96ê°œì˜ ë‹¤ë¥¸ ê´€ì !
```

### 2. BERT
```
- 12 layers (Base) / 24 (Large)
- d_model = 768 (Base) / 1024 (Large)
- num_heads = 12 / 16
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **Multi-Headê°€ ì™œ í•„ìš”í•œì§€ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **ì°¨ì› ê´€ë¦¬ (d_model / h = d_k)ë¥¼ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **Self vs Cross Attentionì˜ ì°¨ì´ë¥¼ ì•„ë‚˜ìš”?**

- [ ] **êµ¬í˜„í•  ìˆ˜ ìˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

1. **Multi-Head**: ì—¬ëŸ¬ ê´€ì  ë³‘ë ¬ í•™ìŠµ
2. **ì°¨ì› ë¶„í• **: d_k = d_model / h
3. **ê³¼ì •**: íˆ¬ì˜ â†’ Attention Ã— h â†’ Concat â†’ íˆ¬ì˜
4. **íš¨ê³¼**: ë‹¤ì–‘í•œ íŒ¨í„´ í¬ì°©, í‘œí˜„ë ¥ í–¥ìƒ

### ë‹¤ìŒ í•™ìŠµ
- **Day 45-46**: Transformer ì•„í‚¤í…ì²˜
  - ì „ì²´ êµ¬ì¡° ì™„ì„±!

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**Multi-Head Attentionì€ Transformerì˜ í•µì‹¬ í˜ì‹ ì…ë‹ˆë‹¤!**
