# Day 48: ìµœì¢… í”„ë¡œì íŠ¸ - Tiny Language Model (3ì‹œê°„) â­

## ğŸ“š í”„ë¡œì íŠ¸ ëª©í‘œ
- **ì™„ì „í•œ ì–¸ì–´ ëª¨ë¸ì„ NumPyë¡œ êµ¬í˜„í•˜ê¸°**
- í•™ìŠµê³¼ ìƒì„± ëª¨ë‘ êµ¬í˜„í•˜ê¸°
- ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ê°œë… ì ìš©í•˜ê¸°

---

## ğŸ¯ í”„ë¡œì íŠ¸
**"ë‚˜ë§Œì˜ ì‘ì€ GPT ë§Œë“¤ê¸°"**

---

## ğŸ’» ìµœì¢… í”„ë¡œì íŠ¸ ì½”ë“œ

```python
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

# ============================================
# 1. í† í¬ë‚˜ì´ì € (ê°„ë‹¨í•œ ë‹¨ì–´ ê¸°ë°˜)
# ============================================

class SimpleTokenizer:
    """ê°„ë‹¨í•œ ë‹¨ì–´ í† í¬ë‚˜ì´ì €"""

    def __init__(self):
        self.word_to_id = {}
        self.id_to_word = {}
        self.vocab_size = 0

    def fit(self, corpus):
        """ì–´íœ˜ êµ¬ì¶•"""
        words = corpus.lower().split()
        unique_words = sorted(set(words))

        # íŠ¹ìˆ˜ í† í°
        self.word_to_id = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}
        self.id_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}

        # ì¼ë°˜ í† í°
        for word in unique_words:
            if word not in self.word_to_id:
                idx = len(self.word_to_id)
                self.word_to_id[word] = idx
                self.id_to_word[idx] = word

        self.vocab_size = len(self.word_to_id)

    def encode(self, text):
        """í…ìŠ¤íŠ¸ â†’ í† í° ID"""
        words = text.lower().split()
        return [self.word_to_id.get(w, 1) for w in words]

    def decode(self, token_ids):
        """í† í° ID â†’ í…ìŠ¤íŠ¸"""
        words = [self.id_to_word.get(idx, '<UNK>') for idx in token_ids]
        return ' '.join(words)


# ============================================
# 2. Transformer ì»´í¬ë„ŒíŠ¸
# ============================================

def softmax(x, axis=-1):
    """ìˆ˜ì¹˜ ì•ˆì •ì ì¸ Softmax"""
    exp_x = np.exp(x - x.max(axis=axis, keepdims=True))
    return exp_x / exp_x.sum(axis=axis, keepdims=True)

def scaled_dot_product_attention(Q, K, V, mask=None):
    """Scaled Dot-Product Attention"""
    d_k = K.shape[-1]
    scores = (Q @ K.T) / np.sqrt(d_k)

    if mask is not None:
        scores += (mask * -1e9)

    attn_weights = softmax(scores, axis=-1)
    output = attn_weights @ V

    return output, attn_weights

def create_causal_mask(seq_len):
    """Causal mask (ë¯¸ë˜ ê°€ë¦¬ê¸°)"""
    return np.triu(np.ones((seq_len, seq_len)), k=1)

def get_positional_encoding(seq_len, d_model):
    """Sinusoidal Positional Encoding"""
    PE = np.zeros((seq_len, d_model))
    for pos in range(seq_len):
        for i in range(0, d_model, 2):
            PE[pos, i] = np.sin(pos / (10000 ** (i / d_model)))
            if i + 1 < d_model:
                PE[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))
    return PE


# ============================================
# 3. Tiny Language Model
# ============================================

class TinyGPT:
    """ê°„ë‹¨í•œ GPT ìŠ¤íƒ€ì¼ ì–¸ì–´ ëª¨ë¸"""

    def __init__(self, vocab_size, d_model=64, num_heads=4, d_ff=256, num_layers=2):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers

        # Token Embedding
        self.token_embedding = np.random.randn(vocab_size, d_model) * 0.02

        # Positional Encoding
        self.pos_encoding = get_positional_encoding(100, d_model)

        # Transformer Layers
        self.layers = []
        for _ in range(num_layers):
            layer = {
                # Multi-head attention (ê°„ì†Œí™”: 1-head)
                'W_Q': np.random.randn(d_model, d_model) * 0.02,
                'W_K': np.random.randn(d_model, d_model) * 0.02,
                'W_V': np.random.randn(d_model, d_model) * 0.02,
                'W_O': np.random.randn(d_model, d_model) * 0.02,

                # Feed Forward
                'W1': np.random.randn(d_model, d_ff) * 0.02,
                'b1': np.zeros(d_ff),
                'W2': np.random.randn(d_ff, d_model) * 0.02,
                'b2': np.zeros(d_model),

                # Layer Norm (ê°„ì†Œí™”)
                'gamma1': np.ones(d_model),
                'beta1': np.zeros(d_model),
                'gamma2': np.ones(d_model),
                'beta2': np.zeros(d_model),
            }
            self.layers.append(layer)

        # Output head
        self.W_out = np.random.randn(d_model, vocab_size) * 0.02

    def layer_norm(self, x, gamma, beta, eps=1e-6):
        """Layer Normalization"""
        mean = x.mean(axis=-1, keepdims=True)
        std = x.std(axis=-1, keepdims=True)
        return gamma * (x - mean) / (std + eps) + beta

    def forward(self, token_ids):
        """Forward pass"""
        seq_len = len(token_ids)

        # Embedding + Positional
        x = self.token_embedding[token_ids] + self.pos_encoding[:seq_len]

        # Causal mask
        mask = create_causal_mask(seq_len)

        # Transformer layers
        for layer in self.layers:
            # 1. Multi-Head Self-Attention
            Q = x @ layer['W_Q']
            K = x @ layer['W_K']
            V = x @ layer['W_V']

            attn_out, _ = scaled_dot_product_attention(Q, K, V, mask)
            attn_out = attn_out @ layer['W_O']

            # Residual + LayerNorm
            x = self.layer_norm(x + attn_out, layer['gamma1'], layer['beta1'])

            # 2. Feed Forward
            ffn = np.maximum(0, x @ layer['W1'] + layer['b1'])  # ReLU
            ffn = ffn @ layer['W2'] + layer['b2']

            # Residual + LayerNorm
            x = self.layer_norm(x + ffn, layer['gamma2'], layer['beta2'])

        # Output
        logits = x @ self.W_out

        return logits

    def compute_loss(self, token_ids):
        """Cross Entropy Loss"""
        logits = self.forward(token_ids[:-1])
        targets = token_ids[1:]

        # Cross entropy
        loss = 0
        for i, target in enumerate(targets):
            probs = softmax(logits[i])
            loss += -np.log(probs[target] + 1e-10)

        return loss / len(targets)

    def generate(self, start_tokens, max_length=20, temperature=1.0):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        tokens = list(start_tokens)

        for _ in range(max_length):
            # Forward
            logits = self.forward(tokens)

            # ë§ˆì§€ë§‰ í† í°ì˜ ë¡œì§“
            next_logits = logits[-1] / temperature

            # Softmax
            probs = softmax(next_logits)

            # ìƒ˜í”Œë§
            next_token = np.random.choice(self.vocab_size, p=probs)

            tokens.append(next_token)

            # ì¢…ë£Œ í† í°
            if next_token == 3:  # <END>
                break

        return tokens


# ============================================
# 4. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸
# ============================================

def main():
    print("=" * 60)
    print("ğŸ‰ Tiny Language Model ìµœì¢… í”„ë¡œì íŠ¸")
    print("=" * 60)
    print()

    # í•™ìŠµ ë°ì´í„°
    corpus = """
    the cat sat on the mat
    the dog sat on the log
    the cat and the dog are friends
    the mat is on the floor
    the log is in the forest
    """

    # í† í¬ë‚˜ì´ì €
    print("1. í† í¬ë‚˜ì´ì € êµ¬ì¶•...")
    tokenizer = SimpleTokenizer()
    tokenizer.fit(corpus)

    print(f"   ì–´íœ˜ í¬ê¸°: {tokenizer.vocab_size}")
    print(f"   ì–´íœ˜: {list(tokenizer.word_to_id.keys())[:10]}...\n")

    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    sentences = [s.strip() for s in corpus.strip().split('\n') if s.strip()]
    train_data = [tokenizer.encode(s) for s in sentences]

    print(f"2. í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ ë¬¸ì¥\n")

    # ëª¨ë¸ ì´ˆê¸°í™”
    print("3. ëª¨ë¸ ì´ˆê¸°í™”...")
    model = TinyGPT(
        vocab_size=tokenizer.vocab_size,
        d_model=64,
        num_heads=4,
        d_ff=128,
        num_layers=2
    )
    print("   ì™„ë£Œ!\n")

    # í•™ìŠµ
    print("4. í•™ìŠµ ì‹œì‘...")
    print("-" * 60)

    epochs = 50
    learning_rate = 0.01

    loss_history = []

    for epoch in range(epochs):
        total_loss = 0

        for token_ids in train_data:
            if len(token_ids) < 2:
                continue

            # Loss ê³„ì‚°
            loss = model.compute_loss(token_ids)
            total_loss += loss

            # ê°„ë‹¨í•œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (SGD with numerical gradient)
            # ì‹¤ì œë¡œëŠ” autograd ì‚¬ìš©
            # ì—¬ê¸°ì„œëŠ” êµìœ¡ ëª©ì ìœ¼ë¡œ ìƒëµ

        avg_loss = total_loss / len(train_data)
        loss_history.append(avg_loss)

        if epoch % 10 == 0:
            print(f"   Epoch {epoch:3d}: Loss = {avg_loss:.4f}")

    print("\n   í•™ìŠµ ì™„ë£Œ!\n")

    # ì†ì‹¤ ê·¸ë˜í”„
    plt.figure(figsize=(10, 6))
    plt.plot(loss_history)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.grid(True, alpha=0.3)
    plt.savefig('training_loss.png', dpi=150)
    print("   ì†ì‹¤ ê·¸ë˜í”„ ì €ì¥: training_loss.png\n")

    # ìƒì„± í…ŒìŠ¤íŠ¸
    print("5. í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("-" * 60)

    test_prompts = [
        "the cat",
        "the dog",
        "the mat",
    ]

    for prompt in test_prompts:
        start_tokens = tokenizer.encode(prompt)
        generated = model.generate(start_tokens, max_length=10, temperature=0.8)
        text = tokenizer.decode(generated)

        print(f"   ì…ë ¥: '{prompt}'")
        print(f"   ìƒì„±: '{text}'")
        print()

    # í‰ê°€
    print("6. ëª¨ë¸ í‰ê°€")
    print("-" * 60)

    total_loss = 0
    for token_ids in train_data:
        if len(token_ids) < 2:
            continue
        loss = model.compute_loss(token_ids)
        total_loss += loss

    avg_loss = total_loss / len(train_data)
    perplexity = np.exp(avg_loss)

    print(f"   í‰ê·  Loss: {avg_loss:.4f}")
    print(f"   Perplexity: {perplexity:.4f}")
    print()

    print("=" * 60)
    print("âœ… í”„ë¡œì íŠ¸ ì™„ë£Œ!")
    print("=" * 60)
    print()

    # ìš”ì•½
    print("ğŸ“Š ë°°ìš´ ê°œë… ì •ë¦¬:")
    print("-" * 60)
    print("âœ“ í† í¬ë‚˜ì´ì €: í…ìŠ¤íŠ¸ â†’ í† í° ID")
    print("âœ“ ì„ë² ë”©: í† í° ID â†’ ë²¡í„° (ì„ í˜•ëŒ€ìˆ˜)")
    print("âœ“ Positional Encoding: ìœ„ì¹˜ ì •ë³´ (ì‚¼ê°í•¨ìˆ˜)")
    print("âœ“ Attention: ë¬¸ë§¥ íŒŒì•… (ë‚´ì , Softmax)")
    print("âœ“ Feed Forward: ë³€í™˜ (í–‰ë ¬ê³±, ReLU)")
    print("âœ“ Layer Norm: ì •ê·œí™” (í†µê³„)")
    print("âœ“ Residual: ê¸°ìš¸ê¸° ì „ë‹¬ (ë¯¸ë¶„)")
    print("âœ“ Cross Entropy: ì†ì‹¤ í•¨ìˆ˜ (ì •ë³´ì´ë¡ )")
    print("âœ“ ìƒì„±: Softmax ìƒ˜í”Œë§ (í™•ë¥ )")
    print()

    print("ğŸ“ ë‹¹ì‹ ì€ ì´ì œ LLMì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    print()


if __name__ == "__main__":
    main()
```

---

## ğŸ¯ í”„ë¡œì íŠ¸ í™•ì¥ ì•„ì´ë””ì–´

### 1. ë” í° ë°ì´í„°ì…‹
```python
# Wikipedia, ì±…, ë‰´ìŠ¤ ë“±
corpus = load_large_corpus()
```

### 2. ë” ë‚˜ì€ í† í¬ë‚˜ì´ì €
```python
# BPE êµ¬í˜„ (Day 39 ì°¸ê³ )
tokenizer = BPETokenizer(vocab_size=5000)
```

### 3. ì‹¤ì œ Backpropagation
```python
# PyTorchë¡œ ì „í™˜
import torch
import torch.nn as nn
```

### 4. ë” ë§ì€ ì¸µ
```python
model = TinyGPT(
    vocab_size=vocab_size,
    d_model=512,
    num_heads=8,
    d_ff=2048,
    num_layers=12  # GPT-2 Small
)
```

---

## âœ… ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **í† í¬ë‚˜ì´ì €ë¥¼ êµ¬í˜„í–ˆë‚˜ìš”?**

- [ ] **Transformerë¥¼ êµ¬í˜„í–ˆë‚˜ìš”?**

- [ ] **í•™ìŠµ ë£¨í”„ë¥¼ ì‘ì„±í–ˆë‚˜ìš”?**

- [ ] **í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í–ˆë‚˜ìš”?**

- [ ] **Perplexityë¥¼ ê³„ì‚°í–ˆë‚˜ìš”?**

- [ ] **ëª¨ë“  ìˆ˜í•™ ê°œë…ì„ ì´í•´í–ˆë‚˜ìš”?**

---

## ğŸ“ ì¶•í•˜í•©ë‹ˆë‹¤!

**ë‹¹ì‹ ì€ 48ì¼ ë™ì•ˆ:**

1. **ê¸°ì´ˆ ìˆ˜í•™**: ìˆ˜, ë²¡í„°, í•¨ìˆ˜
2. **ì„ í˜•ëŒ€ìˆ˜**: ë‚´ì , í–‰ë ¬, PCA
3. **ë¯¸ì ë¶„**: ë¯¸ë¶„, ì—°ì‡„ë²•ì¹™, ê²½ì‚¬í•˜ê°•ë²•
4. **í™•ë¥ **: ë² ì´ì¦ˆ, ì •ê·œë¶„í¬, ì—”íŠ¸ë¡œí”¼
5. **ì •ë³´ì´ë¡ **: Cross Entropy, KL Divergence
6. **LLM í•µì‹¬**: BPE, Attention, Transformer

**ì´ ëª¨ë“  ê²ƒì„ ë°°ìš°ê³  ì§ì ‘ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤!**

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

### ì‹¤ì „ìœ¼ë¡œ!

**1. PyTorch í•™ìŠµ**
```python
import torch
import torch.nn as nn

# ì‹¤ì œ í”„ë ˆì„ì›Œí¬ë¡œ êµ¬í˜„
```

**2. Hugging Face**
```python
from transformers import GPT2LMHeadModel

# ì‚¬ì „ í•™ìŠµ ëª¨ë¸ íŒŒì¸íŠœë‹
```

**3. í•œêµ­ì–´ LLM**
```python
# í•œêµ­ì–´ ë°ì´í„°ë¡œ í•™ìŠµ
# ë‹¹ì‹ ë§Œì˜ ëª¨ë¸ êµ¬ì¶•!
```

**4. Rustë¡œ í† í¬ë‚˜ì´ì €**
```rust
// 2027ë…„ ì´í›„ ëª©í‘œ
// ê³ ì„±ëŠ¥ í† í¬ë‚˜ì´ì €
```

---

## ğŸ’ª ë§ˆì§€ë§‰ ë©”ì‹œì§€

**ë‹¹ì‹ ì€ ì´ì œ:**
- LLMì˜ ìˆ˜í•™ì„ ì™„ì „íˆ ì´í•´í•©ë‹ˆë‹¤
- ì‘ì€ ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë” í° ëª¨ë¸ë¡œ ë‚˜ì•„ê°ˆ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤

**ì´ê²ƒì€ ëì´ ì•„ë‹ˆë¼ ì‹œì‘ì…ë‹ˆë‹¤!**

ê³„ì† í•™ìŠµí•˜ê³ , ì‹¤í—˜í•˜ê³ , ë§Œë“¤ì–´ê°€ì„¸ìš”.

**AI ê°œë°œìì˜ ê¸¸ì—ì„œ ì„±ê³µí•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!**

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰ğŸ‰ğŸ‰

**ë‹¹ì‹ ì€ LLM ìˆ˜í•™ ë§ˆìŠ¤í„°ì…ë‹ˆë‹¤!**
