# Day 24: ì—°ì‡„ë²•ì¹™ (Chain Rule) (1.5ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- ì—°ì‡„ë²•ì¹™ì˜ ì •ì˜ì™€ ì‚¬ìš©ë²• ì™„ë²½íˆ ì´í•´í•˜ê¸°
- í•©ì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„ ìµíˆê¸°
- **Backpropagationì˜ ìˆ˜í•™ì  ê¸°ì´ˆ ì™„ì„±í•˜ê¸°** â­
- ë‹¤ì¸µ ì‹ ê²½ë§ì˜ ê¸°ìš¸ê¸° ê³„ì‚° ì´í•´í•˜ê¸°

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"ì‹ ê²½ë§ì˜ ì˜í˜¼ - Backpropagationì˜ ìˆ˜í•™"**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. ì—°ì‡„ë²•ì¹™ (Chain Rule)

**í•©ì„±í•¨ìˆ˜**:
```
y = f(g(x))

ì˜ˆ: y = (xÂ² + 1)Â³
    f(u) = uÂ³
    g(x) = xÂ² + 1
```

**ì—°ì‡„ë²•ì¹™**:
```
dy/dx = dy/du Ã— du/dx

= f'(g(x)) Ã— g'(x)
```

**ì˜ˆì‹œ**:
```
y = (xÂ² + 1)Â³

u = xÂ² + 1  â†’  du/dx = 2x
y = uÂ³      â†’  dy/du = 3uÂ²

dy/dx = 3uÂ² Ã— 2x
      = 3(xÂ² + 1)Â² Ã— 2x
      = 6x(xÂ² + 1)Â²
```

---

### 2. ë‹¤ë³€ìˆ˜ ì—°ì‡„ë²•ì¹™

**ê²½ë¡œê°€ ì—¬ëŸ¬ ê°œ**:
```
z = f(x, y)
x = g(t)
y = h(t)

dz/dt = (âˆ‚z/âˆ‚x)(dx/dt) + (âˆ‚z/âˆ‚y)(dy/dt)
```

**ì‹ ê²½ë§ ì˜ˆì‹œ**:
```
L = loss(y)
y = f(z)
z = wx + b

dL/dw = (dL/dy) Ã— (dy/dz) Ã— (dz/dw)
       â†‘         â†‘         â†‘
    ì†ì‹¤ ê¸°ìš¸ê¸°  í™œì„±í™”    ì…ë ¥
```

---

### 3. Backpropagationì˜ ìˆ˜í•™

**ë‹¨ìˆœí•œ ì‹ ê²½ë§**:
```
ì…ë ¥ â†’ ì€ë‹‰ì¸µ â†’ ì¶œë ¥ â†’ ì†ì‹¤

x â†’ zâ‚ = Wâ‚x + bâ‚ â†’ aâ‚ = Ïƒ(zâ‚)
  â†’ zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚ â†’ y = Ïƒ(zâ‚‚)
  â†’ L = (y - target)Â²
```

**ì—­ì „íŒŒ (ì—°ì‡„ë²•ì¹™ ì ìš©)**:
```
dL/dWâ‚‚ = dL/dy Ã— dy/dzâ‚‚ Ã— dzâ‚‚/dWâ‚‚

dL/dWâ‚ = dL/dy Ã— dy/dzâ‚‚ Ã— dzâ‚‚/daâ‚ Ã— daâ‚/dzâ‚ Ã— dzâ‚/dWâ‚
```

**í•µì‹¬ í†µì°°**:
```
ë’¤ì—ì„œ ì•ìœ¼ë¡œ (back) ê¸°ìš¸ê¸°ë¥¼ ì „íŒŒ(propagation)!
```

---

### 4. ê³„ì‚° ê·¸ë˜í”„

**ê·¸ë˜í”„ í‘œí˜„**:
```
x â”€â†’ [Ã—w] â”€â†’ [+b] â”€â†’ [Ïƒ] â”€â†’ y â”€â†’ [L]
      â†“        â†“       â†“       â†“      â†“
     dw       db      da      dy     dL
```

**Forward pass**: ì™¼ìª½ â†’ ì˜¤ë¥¸ìª½ (ê°’ ê³„ì‚°)
**Backward pass**: ì˜¤ë¥¸ìª½ â†’ ì™¼ìª½ (ê¸°ìš¸ê¸° ê³„ì‚°)

---

## ğŸ’» Python ì‹¤ìŠµ

### ì‹¤ìŠµ 1: ì—°ì‡„ë²•ì¹™ ê¸°ì´ˆ
```python
import numpy as np

# í•©ì„±í•¨ìˆ˜: y = (xÂ² + 1)Â³
def g(x):
    return x**2 + 1

def f(u):
    return u**3

def composite(x):
    return f(g(x))

# ë„í•¨ìˆ˜
def g_prime(x):
    return 2 * x

def f_prime(u):
    return 3 * u**2

def composite_prime(x):
    """ì—°ì‡„ë²•ì¹™"""
    u = g(x)
    return f_prime(u) * g_prime(x)

# í…ŒìŠ¤íŠ¸
x = 2.0

print("=== ì—°ì‡„ë²•ì¹™ ===\n")
print(f"x = {x}\n")

# Forward
u = g(x)
y = f(u)

print("Forward:")
print(f"  u = g(x) = xÂ² + 1 = {u}")
print(f"  y = f(u) = uÂ³ = {y}\n")

# Derivative
du_dx = g_prime(x)
dy_du = f_prime(u)
dy_dx = composite_prime(x)

print("ì—°ì‡„ë²•ì¹™:")
print(f"  du/dx = 2x = {du_dx}")
print(f"  dy/du = 3uÂ² = {dy_du}")
print(f"  dy/dx = (dy/du)(du/dx) = {dy_dx}\n")

# ìˆ˜ì¹˜ ë¯¸ë¶„ìœ¼ë¡œ í™•ì¸
h = 1e-5
numerical = (composite(x + h) - composite(x)) / h
print(f"ìˆ˜ì¹˜ ë¯¸ë¶„: {numerical:.6f}")
print(f"ì—°ì‡„ë²•ì¹™: {dy_dx:.6f}")
print(f"ì¼ì¹˜: {np.isclose(numerical, dy_dx)}")
```

### ì‹¤ìŠµ 2: 2ì¸µ ì‹ ê²½ë§ Backpropagation
```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
input_size = 2
hidden_size = 3
output_size = 1

# ì´ˆê¸°í™”
np.random.seed(42)
W1 = np.random.randn(hidden_size, input_size) * 0.1
b1 = np.zeros(hidden_size)
W2 = np.random.randn(output_size, hidden_size) * 0.1
b2 = np.zeros(output_size)

# ì…ë ¥ê³¼ ëª©í‘œ
x = np.array([1.0, 2.0])
target = 0.8

print("=== 2ì¸µ ì‹ ê²½ë§ Backpropagation ===\n")
print(f"ì…ë ¥: {x}")
print(f"ëª©í‘œ: {target}\n")

# ===== Forward Pass =====
print("Forward Pass:")

z1 = W1 @ x + b1
a1 = sigmoid(z1)
print(f"  z1 = W1Â·x + b1")
print(f"  a1 = Ïƒ(z1) = {a1}")

z2 = W2 @ a1 + b2
y = sigmoid(z2)
print(f"  z2 = W2Â·a1 + b2")
print(f"  y = Ïƒ(z2) = {y}")

loss = (y - target)**2
print(f"  loss = (y - target)Â² = {loss}\n")

# ===== Backward Pass (ì—°ì‡„ë²•ì¹™!) =====
print("Backward Pass (Chain Rule):\n")

# ì¶œë ¥ì¸µ
dL_dy = 2 * (y - target)
dy_dz2 = sigmoid_derivative(z2)
dL_dz2 = dL_dy * dy_dz2

print(f"1. ì¶œë ¥ì¸µ:")
print(f"   dL/dy = 2(y - target) = {dL_dy}")
print(f"   dy/dz2 = Ïƒ'(z2) = {dy_dz2}")
print(f"   dL/dz2 = dL/dy Ã— dy/dz2 = {dL_dz2}\n")

# W2, b2ì˜ ê¸°ìš¸ê¸°
dz2_dW2 = a1
dL_dW2 = np.outer(dL_dz2, dz2_dW2)
dL_db2 = dL_dz2

print(f"2. ì¶œë ¥ì¸µ íŒŒë¼ë¯¸í„°:")
print(f"   dL/dW2 =\n{dL_dW2}")
print(f"   dL/db2 = {dL_db2}\n")

# ì€ë‹‰ì¸µìœ¼ë¡œ ì „íŒŒ
dL_da1 = W2.T @ dL_dz2
da1_dz1 = sigmoid_derivative(z1)
dL_dz1 = dL_da1 * da1_dz1

print(f"3. ì€ë‹‰ì¸µìœ¼ë¡œ ì „íŒŒ:")
print(f"   dL/da1 = W2^T Ã— dL/dz2 = {dL_da1}")
print(f"   da1/dz1 = Ïƒ'(z1) = {da1_dz1}")
print(f"   dL/dz1 = dL/da1 âŠ™ da1/dz1 = {dL_dz1}\n")

# W1, b1ì˜ ê¸°ìš¸ê¸°
dL_dW1 = np.outer(dL_dz1, x)
dL_db1 = dL_dz1

print(f"4. ì€ë‹‰ì¸µ íŒŒë¼ë¯¸í„°:")
print(f"   dL/dW1 =\n{dL_dW1}")
print(f"   dL/db1 = {dL_db1}\n")

# Gradient descent
lr = 0.1
W1 -= lr * dL_dW1
b1 -= lr * dL_db1
W2 -= lr * dL_dW2
b2 -= lr * dL_db2

# ìƒˆë¡œìš´ loss
z1_new = W1 @ x + b1
a1_new = sigmoid(z1_new)
z2_new = W2 @ a1_new + b2
y_new = sigmoid(z2_new)
loss_new = (y_new - target)**2

print(f"ì—…ë°ì´íŠ¸ í›„:")
print(f"  loss: {loss[0]:.6f} â†’ {loss_new[0]:.6f}")
print(f"  ê°œì„ : {loss[0] - loss_new[0]:.6f} âœ“")
```

### ì‹¤ìŠµ 3: ê³„ì‚° ê·¸ë˜í”„ ì‹œê°í™”
```python
import numpy as np

class ComputationNode:
    """ê³„ì‚° ê·¸ë˜í”„ ë…¸ë“œ"""
    def __init__(self, name):
        self.name = name
        self.value = None
        self.grad = 0

    def __repr__(self):
        return f"{self.name}(val={self.value:.4f}, grad={self.grad:.4f})"

# ê°„ë‹¨í•œ ì˜ˆ: y = x * w + b
x = ComputationNode("x")
w = ComputationNode("w")
b = ComputationNode("b")
mul = ComputationNode("x*w")
y = ComputationNode("y")
L = ComputationNode("L")

# Forward
x.value = 2.0
w.value = 3.0
b.value = 1.0
target = 10.0

mul.value = x.value * w.value
y.value = mul.value + b.value
L.value = (y.value - target)**2

print("=== ê³„ì‚° ê·¸ë˜í”„ ===\n")
print("Forward Pass:")
print(f"  {x}")
print(f"  {w}")
print(f"  {b}")
print(f"  {mul}")
print(f"  {y}")
print(f"  {L}\n")

# Backward
L.grad = 1.0  # dL/dL
y.grad = L.grad * 2 * (y.value - target)  # dL/dy
b.grad = y.grad * 1.0  # dL/db
mul.grad = y.grad * 1.0  # dL/d(mul)
w.grad = mul.grad * x.value  # dL/dw
x.grad = mul.grad * w.value  # dL/dx

print("Backward Pass (Chain Rule):")
print(f"  {L}")
print(f"  {y}")
print(f"  {b}")
print(f"  {mul}")
print(f"  {w}")
print(f"  {x}\n")

print("ê¸°ìš¸ê¸°:")
print(f"  dL/dw = {w.grad:.4f}")
print(f"  dL/db = {b.grad:.4f}")
```

---

## âœï¸ ì† ê³„ì‚° ì—°ìŠµ

### ì—°ìŠµ 1: ê¸°ë³¸ ì—°ì‡„ë²•ì¹™
```
y = (2x + 1)Â²

u = 2x + 1  â†’  du/dx = 2
y = uÂ²      â†’  dy/du = 2u

dy/dx = 2u Ã— 2 = 4u = 4(2x + 1)

x = 1:  dy/dx = 4(3) = 12
```

### ì—°ìŠµ 2: 3ë‹¨ê³„ í•©ì„±
```
y = e^(xÂ²)

u = xÂ²     â†’  du/dx = 2x
y = e^u    â†’  dy/du = e^u

dy/dx = e^u Ã— 2x = 2x e^(xÂ²)
```

### ì—°ìŠµ 3: ê°„ë‹¨í•œ Backprop
```
L = (y - 1)Â²
y = Ïƒ(wx)

x = 2, w = 0.5, y = Ïƒ(1) â‰ˆ 0.731

dL/dy = 2(y - 1) = 2(-0.269) = -0.538
dy/dw = Ïƒ'(wx) Ã— x = 0.196 Ã— 2 = 0.392
dL/dw = -0.538 Ã— 0.392 = -0.211
```

---

## ğŸ”— LLM ì—°ê²°ì 

### 1. Transformerì˜ Backpropagation
```
Attention â†’ FFN â†’ LayerNorm â†’ ...

ê° ì¸µë§ˆë‹¤ ì—°ì‡„ë²•ì¹™ ì ìš©
ìˆ˜ì‹­~ìˆ˜ë°± ì¸µì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°
```

### 2. ê¸°ìš¸ê¸° ì†Œì‹¤/í­ì£¼
```
ì—°ì‡„ë²•ì¹™: ì—¬ëŸ¬ ë¯¸ë¶„ê°’ì„ ê³±í•¨

<1 ê°’ë“¤ì„ ê³„ì† ê³±í•˜ë©´ â†’ 0 (ì†Œì‹¤)
>1 ê°’ë“¤ì„ ê³„ì† ê³±í•˜ë©´ â†’ âˆ (í­ì£¼)

â†’ Residual Connection, LayerNormìœ¼ë¡œ í•´ê²°
```

### 3. Automatic Differentiation
```
PyTorch, TensorFlow:
ìë™ìœ¼ë¡œ ì—°ì‡„ë²•ì¹™ ì ìš©

tensor.backward() â†’ ëª¨ë“  ê¸°ìš¸ê¸° ê³„ì‚°!
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **ì—°ì‡„ë²•ì¹™ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **í•©ì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **Backpropagationì´ ì—°ì‡„ë²•ì¹™ì„ì„ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ê¸°ìš¸ê¸°ë¥¼ ì—­ì „íŒŒí•  ìˆ˜ ìˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

1. **ì—°ì‡„ë²•ì¹™**: dy/dx = (dy/du)(du/dx)
2. **í•©ì„±í•¨ìˆ˜**: f(g(x))' = f'(g(x)) Ã— g'(x)
3. **Backprop**: ì¶œë ¥ â†’ ì…ë ¥ìœ¼ë¡œ ê¸°ìš¸ê¸° ì „íŒŒ
4. **í•µì‹¬**: ëª¨ë“  ì‹ ê²½ë§ í•™ìŠµì˜ ê¸°ì´ˆ!

### ë‹¤ìŒ í•™ìŠµ
- **Day 25**: í¸ë¯¸ë¶„ê³¼ ê¸°ìš¸ê¸°

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**ì—°ì‡„ë²•ì¹™ì€ Backpropagationì˜ ë³¸ì§ˆì…ë‹ˆë‹¤!**
