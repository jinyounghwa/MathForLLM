# Day 2-3: 지수와 로그 (3시간)

## 📚 학습 목표
- 지수와 로그의 본질적 관계 이해하기
- 로그를 이용한 정보량 측정 원리 파악하기
- Log-Softmax가 왜 수치적으로 안정적인지 이해하기
- LLM의 엔트로피와 Perplexity 개념 연결하기

---

## 🎯 강의 주제
**"2를 3번 곱하면 8" vs "8을 2로 몇 번 나누면 1?"**

---

## 📖 핵심 개념

### 1. 지수 (Exponent)

#### 1.1 지수의 정의
**"같은 수를 여러 번 곱하기"**

```
2³ = 2 × 2 × 2 = 8
```

**일반 형태**:
```
aⁿ = a × a × a × ... × a  (n번)
```

- a: 밑(base)
- n: 지수(exponent)
- aⁿ: 거듭제곱(power)

#### 1.2 지수 법칙
```
1. aⁿ × aᵐ = aⁿ⁺ᵐ
   예: 2³ × 2² = 2⁵ = 32

2. (aⁿ)ᵐ = aⁿˣᵐ
   예: (2³)² = 2⁶ = 64

3. aⁿ / aᵐ = aⁿ⁻ᵐ
   예: 2⁵ / 2² = 2³ = 8

4. a⁰ = 1  (a ≠ 0)
   예: 2⁰ = 1

5. a⁻ⁿ = 1/aⁿ
   예: 2⁻³ = 1/8 = 0.125
```

#### 1.3 특별한 밑: e (자연상수)
```
e ≈ 2.71828182845...
```

**왜 e가 특별한가?**
- eˣ의 미분 = eˣ (자기 자신!)
- 자연 현성장, 복리 이자, 확률 분포에서 등장
- LLM에서 Softmax 함수에 사용

---

### 2. 로그 (Logarithm)

#### 2.1 로그의 정의
**"지수의 역연산"**

```
2³ = 8  ⟺  log₂(8) = 3
```

**읽는 법**:
- log₂(8) = 3: "2를 몇 번 곱하면 8이 되나? → 3번"
- log₁₀(100) = 2: "10을 몇 번 곱하면 100이 되나? → 2번"

**일반 형태**:
```
aⁿ = x  ⟺  log_a(x) = n
```

#### 2.2 주요 로그
```
1. 상용로그: log₁₀(x) 또는 log(x)
   - 10을 밑으로 하는 로그

2. 자연로그: ln(x) = log_e(x)
   - e를 밑으로 하는 로그
   - LLM에서 가장 많이 사용

3. 이진로그: log₂(x)
   - 2를 밑으로 하는 로그
   - 정보이론에서 사용 (비트 단위)
```

#### 2.3 로그 법칙
```
1. log_a(x × y) = log_a(x) + log_a(y)
   예: log₂(8 × 4) = log₂(8) + log₂(4) = 3 + 2 = 5

2. log_a(x / y) = log_a(x) - log_a(y)
   예: log₂(8 / 4) = log₂(8) - log₂(4) = 3 - 2 = 1

3. log_a(xⁿ) = n × log_a(x)
   예: log₂(8²) = 2 × log₂(8) = 2 × 3 = 6

4. log_a(1) = 0
   예: log₂(1) = 0

5. log_a(a) = 1
   예: log₂(2) = 1
```

#### 2.4 밑 변환 공식 ⭐
**모든 로그를 자연로그로 변환 가능!**

```
log_a(x) = ln(x) / ln(a)
```

**예시**:
```python
log₂(8) = ln(8) / ln(2)
        = 2.0794 / 0.6931
        = 3
```

**중요성**:
- 컴퓨터는 보통 ln(x)만 지원
- 다른 밑의 로그는 변환해서 계산

---

### 3. 로그와 정보량

#### 3.1 왜 로그로 정보를 측정하는가?

**직관적 예시**: 동전 던지기
```
1번 던지기: 2가지 결과 (앞, 뒤)
2번 던지기: 4가지 결과 (앞앞, 앞뒤, 뒤앞, 뒤뒤)
3번 던지기: 8가지 결과
n번 던지기: 2ⁿ가지 결과
```

**역으로 생각하면**:
```
2가지 결과 → 1비트 정보 (log₂(2) = 1)
4가지 결과 → 2비트 정보 (log₂(4) = 2)
8가지 결과 → 3비트 정보 (log₂(8) = 3)
```

**일반화**:
```
N가지 가능한 결과 → log₂(N) 비트 정보
```

#### 3.2 정보량의 공식
**하나의 사건이 일어날 확률이 p일 때, 정보량은:**
```
I(x) = -log₂(p) = log₂(1/p)
```

**직관**:
- 확률이 낮은 사건 = 정보량이 많다
- 확률이 높은 사건 = 정보량이 적다

**예시**:
```python
# "태양이 동쪽에서 뜬다" (거의 확실)
p = 0.9999
I = -log₂(0.9999) ≈ 0.0001 비트 (정보량 거의 없음)

# "내일 비가 온다" (반반)
p = 0.5
I = -log₂(0.5) = 1 비트

# "로또 1등 당첨" (매우 희박)
p = 0.0000001
I = -log₂(0.0000001) ≈ 23 비트 (정보량 많음!)
```

---

### 4. Softmax와 Log-Softmax

#### 4.1 Softmax 함수
**점수를 확률 분포로 변환**

```
Softmax(xᵢ) = exp(xᵢ) / Σⱼ exp(xⱼ)
```

**예시**:
```python
scores = [2, 1, 0.5]

# 지수 계산
exp_scores = [e², e¹, e⁰·⁵] = [7.39, 2.72, 1.65]

# 합계
sum = 7.39 + 2.72 + 1.65 = 11.76

# Softmax
probs = [7.39/11.76, 2.72/11.76, 1.65/11.76]
      = [0.628, 0.231, 0.140]
```

#### 4.2 Softmax의 문제점 ⚠️
**수치 불안정성 (Numerical Instability)**

```python
# 큰 숫자가 들어오면?
scores = [1000, 1001, 1002]
exp(1000) = 너무 큰 숫자! → Overflow 발생
```

#### 4.3 Log-Softmax: 안정적인 대안 ⭐

**Log-Softmax 공식**:
```
LogSoftmax(xᵢ) = xᵢ - log(Σⱼ exp(xⱼ))
```

**Trick: Max 값 빼기**
```python
# 원래
scores = [1000, 1001, 1002]

# Max 값 빼기
max_score = 1002
adjusted = [1000-1002, 1001-1002, 1002-1002]
          = [-2, -1, 0]

# 이제 안전하게 계산 가능!
exp(0) = 1
exp(-1) ≈ 0.37
exp(-2) ≈ 0.14
```

**왜 안전한가?**
1. 로그 공간에서 계산 → 큰 숫자 문제 해결
2. 덧셈/뺄셈으로 변환 → 곱셈의 오버플로우 방지
3. 수치적 안정성 확보

---

## 💻 Python 실습

### 실습 1: 지수와 로그 기본
```python
import numpy as np
import math

# 지수 계산
print("=== 지수 계산 ===")
print(f"2³ = {2**3}")
print(f"2⁵ = {2**5}")
print(f"2³ × 2² = 2⁵ = {2**3 * 2**2}")
print(f"e² = {math.e**2:.4f}")

# 로그 계산
print("\n=== 로그 계산 ===")
print(f"log₂(8) = {math.log2(8)}")
print(f"log₁₀(100) = {math.log10(100)}")
print(f"ln(e²) = {math.log(math.e**2)}")

# 밑 변환 공식
print("\n=== 밑 변환 ===")
def log_base(x, base):
    """임의의 밑으로 로그 계산"""
    return math.log(x) / math.log(base)

print(f"log₂(8) = {log_base(8, 2)}")
print(f"log₃(27) = {log_base(27, 3)}")
print(f"log₅(125) = {log_base(125, 5)}")

# 로그 법칙 검증
print("\n=== 로그 법칙 검증 ===")
x, y = 8, 4
print(f"log₂({x} × {y}) = log₂({x}) + log₂({y})")
print(f"{math.log2(x * y)} = {math.log2(x)} + {math.log2(y)}")
print(f"{math.log2(x * y)} = {math.log2(x) + math.log2(y)}")
```

### 실습 2: 정보량 계산
```python
import numpy as np
import matplotlib.pyplot as plt

def information_content(p):
    """확률 p의 정보량 계산 (비트)"""
    return -np.log2(p)

# 다양한 확률에 대한 정보량
probabilities = np.linspace(0.01, 1, 100)
info = information_content(probabilities)

# 시각화
plt.figure(figsize=(10, 6))
plt.plot(probabilities, info, linewidth=2)
plt.xlabel('Probability', fontsize=12)
plt.ylabel('Information Content (bits)', fontsize=12)
plt.title('Information Content vs Probability', fontsize=14)
plt.grid(True, alpha=0.3)

# 특정 확률 표시
special_probs = [0.01, 0.1, 0.5, 0.9]
special_info = information_content(np.array(special_probs))

for p, i in zip(special_probs, special_info):
    plt.plot(p, i, 'ro', markersize=10)
    plt.annotate(f'p={p}\nI={i:.2f} bits',
                 xy=(p, i), xytext=(p+0.1, i+1),
                 fontsize=10, ha='left',
                 arrowprops=dict(arrowstyle='->', color='red'))

plt.tight_layout()
plt.savefig('information_content.png', dpi=150, bbox_inches='tight')
print("정보량 그래프 저장 완료!")

# 실제 예시
print("\n=== 정보량 예시 ===")
events = [
    ("태양이 동쪽에서 뜬다", 0.9999),
    ("내일 비가 온다", 0.5),
    ("주사위에서 6이 나온다", 1/6),
    ("로또 1등", 1/8145060)
]

for event, prob in events:
    info = information_content(prob)
    print(f"{event:25s}: p={prob:.8f}, I={info:.2f} bits")
```

### 실습 3: Softmax vs Log-Softmax
```python
import numpy as np

def softmax(x):
    """기본 Softmax (불안정)"""
    exp_x = np.exp(x)
    return exp_x / np.sum(exp_x)

def stable_softmax(x):
    """안정적인 Softmax"""
    # Max trick: 최대값을 빼서 오버플로우 방지
    max_x = np.max(x)
    exp_x = np.exp(x - max_x)
    return exp_x / np.sum(exp_x)

def log_softmax(x):
    """Log-Softmax"""
    max_x = np.max(x)
    shifted = x - max_x
    return shifted - np.log(np.sum(np.exp(shifted)))

# 테스트 1: 작은 값
print("=== 작은 값 테스트 ===")
small_scores = np.array([2.0, 1.0, 0.5])
print(f"Scores: {small_scores}")
print(f"Softmax: {softmax(small_scores)}")
print(f"Stable Softmax: {stable_softmax(small_scores)}")
print(f"Log-Softmax: {log_softmax(small_scores)}")

# 테스트 2: 큰 값 (오버플로우 위험)
print("\n=== 큰 값 테스트 (Overflow 위험) ===")
large_scores = np.array([1000.0, 1001.0, 1002.0])
print(f"Scores: {large_scores}")

try:
    result = softmax(large_scores)
    print(f"Softmax: {result}")
except:
    print("Softmax: Overflow 발생! (inf/nan)")

print(f"Stable Softmax: {stable_softmax(large_scores)}")
print(f"Log-Softmax: {log_softmax(large_scores)}")

# 테스트 3: 수치 안정성 비교
print("\n=== 수치 안정성 비교 ===")
test_scores = np.array([100, 101, 102])

# 방법 1: 직접 Softmax 계산 후 로그
with np.errstate(over='raise'):
    try:
        probs = stable_softmax(test_scores)
        log_probs_method1 = np.log(probs)
        print(f"방법 1 (Softmax → Log): {log_probs_method1}")
    except FloatingPointError:
        print("방법 1: 오버플로우 발생!")

# 방법 2: Log-Softmax 직접 계산
log_probs_method2 = log_softmax(test_scores)
print(f"방법 2 (Log-Softmax): {log_probs_method2}")

print("\n✅ Log-Softmax가 수치적으로 더 안정적!")
```

### 실습 4: LLM에서의 활용
```python
import numpy as np

# LLM 토큰 예측 시뮬레이션
vocab = ["안녕", "하세요", "감사", "합니다", ".",]
scores = np.array([3.2, 2.1, 1.5, 0.8, -0.5])

print("=== LLM 토큰 예측 ===")
print(f"어휘: {vocab}")
print(f"Logits (점수): {scores}")

# Softmax로 확률 계산
probs = stable_softmax(scores)
print(f"\nSoftmax 확률:")
for token, prob in zip(vocab, probs):
    print(f"  {token}: {prob:.4f} ({prob*100:.2f}%)")

# Log-Softmax로 로그 확률 계산
log_probs = log_softmax(scores)
print(f"\nLog-Softmax (로그 확률):")
for token, log_prob in zip(vocab, log_probs):
    print(f"  {token}: {log_prob:.4f}")

# 예측: 가장 높은 확률의 토큰 선택
predicted_idx = np.argmax(probs)
print(f"\n예측된 토큰: '{vocab[predicted_idx]}' (확률: {probs[predicted_idx]:.4f})")
```

---

## ✍️ 손 계산 연습

### 연습 1: 기본 계산
다음을 계산하세요:

1. 2⁵ = ?
   ```
   답: 2 × 2 × 2 × 2 × 2 = 32
   ```

2. log₂(16) = ?
   ```
   답: 2를 몇 번 곱하면 16? → 4번
   log₂(16) = 4
   ```

3. log₁₀(1000) = ?
   ```
   답: 10을 몇 번 곱하면 1000? → 3번
   log₁₀(1000) = 3
   ```

### 연습 2: 로그 법칙
다음을 계산하세요:

1. log₂(8 × 4) = ?
   ```
   log₂(8) + log₂(4) = 3 + 2 = 5
   검증: log₂(32) = 5 ✓
   ```

2. log₂(16 / 4) = ?
   ```
   log₂(16) - log₂(4) = 4 - 2 = 2
   검증: log₂(4) = 2 ✓
   ```

3. log₂(8²) = ?
   ```
   2 × log₂(8) = 2 × 3 = 6
   검증: log₂(64) = 6 ✓
   ```

### 연습 3: 밑 변환
log₅(125)를 자연로그로 변환하여 계산하세요:

```
log₅(125) = ln(125) / ln(5)
          = ln(5³) / ln(5)
          = 3 × ln(5) / ln(5)
          = 3
```

### 연습 4: 정보량
다음 사건의 정보량을 계산하세요:

1. 동전 던지기에서 앞면 (p = 0.5)
   ```
   I = -log₂(0.5) = log₂(2) = 1 비트
   ```

2. 주사위에서 6이 나옴 (p = 1/6)
   ```
   I = -log₂(1/6) = log₂(6) ≈ 2.58 비트
   ```

### 연습 5: Softmax (손계산)
scores = [0, 1, 2]에 대해 Softmax를 계산하세요:

```
Step 1: 지수 계산
exp(0) = 1
exp(1) ≈ 2.72
exp(2) ≈ 7.39

Step 2: 합계
sum = 1 + 2.72 + 7.39 = 11.11

Step 3: 정규화
p₁ = 1 / 11.11 ≈ 0.09
p₂ = 2.72 / 11.11 ≈ 0.24
p₃ = 7.39 / 11.11 ≈ 0.67

검증: 0.09 + 0.24 + 0.67 = 1.00 ✓
```

---

## 🔗 LLM 연결점

### 1. 엔트로피 (Entropy)
**불확실성의 평균 정보량**

```
H(X) = -Σᵢ p(xᵢ) × log₂(p(xᵢ))
```

**LLM 예시**:
```python
# 다음 토큰 예측 분포
probs = [0.7, 0.2, 0.1]  # ["안녕", "하세요", "감사"]

# 엔트로피 계산
H = -(0.7 * log₂(0.7) + 0.2 * log₂(0.2) + 0.1 * log₂(0.1))
  = -(0.7 × -0.51 + 0.2 × -2.32 + 0.1 × -3.32)
  = -(-0.36 - 0.46 - 0.33)
  = 1.15 비트
```

**해석**:
- 낮은 엔트로피 (0에 가까움): 모델이 확신함
- 높은 엔트로피 (log₂(vocab_size)에 가까움): 모델이 불확실함

### 2. Perplexity
**"모델이 얼마나 당혹스러워하는가?"**

```
Perplexity = 2^H
```

- 낮은 Perplexity = 좋은 모델 (확신 있음)
- 높은 Perplexity = 나쁜 모델 (당혹스러움)

**예시**:
```
H = 1.15 비트
Perplexity = 2^1.15 ≈ 2.22

해석: 모델이 평균적으로 2.22개의 선택지 중에 고민하고 있음
```

### 3. Cross-Entropy Loss
**LLM 학습의 핵심 손실 함수**

```
Loss = -Σᵢ y_true(i) × log(y_pred(i))
```

- y_true: 정답 레이블 (one-hot)
- y_pred: 모델의 예측 (Softmax 출력)

**PyTorch 구현**:
```python
import torch
import torch.nn.functional as F

# Logits (모델 출력)
logits = torch.tensor([[3.2, 2.1, 1.5]])

# 정답 레이블
target = torch.tensor([0])  # 첫 번째 클래스가 정답

# Cross-Entropy Loss (내부적으로 Log-Softmax 사용)
loss = F.cross_entropy(logits, target)
print(f"Loss: {loss.item():.4f}")
```

---

## ✅ 체크포인트

### 스스로 확인하기

- [ ] **2³과 log₂(8)의 관계를 설명할 수 있나요?**
  - 2³ = 8이면 log₂(8) = 3 (역연산 관계)

- [ ] **밑 변환 공식을 사용할 수 있나요?**
  - log_a(x) = ln(x) / ln(a)

- [ ] **왜 로그로 정보량을 측정하나요?**
  - 곱셈을 덧셈으로 변환, 지수적 관계를 선형으로 표현

- [ ] **Log-Softmax가 왜 수치적으로 안정적인지 이해했나요?**
  - Max trick으로 오버플로우 방지, 로그 공간에서 계산

- [ ] **Perplexity가 무엇인지 설명할 수 있나요?**
  - 2^H, 모델의 불확실성을 측정

---

## 🎓 핵심 요약

### 오늘 배운 것
1. **지수**: aⁿ = a를 n번 곱하기
2. **로그**: 지수의 역연산, log_a(x) = n ⟺ aⁿ = x
3. **로그 법칙**: 곱셈 → 덧셈, 나눗셈 → 뺄셈
4. **정보량**: I(x) = -log₂(p), 희귀한 사건일수록 정보량이 많다
5. **Log-Softmax**: 수치 안정적인 확률 계산
6. **엔트로피**: 평균 정보량, 불확실성의 측정
7. **Perplexity**: 2^H, 모델 평가 지표

### 다음 학습과의 연결
- **Day 4**: 함수와 그래프
  - 지수 함수, 로그 함수의 시각화
  - Softmax, Sigmoid 등 활성화 함수

---

## 📝 오늘의 성찰

1. **로그를 사용하는 이유를 이해했나요?**

2. **LLM의 Cross-Entropy Loss에서 로그의 역할은?**

3. **가장 어려웠던 개념은 무엇인가요?**

---

**수고하셨습니다!** 🎉

지수와 로그는 LLM의 모든 곳에 등장합니다. 완벽히 이해하는 것이 중요합니다!
