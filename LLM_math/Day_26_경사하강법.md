# Day 26: ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent) (1.5ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- ê²½ì‚¬í•˜ê°•ë²•ì˜ ì›ë¦¬ ì™„ë²½íˆ ì´í•´í•˜ê¸°
- í•™ìŠµë¥ ì˜ ì¤‘ìš”ì„± íŒŒì•…í•˜ê¸°
- SGD, Momentum, Adam ë“± ë³€í˜• ì´í•´í•˜ê¸°
- ì‹¤ì œ ì‹ ê²½ë§ í•™ìŠµì— ì ìš©í•˜ê¸°

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"ê¸°ìš¸ê¸°ë¥¼ ë”°ë¼ ìµœì†Ÿê°’ ì°¾ê¸°"**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent)

**ëª©í‘œ**: í•¨ìˆ˜ f(Î¸)ì˜ ìµœì†Ÿê°’ ì°¾ê¸°

**ì•Œê³ ë¦¬ì¦˜**:
```
1. Î¸ë¥¼ ì„ì˜ë¡œ ì´ˆê¸°í™”
2. ë°˜ë³µ:
   a. ê¸°ìš¸ê¸° ê³„ì‚°: g = âˆ‡f(Î¸)
   b. ì—…ë°ì´íŠ¸: Î¸ = Î¸ - Î± Ã— g
   c. ìˆ˜ë ´ í™•ì¸
```

**ì§ê´€**:
```
í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€íŒŒë¥¸ ë‚´ë¦¬ë§‰ ë°©í–¥(-âˆ‡f)ìœ¼ë¡œ ì´ë™
```

---

### 2. í•™ìŠµë¥  (Learning Rate)

**Î± (ì•ŒíŒŒ)**: í•œ ë²ˆì— ì–¼ë§ˆë‚˜ ì´ë™í• ì§€
```
Î¸_new = Î¸_old - Î± Ã— âˆ‡f(Î¸)
```

**ë„ˆë¬´ ì‘ìœ¼ë©´**:
- ìˆ˜ë ´ì´ ë§¤ìš° ëŠë¦¼
- ê³„ì‚° ë¹„ìš© ì¦ê°€

**ë„ˆë¬´ í¬ë©´**:
- ì§„ë™ (oscillation)
- ë°œì‚° (divergence)

**ì ì ˆí•œ ê°’**:
- ë¹ ë¥´ê²Œ ìˆ˜ë ´
- ì•ˆì •ì 

---

### 3. ê²½ì‚¬í•˜ê°•ë²•ì˜ ì¢…ë¥˜

**Batch Gradient Descent**:
```
ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°
g = (1/N) Î£ âˆ‡L(x_i, y_i)

ì¥ì : ì •í™•í•œ ê¸°ìš¸ê¸°
ë‹¨ì : ëŠë¦¼, ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
```

**Stochastic Gradient Descent (SGD)**:
```
í•œ ìƒ˜í”Œë¡œ ê¸°ìš¸ê¸° ê³„ì‚°
g = âˆ‡L(x_i, y_i)

ì¥ì : ë¹ ë¦„, ë©”ëª¨ë¦¬ ì ê²Œ ì‚¬ìš©
ë‹¨ì : ë…¸ì´ì¦ˆ ë§ìŒ
```

**Mini-batch Gradient Descent**:
```
ë°°ì¹˜ í¬ê¸°ë§Œí¼ í‰ê· 
g = (1/B) Î£_{i in batch} âˆ‡L(x_i, y_i)

ì‹¤ì œë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©! (B=32, 64, 128, ...)
```

---

### 4. ê°œì„ ëœ ë°©ë²•ë“¤

**Momentum**:
```
v = Î² Ã— v + (1-Î²) Ã— g
Î¸ = Î¸ - Î± Ã— v

ì´ì „ ë°©í–¥ì„ ê¸°ì–µ â†’ ì§„ë™ ê°ì†Œ
```

**Adam** (Adaptive Moment Estimation):
```
m = Î²â‚ Ã— m + (1-Î²â‚) Ã— g     # 1ì°¨ ëª¨ë©˜íŠ¸
v = Î²â‚‚ Ã— v + (1-Î²â‚‚) Ã— gÂ²    # 2ì°¨ ëª¨ë©˜íŠ¸

Î¸ = Î¸ - Î± Ã— m / (âˆšv + Îµ)

ê°€ì¥ ë§ì´ ì‚¬ìš©ë¨!
```

---

## ğŸ’» Python ì‹¤ìŠµ

### ì‹¤ìŠµ 1: ê¸°ë³¸ ê²½ì‚¬í•˜ê°•ë²•
```python
import numpy as np
import matplotlib.pyplot as plt

# í•¨ìˆ˜: f(x) = xÂ² - 4x + 4 = (x-2)Â²
def f(x):
    return x**2 - 4*x + 4

def df(x):
    return 2*x - 4

# ê²½ì‚¬í•˜ê°•ë²•
def gradient_descent(start, learning_rate, iterations):
    x = start
    history = [x]

    for i in range(iterations):
        grad = df(x)
        x = x - learning_rate * grad
        history.append(x)

    return x, history

# ì‹œê°í™”
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

learning_rates = [0.1, 0.5, 1.1]
x_plot = np.linspace(-1, 5, 200)
y_plot = f(x_plot)

for idx, lr in enumerate(learning_rates):
    ax = axes[idx]

    # í•¨ìˆ˜ ê·¸ë˜í”„
    ax.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x)')

    # ê²½ì‚¬í•˜ê°•ë²•
    start = 4.0
    iterations = 20
    final_x, history = gradient_descent(start, lr, iterations)

    # ê²½ë¡œ
    for i in range(len(history)-1):
        ax.arrow(history[i], f(history[i]),
                history[i+1] - history[i], f(history[i+1]) - f(history[i]),
                head_width=0.1, head_length=0.1, fc='red', ec='red',
                alpha=0.5)

    ax.scatter(history, [f(x) for x in history], c='red', s=50, zorder=5)
    ax.scatter([2], [0], c='green', s=200, marker='*', zorder=10, label='Minimum')

    ax.set_title(f'Learning Rate = {lr}', fontsize=12)
    ax.set_xlabel('x')
    ax.set_ylabel('f(x)')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nLearning Rate = {lr}:")
    print(f"  Start: {start:.4f}")
    print(f"  Final: {final_x:.4f}")
    print(f"  Iterations: {len(history)-1}")

plt.tight_layout()
plt.savefig('learning_rate_comparison.png', dpi=150)
print("\ní•™ìŠµë¥  ë¹„êµ ì‹œê°í™” ì €ì¥ ì™„ë£Œ!")
```

### ì‹¤ìŠµ 2: SGD vs Mini-batch
```python
import numpy as np

# ê°„ë‹¨í•œ ì„ í˜• íšŒê·€ ë°ì´í„°
np.random.seed(42)
N = 1000
X = np.random.randn(N, 1)
y = 3 * X + 2 + np.random.randn(N, 1) * 0.5  # y = 3x + 2 + noise

# ì†ì‹¤ í•¨ìˆ˜: MSE
def loss(X, y, w, b):
    predictions = X * w + b
    return np.mean((predictions - y)**2)

# ê¸°ìš¸ê¸°
def gradient(X, y, w, b):
    N = len(X)
    predictions = X * w + b
    dw = (2/N) * np.sum(X * (predictions - y))
    db = (2/N) * np.sum(predictions - y)
    return dw, db

print("=== SGD vs Mini-batch ===\n")

# Batch GD
w_batch, b_batch = 0.0, 0.0
lr = 0.01
epochs = 50

for epoch in range(epochs):
    dw, db = gradient(X, y, w_batch, b_batch)
    w_batch -= lr * dw
    b_batch -= lr * db

print("Batch Gradient Descent:")
print(f"  w = {w_batch:.4f}, b = {b_batch:.4f}")
print(f"  True: w = 3.0, b = 2.0\n")

# Mini-batch GD
w_mini, b_mini = 0.0, 0.0
batch_size = 32
lr = 0.01
epochs = 50

for epoch in range(epochs):
    # ë°ì´í„° ì„ê¸°
    indices = np.random.permutation(N)

    for i in range(0, N, batch_size):
        batch_indices = indices[i:i+batch_size]
        X_batch = X[batch_indices]
        y_batch = y[batch_indices]

        dw, db = gradient(X_batch, y_batch, w_mini, b_mini)
        w_mini -= lr * dw
        b_mini -= lr * db

print("Mini-batch Gradient Descent (batch_size=32):")
print(f"  w = {w_mini:.4f}, b = {b_mini:.4f}")
```

### ì‹¤ìŠµ 3: Momentum vs Adam
```python
import numpy as np
import matplotlib.pyplot as plt

# í•¨ìˆ˜: Rosenbrock (ìµœì í™”ê°€ ì–´ë ¤ìš´ í•¨ìˆ˜)
def rosenbrock(x, y):
    return (1 - x)**2 + 100 * (y - x**2)**2

def grad_rosenbrock(x, y):
    dx = -2 * (1 - x) - 400 * x * (y - x**2)
    dy = 200 * (y - x**2)
    return np.array([dx, dy])

# SGD
def sgd(start, lr, iterations):
    theta = np.array(start, dtype=float)
    path = [theta.copy()]

    for _ in range(iterations):
        grad = grad_rosenbrock(*theta)
        theta = theta - lr * grad
        path.append(theta.copy())

    return path

# Momentum
def momentum(start, lr, beta, iterations):
    theta = np.array(start, dtype=float)
    v = np.zeros_like(theta)
    path = [theta.copy()]

    for _ in range(iterations):
        grad = grad_rosenbrock(*theta)
        v = beta * v + grad
        theta = theta - lr * v
        path.append(theta.copy())

    return path

# Adam (ê°„ì†Œí™”)
def adam(start, lr, beta1, beta2, iterations):
    theta = np.array(start, dtype=float)
    m = np.zeros_like(theta)
    v = np.zeros_like(theta)
    path = [theta.copy()]
    epsilon = 1e-8

    for t in range(1, iterations+1):
        grad = grad_rosenbrock(*theta)

        m = beta1 * m + (1 - beta1) * grad
        v = beta2 * v + (1 - beta2) * grad**2

        m_hat = m / (1 - beta1**t)
        v_hat = v / (1 - beta2**t)

        theta = theta - lr * m_hat / (np.sqrt(v_hat) + epsilon)
        path.append(theta.copy())

    return path

# ì‹¤í–‰
start = [-1.0, -0.5]
iterations = 200

path_sgd = sgd(start, lr=0.001, iterations=iterations)
path_momentum = momentum(start, lr=0.001, beta=0.9, iterations=iterations)
path_adam = adam(start, lr=0.01, beta1=0.9, beta2=0.999, iterations=iterations)

# ì‹œê°í™”
x = np.linspace(-1.5, 1.5, 100)
y = np.linspace(-1, 2, 100)
X, Y = np.meshgrid(x, y)
Z = rosenbrock(X, Y)

plt.figure(figsize=(12, 10))
plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.6)

path_sgd = np.array(path_sgd)
path_momentum = np.array(path_momentum)
path_adam = np.array(path_adam)

plt.plot(path_sgd[:, 0], path_sgd[:, 1], 'r-', label='SGD', linewidth=2)
plt.plot(path_momentum[:, 0], path_momentum[:, 1], 'g-', label='Momentum', linewidth=2)
plt.plot(path_adam[:, 0], path_adam[:, 1], 'b-', label='Adam', linewidth=2)

plt.scatter([1], [1], c='yellow', s=300, marker='*', zorder=10, label='Optimum (1, 1)')
plt.scatter([start[0]], [start[1]], c='red', s=200, zorder=10, label='Start')

plt.xlabel('x')
plt.ylabel('y')
plt.title('Optimizer Comparison: Rosenbrock Function')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('optimizer_comparison.png', dpi=150)
print("ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹œê°í™” ì €ì¥ ì™„ë£Œ!")

print("\nìµœì¢… ìœ„ì¹˜:")
print(f"  SGD:      {path_sgd[-1]}")
print(f"  Momentum: {path_momentum[-1]}")
print(f"  Adam:     {path_adam[-1]}")
print(f"  Optimum:  [1.0, 1.0]")
```

---

## âœï¸ ì† ê³„ì‚° ì—°ìŠµ

### ì—°ìŠµ 1: ê²½ì‚¬í•˜ê°•ë²• 2ë‹¨ê³„
```
f(x) = xÂ² - 6x + 9
f'(x) = 2x - 6

ì‹œì‘: x = 0, Î± = 0.5

Step 1:
  g = 2(0) - 6 = -6
  x = 0 - 0.5(-6) = 3

Step 2:
  g = 2(3) - 6 = 0
  x = 3 - 0.5(0) = 3

ìˆ˜ë ´! (ìµœì†Ÿê°’: x=3)
```

### ì—°ìŠµ 2: Momentum 1ë‹¨ê³„
```
Î¸ = [1, 2], g = [4, 6]
v = [0, 0], Î² = 0.9, Î± = 0.1

v_new = 0.9[0, 0] + [4, 6] = [4, 6]
Î¸_new = [1, 2] - 0.1[4, 6] = [0.6, 1.4]
```

---

## ğŸ”— LLM ì—°ê²°ì 

### 1. ì‹¤ì œ í•™ìŠµ
```python
# PyTorch
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for batch in dataloader:
    loss = compute_loss(batch)
    loss.backward()  # ê¸°ìš¸ê¸° ê³„ì‚°
    optimizer.step()  # ê²½ì‚¬í•˜ê°•ë²•!
    optimizer.zero_grad()
```

### 2. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
```
ì´ˆê¸°: í° í•™ìŠµë¥  (ë¹ ë¥¸ ìˆ˜ë ´)
í›„ê¸°: ì‘ì€ í•™ìŠµë¥  (ë¯¸ì„¸ ì¡°ì •)

Warmup: ì²˜ìŒì—” ì²œì²œíˆ
Decay: ì ì  ì¤„ì´ê¸°
```

### 3. Gradient Accumulation
```
ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ:
ì—¬ëŸ¬ ë°°ì¹˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ëˆ„ì  í›„ ì—…ë°ì´íŠ¸

ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° ì¦ê°€ íš¨ê³¼
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **ê²½ì‚¬í•˜ê°•ë²•ì˜ ì›ë¦¬ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **í•™ìŠµë¥ ì´ ì™œ ì¤‘ìš”í•œì§€ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **SGD, Momentum, Adamì˜ ì°¨ì´ë¥¼ ì•„ë‚˜ìš”?**

- [ ] **ì‹ ê²½ë§ í•™ìŠµì—ì„œì˜ ì—­í• ì„ ì´í•´í–ˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

1. **ì›ë¦¬**: Î¸ = Î¸ - Î±âˆ‡f(Î¸)
2. **í•™ìŠµë¥ **: ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ì•ˆ ë¨
3. **Mini-batch**: ì‹¤ì œ ë§ì´ ì‚¬ìš©
4. **Adam**: ê°€ì¥ ì•ˆì •ì ì´ê³  íš¨ê³¼ì 

### ë‹¤ìŒ í•™ìŠµ
- **Day 27**: ë¯¸ì ë¶„ ì¢…í•© ë³µìŠµ

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**ê²½ì‚¬í•˜ê°•ë²•ì€ ëª¨ë“  ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ì…ë‹ˆë‹¤!**
