# Day 43: Scaled Dot-Product Attention (2ì‹œê°„) â­

## ğŸ“š í•™ìŠµ ëª©í‘œ
- **Scaled Dot-Product Attentionì˜ ëª¨ë“  ë‹¨ê³„ ì™„ë²½íˆ ì´í•´í•˜ê¸°**
- Q, K, Vì˜ ì˜ë¯¸ íŒŒì•…í•˜ê¸°
- âˆšd_kë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ  ì´í•´í•˜ê¸°
- NumPyë¡œ ì§ì ‘ êµ¬í˜„í•˜ê¸°

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"LLMì˜ ì‹¬ì¥ - Attention ë©”ì»¤ë‹ˆì¦˜"**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. Attentionì˜ ì§ê´€

**ë¬¸ì œ**:
```
ë¬¸ì¥: "The cat sat on the mat"

"it"ì´ ë¬´ì—‡ì„ ê°€ë¦¬í‚¤ëŠ”ì§€ ì•Œë ¤ë©´?
â†’ ë¬¸ë§¥(context)ì„ ë´ì•¼ í•¨!
```

**Attentionì˜ ë‹µ**:
```
ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì—
ì–¼ë§ˆë‚˜ "ì£¼ì˜(attention)"ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•˜ëŠ”ì§€ ê³„ì‚°
```

---

### 2. Query, Key, Value

**ë¹„ìœ : ë„ì„œê´€ ê²€ìƒ‰**
```
Query (ì§ˆë¬¸):
"ë¨¸ì‹ ëŸ¬ë‹ ì±…ì„ ì°¾ìŠµë‹ˆë‹¤"

Key (ì±… ì œëª©):
"ë”¥ëŸ¬ë‹", "ì•Œê³ ë¦¬ì¦˜", "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ"

Value (ì±… ë‚´ìš©):
ì‹¤ì œ ì±…

ê³¼ì •:
1. Queryì™€ ê° Keyì˜ ìœ ì‚¬ë„ ê³„ì‚°
2. ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ
3. Valueë“¤ì˜ ê°€ì¤‘ í•© ë°˜í™˜
```

**LLMì—ì„œ**:
```
Query: "ì´ ë‹¨ì–´ëŠ” ì–´ë–¤ ë§¥ë½ì¸ê°€?"
Key: "ë‚˜ëŠ” ì´ëŸ° ì˜ë¯¸ì•¼"
Value: "ë‚´ ì •ë³´ë¥¼ ì „ë‹¬í•´ì¤„ê²Œ"
```

---

### 3. Scaled Dot-Product Attention ê³µì‹

**ê³µì‹**:
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

**ë‹¨ê³„ë³„**:
```
1. Score ê³„ì‚°: QK^T
   - Qì™€ Kì˜ ë‚´ì  (ìœ ì‚¬ë„)

2. Scaling: / âˆšd_k
   - ì°¨ì›ì´ í´ìˆ˜ë¡ ë‚´ì ì´ ì»¤ì§€ëŠ” ê²ƒ ë°©ì§€
   - d_k: Keyì˜ ì°¨ì›

3. Softmax: softmax(scaled scores)
   - í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
   - í•©ì´ 1

4. ê°€ì¤‘í•©: Ã— V
   - Valueë“¤ì„ ê°€ì¤‘ í‰ê· 
```

---

### 4. ì™œ âˆšd_kë¡œ ë‚˜ëˆ„ë‚˜?

**ì´ìœ **:
```
Q, Kì˜ ì°¨ì› d_kê°€ í¬ë©´:
- ë‚´ì  ê°’ì´ ë§¤ìš° ì»¤ì§
- Softmaxì˜ ê¸°ìš¸ê¸° ì†Œì‹¤
- í•™ìŠµ ë¶ˆì•ˆì •

âˆšd_kë¡œ ë‚˜ëˆ„ë©´:
- ë¶„ì‚°ì´ 1ë¡œ ì •ê·œí™”
- Softmaxê°€ ì•ˆì •ì 
```

**ìˆ˜ì‹ì  ì´í•´**:
```
Q, K ~ N(0, 1)ì¸ d_kì°¨ì› ë²¡í„°

QÂ·Kì˜ ë¶„ì‚° = d_k

QÂ·K / âˆšd_kì˜ ë¶„ì‚° = 1  âœ“
```

---

### 5. ì˜ˆì œ: ì† ê³„ì‚°

**ì„¤ì •**:
```
ë‹¨ì–´ 3ê°œ (seq_len = 3)
ì°¨ì› 4 (d_k = 4)

Q = [1, 0, 1, 0]    # "cat"ì˜ Query
    [0, 1, 0, 1]    # "sat"ì˜ Query
    [1, 1, 0, 0]    # "mat"ì˜ Query

K = [1, 0, 1, 0]    # "cat"ì˜ Key
    [0, 1, 0, 1]    # "sat"ì˜ Key
    [1, 1, 0, 0]    # "mat"ì˜ Key

V = [1, 2, 3, 4]    # "cat"ì˜ Value
    [5, 6, 7, 8]    # "sat"ì˜ Value
    [9, 10, 11, 12] # "mat"ì˜ Value
```

**1ë‹¨ê³„: QK^T**:
```
"cat" Query Â· "cat" Key = 1Â·1 + 0Â·0 + 1Â·1 + 0Â·0 = 2
"cat" Query Â· "sat" Key = 1Â·0 + 0Â·1 + 1Â·0 + 0Â·1 = 0
"cat" Query Â· "mat" Key = 1Â·1 + 0Â·1 + 1Â·0 + 0Â·0 = 1

scores = [2, 0, 1]
```

**2ë‹¨ê³„: Scaling**:
```
âˆšd_k = âˆš4 = 2
scaled = [2/2, 0/2, 1/2] = [1, 0, 0.5]
```

**3ë‹¨ê³„: Softmax**:
```
exp([1, 0, 0.5]) = [2.72, 1, 1.65]
sum = 5.37

softmax = [0.51, 0.19, 0.31]
```

**4ë‹¨ê³„: Ã— V**:
```
output = 0.51[1,2,3,4] + 0.19[5,6,7,8] + 0.31[9,10,11,12]
       â‰ˆ [4.2, 5.3, 6.4, 7.6]
```

---

## ğŸ’» Python ì‹¤ìŠµ

### ì‹¤ìŠµ 1: Scaled Dot-Product Attention êµ¬í˜„
```python
import numpy as np

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Scaled Dot-Product Attention

    Args:
        Q: Query (seq_len_q, d_k)
        K: Key (seq_len_k, d_k)
        V: Value (seq_len_v, d_v)
        mask: Optional mask

    Returns:
        output: (seq_len_q, d_v)
        attention_weights: (seq_len_q, seq_len_k)
    """
    d_k = K.shape[-1]

    # 1. Score ê³„ì‚°: QK^T
    scores = Q @ K.T  # (seq_len_q, seq_len_k)

    # 2. Scaling
    scaled_scores = scores / np.sqrt(d_k)

    # 3. Mask (ì˜µì…˜)
    if mask is not None:
        scaled_scores += (mask * -1e9)

    # 4. Softmax
    attention_weights = np.exp(scaled_scores)
    attention_weights /= attention_weights.sum(axis=-1, keepdims=True)

    # 5. ê°€ì¤‘í•©: Ã— V
    output = attention_weights @ V  # (seq_len_q, d_v)

    return output, attention_weights


# ì˜ˆì œ
print("=== Scaled Dot-Product Attention ===\n")

# ì„¤ì •
seq_len = 3
d_k = 4
d_v = 4

Q = np.array([[1, 0, 1, 0],
              [0, 1, 0, 1],
              [1, 1, 0, 0]], dtype=float)

K = np.array([[1, 0, 1, 0],
              [0, 1, 0, 1],
              [1, 1, 0, 0]], dtype=float)

V = np.array([[1, 2, 3, 4],
              [5, 6, 7, 8],
              [9, 10, 11, 12]], dtype=float)

print(f"Q shape: {Q.shape}")
print(f"K shape: {K.shape}")
print(f"V shape: {V.shape}\n")

# Attention ê³„ì‚°
output, attn_weights = scaled_dot_product_attention(Q, K, V)

print("Attention Weights (ê° í–‰ì´ í•˜ë‚˜ì˜ Query):")
print(attn_weights)
print()

print("Output (ê° í–‰ì´ í•˜ë‚˜ì˜ ì¶œë ¥):")
print(output)
print()

print("í•´ì„:")
print("- attn_weights[0]: ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ ëª¨ë“  ë‹¨ì–´ì— ì£¼ëŠ” attention")
print("- í•©ì´ 1.0 (í™•ë¥  ë¶„í¬)")
print("- output[0]: ì²« ë²ˆì§¸ ë‹¨ì–´ì˜ ë§¥ë½í™”ëœ í‘œí˜„")
```

### ì‹¤ìŠµ 2: Self-Attention ì‹œê°í™”
```python
import numpy as np
import matplotlib.pyplot as plt

# ë¬¸ì¥: "The cat sat on the mat"
words = ["The", "cat", "sat", "on", "the", "mat"]
n = len(words)

# ì„ì˜ì˜ Q, K, V (ì‹¤ì œë¡œëŠ” í•™ìŠµë¨)
np.random.seed(42)
d_model = 8

Q = np.random.randn(n, d_model)
K = np.random.randn(n, d_model)
V = np.random.randn(n, d_model)

# Attention
output, attn_weights = scaled_dot_product_attention(Q, K, V)

# ì‹œê°í™”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Attention íˆíŠ¸ë§µ
im = ax1.imshow(attn_weights, cmap='Blues', aspect='auto')
ax1.set_xticks(range(n))
ax1.set_yticks(range(n))
ax1.set_xticklabels(words)
ax1.set_yticklabels(words)
ax1.set_xlabel('Key (attending to)')
ax1.set_ylabel('Query (from)')
ax1.set_title('Attention Weights')

# ê°’ í‘œì‹œ
for i in range(n):
    for j in range(n):
        text = ax1.text(j, i, f'{attn_weights[i, j]:.2f}',
                       ha="center", va="center", color="black", fontsize=10)

plt.colorbar(im, ax=ax1)

# íŠ¹ì • ë‹¨ì–´ ("cat")ì˜ attention ë¶„í¬
cat_idx = 1
ax2.bar(words, attn_weights[cat_idx])
ax2.set_xlabel('ë‹¨ì–´')
ax2.set_ylabel('Attention Weight')
ax2.set_title(f'"{words[cat_idx]}"ì´ ê° ë‹¨ì–´ì— ì£¼ëŠ” Attention')
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('self_attention_visualization.png', dpi=150)
print("\nAttention ì‹œê°í™” ì €ì¥ ì™„ë£Œ!")
```

### ì‹¤ìŠµ 3: Masking (Causal Attention)
```python
import numpy as np

def create_causal_mask(seq_len):
    """ì¸ê³¼ì  ë§ˆìŠ¤í¬ ìƒì„± (ë¯¸ë˜ ë‹¨ì–´ ê°€ë¦¬ê¸°)"""
    mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    return mask  # 1 = mask, 0 = visible

# ì˜ˆ: GPTì˜ Causal Self-Attention
seq_len = 5
words = ["I", "love", "machine", "learning", "."]

# ë§ˆìŠ¤í¬
mask = create_causal_mask(seq_len)

print("=== Causal (Masked) Attention ===\n")
print("ë§ˆìŠ¤í¬ (1 = ê°€ë ¤ì§, 0 = ë³´ì„):")
print(mask)
print()

# ì„ì˜ì˜ Q, K, V
np.random.seed(42)
Q = np.random.randn(seq_len, 4)
K = np.random.randn(seq_len, 4)
V = np.random.randn(seq_len, 4)

# Masked Attention
output, attn_weights = scaled_dot_product_attention(Q, K, V, mask=mask)

print("Masked Attention Weights:")
for i, word in enumerate(words):
    visible_words = words[:i+1]
    print(f"{word:>10}: {attn_weights[i, :i+1]}")
    print(f"           (ë³¼ ìˆ˜ ìˆëŠ” ë‹¨ì–´: {visible_words})\n")
```

### ì‹¤ìŠµ 4: Scalingì˜ ì¤‘ìš”ì„±
```python
import numpy as np

def attention_without_scaling(Q, K, V):
    """Scaling ì—†ëŠ” Attention"""
    scores = Q @ K.T
    attn_weights = np.exp(scores) / np.exp(scores).sum(axis=-1, keepdims=True)
    output = attn_weights @ V
    return output, attn_weights

# ë‹¤ì–‘í•œ ì°¨ì›ì—ì„œ ë¹„êµ
dimensions = [4, 16, 64, 256]

print("=== Scalingì˜ íš¨ê³¼ ===\n")

for d_k in dimensions:
    Q = np.random.randn(3, d_k)
    K = np.random.randn(3, d_k)
    V = np.random.randn(3, d_k)

    # Scaling ì—†ìŒ
    _, attn_no_scale = attention_without_scaling(Q, K, V)

    # Scaling ìˆìŒ
    _, attn_scaled = scaled_dot_product_attention(Q, K, V)

    print(f"d_k = {d_k}:")
    print(f"  No scaling: max={attn_no_scale[0].max():.4f}, "
          f"min={attn_no_scale[0].min():.4f}")
    print(f"  Scaled:     max={attn_scaled[0].max():.4f}, "
          f"min={attn_scaled[0].min():.4f}")
    print()

print("â†’ ì°¨ì›ì´ í´ìˆ˜ë¡ scalingì´ ì¤‘ìš”í•¨!")
```

---

## âœï¸ ì† ê³„ì‚° ì—°ìŠµ

### ì—°ìŠµ: ê°„ë‹¨í•œ Attention
```
Q = [1, 1], K = [1, 0], V = [2, 3]
        [0, 1]      [0, 1]      [4, 5]

d_k = 2, âˆšd_k = âˆš2 â‰ˆ 1.41

1. QK^T:
   [1,1]Â·[1,0] = 1,  [1,1]Â·[0,1] = 1
   [0,1]Â·[1,0] = 0,  [0,1]Â·[0,1] = 1

   scores = [[1, 1],
             [0, 1]]

2. Scaling:
   scaled = [[0.71, 0.71],
             [0, 0.71]]

3. Softmax (ì²« í–‰):
   exp([0.71, 0.71]) = [2.03, 2.03]
   softmax = [0.5, 0.5]

4. ì²« í–‰ ì¶œë ¥:
   0.5[2,3] + 0.5[4,5] = [3, 4]
```

---

## ğŸ”— LLM ì—°ê²°ì 

### 1. GPTì˜ Causal Self-Attention
```python
# PyTorch (ê°œë…ì )
scores = Q @ K.T / sqrt(d_k)
mask = causal_mask  # ë¯¸ë˜ ê°€ë¦¬ê¸°
scores = scores.masked_fill(mask == 1, -1e9)
attn = softmax(scores, dim=-1)
output = attn @ V
```

### 2. BERTì˜ Bidirectional Self-Attention
```python
# ë§ˆìŠ¤í¬ ì—†ìŒ (ì–‘ë°©í–¥)
scores = Q @ K.T / sqrt(d_k)
attn = softmax(scores, dim=-1)
output = attn @ V
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **Q, K, Vì˜ ì—­í• ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **âˆšd_kë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ë¥¼ ì´í•´í–ˆë‚˜ìš”?**

- [ ] **Attentionì„ ì†ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‚˜ìš”?**

- [ ] **Causal Attentionê³¼ ì¼ë°˜ Attentionì˜ ì°¨ì´ë¥¼ ì•„ë‚˜ìš”?**

- [ ] **NumPyë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

1. **Attention**: ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜
2. **ê³µì‹**: softmax(QK^T / âˆšd_k) V
3. **Q**: ì§ˆë¬¸, **K**: í‚¤, **V**: ê°’
4. **Scaling**: ìˆ˜ì¹˜ ì•ˆì •ì„±
5. **Self-Attention**: Q=K=V (ê°™ì€ ë¬¸ì¥ ë‚´)

### ë‹¤ìŒ í•™ìŠµ
- **Day 44**: Multi-Head Attention
  - ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì—!

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**Attentionì€ Transformerì˜ í•µì‹¬ì…ë‹ˆë‹¤!**
