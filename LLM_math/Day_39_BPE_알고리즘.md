# Day 39: BPE ì•Œê³ ë¦¬ì¦˜ (Byte Pair Encoding) (2ì‹œê°„)

## ğŸ“š í•™ìŠµ ëª©í‘œ
- **BPE ì•Œê³ ë¦¬ì¦˜ì˜ ì›ë¦¬ ì™„ë²½íˆ ì´í•´í•˜ê¸°** â­
- ì •ë³´ ì´ë“ ìµœëŒ€í™”ì™€ì˜ ì—°ê²° íŒŒì•…í•˜ê¸°
- ê°„ë‹¨í•œ BPE êµ¬í˜„í•˜ê¸°

---

## ğŸ¯ ê°•ì˜ ì£¼ì œ
**"í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‚˜ëˆ„ê¸°"**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. BPEë€?

**ë¬¸ì œ**:
```
ë‹¨ì–´ ê¸°ë°˜: ì–´íœ˜ê°€ ë„ˆë¬´ í¼
ë¬¸ì ê¸°ë°˜: ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ê¹€

â†’ ì„œë¸Œì›Œë“œ(subword) í•„ìš”!
```

**BPE ì•„ì´ë””ì–´**:
```
ìì£¼ ë‚˜ì˜¤ëŠ” ë°”ì´íŠ¸(ë¬¸ì) ìŒì„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë³‘í•©
```

---

### 2. BPE ì•Œê³ ë¦¬ì¦˜

**ë‹¨ê³„**:
```
1. í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìë¡œ ë¶„ë¦¬
2. ê°€ì¥ ë¹ˆë²ˆí•œ ë°”ì´íŠ¸ ìŒ ì°¾ê¸°
3. ê·¸ ìŒì„ ìƒˆ í† í°ìœ¼ë¡œ ë³‘í•©
4. ì–´íœ˜ í¬ê¸°ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ
```

**ì˜ˆì‹œ**:
```
ì›ë³¸: "low low low lowest"

ì´ˆê¸°: l o w   l o w   l o w   l o w e s t

Step 1: 'l o'ê°€ ê°€ì¥ ë¹ˆë²ˆ â†’ 'lo'
        lo w   lo w   lo w   lo w e s t

Step 2: 'lo w'ê°€ ê°€ì¥ ë¹ˆë²ˆ â†’ 'low'
        low   low   low   low e s t

Step 3: 'low'ê°€ ê°€ì¥ ë¹ˆë²ˆ (ë” ì´ìƒ ë³‘í•© ì•ˆ í•¨)

ìµœì¢… ì–´íœ˜: {l, o, w, e, s, t, lo, low, lowest}
```

---

### 3. ì •ë³´ ì´ë“ê³¼ì˜ ì—°ê²°

**ì •ë³´ ì´ë“ ê´€ì **:
```
ìì£¼ ë‚˜ì˜¤ëŠ” ìŒ ë³‘í•© = ì••ì¶•ë¥  í–¥ìƒ
= ì—”íŠ¸ë¡œí”¼ ê°ì†Œ
= ì •ë³´ ì´ë“ ìµœëŒ€í™”!
```

---

## ğŸ’» Python ì‹¤ìŠµ

### ì‹¤ìŠµ 1: BPE êµ¬í˜„
```python
import re
from collections import Counter

class SimpleBPE:
    """ê°„ë‹¨í•œ BPE í† í¬ë‚˜ì´ì €"""

    def __init__(self, num_merges=10):
        self.num_merges = num_merges
        self.bpe_codes = []

    def get_stats(self, vocab):
        """ë°”ì´íŠ¸ ìŒ ë¹ˆë„ ê³„ì‚°"""
        pairs = Counter()
        for word, freq in vocab.items():
            symbols = word.split()
            for i in range(len(symbols)-1):
                pairs[symbols[i], symbols[i+1]] += freq
        return pairs

    def merge_vocab(self, pair, vocab):
        """ì–´íœ˜ì—ì„œ ìŒ ë³‘í•©"""
        new_vocab = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)

        for word in vocab:
            new_word = word.replace(bigram, replacement)
            new_vocab[new_word] = vocab[word]
        return new_vocab

    def train(self, corpus):
        """BPE í•™ìŠµ"""
        # ë‹¨ì–´ë³„ ë¹ˆë„
        words = corpus.lower().split()
        vocab = Counter(words)

        # ë¬¸ìë¡œ ë¶„ë¦¬
        vocab = {' '.join(word): freq for word, freq in vocab.items()}

        print("=== BPE í•™ìŠµ ===\n")
        print(f"ì´ˆê¸° ì–´íœ˜: {len(vocab)}ê°œ ë‹¨ì–´\n")

        for i in range(self.num_merges):
            pairs = self.get_stats(vocab)

            if not pairs:
                break

            best = max(pairs, key=pairs.get)
            vocab = self.merge_vocab(best, vocab)
            self.bpe_codes.append(best)

            print(f"Step {i+1}: '{best[0]}' + '{best[1]}' â†’ '{''.join(best)}' "
                  f"(ë¹ˆë„: {pairs[best]})")

        print(f"\ní•™ìŠµ ì™„ë£Œ! {len(self.bpe_codes)}ê°œ ë³‘í•©\n")

        return vocab

# ì‚¬ìš© ì˜ˆ
corpus = "low low low low lowest lower"
bpe = SimpleBPE(num_merges=5)
final_vocab = bpe.train(corpus)

print("ìµœì¢… ì–´íœ˜:")
for word, freq in sorted(final_vocab.items(), key=lambda x: -x[1]):
    print(f"  '{word}': {freq}")
```

### ì‹¤ìŠµ 2: ì••ì¶•ë¥  ê³„ì‚°
```python
import numpy as np

def calculate_compression_ratio(original, encoded):
    """ì••ì¶•ë¥  ê³„ì‚°"""
    original_size = len(original.replace(' ', ''))
    encoded_size = len(encoded.split())
    ratio = original_size / encoded_size
    return ratio

# BPE ì „í›„ ë¹„êµ
original = "l o w l o w l o w l o w e s t"
encoded = "low low low lowest"

ratio = calculate_compression_ratio(original, encoded)

print("\n=== ì••ì¶•ë¥  ===")
print(f"ì›ë³¸: '{original}' ({len(original.split())}ê°œ í† í°)")
print(f"BPE: '{encoded}' ({len(encoded.split())}ê°œ í† í°)")
print(f"ì••ì¶•ë¥ : {ratio:.2f}x")
```

### ì‹¤ìŠµ 3: ì—”íŠ¸ë¡œí”¼ ë¹„êµ
```python
from collections import Counter

def calculate_entropy(tokens):
    """í† í°ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
    freq = Counter(tokens)
    total = len(tokens)
    probs = np.array([freq[t]/total for t in freq])
    return -np.sum(probs * np.log2(probs))

# ì›ë³¸ vs BPE
original_tokens = "l o w l o w l o w l o w e s t".split()
bpe_tokens = "low low low lowest".split()

h_original = calculate_entropy(original_tokens)
h_bpe = calculate_entropy(bpe_tokens)

print("\n=== ì—”íŠ¸ë¡œí”¼ ë¹„êµ ===")
print(f"ì›ë³¸ ì—”íŠ¸ë¡œí”¼: {h_original:.4f} bits")
print(f"BPE ì—”íŠ¸ë¡œí”¼: {h_bpe:.4f} bits")
print(f"ê°ì†Œ: {h_original - h_bpe:.4f} bits")
print("â†’ BPEê°€ ë” íš¨ìœ¨ì !")
```

---

## âœï¸ ì† ê³„ì‚° ì—°ìŠµ

### BPE í•œ ìŠ¤í…
```
í…ìŠ¤íŠ¸: "aa aa bb"
ì´ˆê¸°: a a   a a   b b

ë¹ˆë„ ê³„ì‚°:
- (a, a): 2ë²ˆ
- (b, b): 1ë²ˆ

ë³‘í•©: (a, a) â†’ aa
ê²°ê³¼: aa   aa   b b
```

---

## ğŸ”— LLM ì—°ê²°ì 

### 1. GPT/BERTì˜ í† í¬ë‚˜ì´ì €
```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
text = "Tokenization is important!"
tokens = tokenizer.encode(text)

print(f"í…ìŠ¤íŠ¸: {text}")
print(f"í† í° ID: {tokens}")
print(f"í† í°: {tokenizer.convert_ids_to_tokens(tokens)}")

# BPE ê¸°ë°˜!
```

### 2. ë‹¤êµ­ì–´ ì§€ì›
```
BPEëŠ” ì–¸ì–´ ë…ë¦½ì 
- ë°”ì´íŠ¸ ê¸°ë°˜
- ëª¨ë“  ì–¸ì–´ì— ì ìš© ê°€ëŠ¥
- ë¯¸ë“±ë¡ ë‹¨ì–´(UNK) ìµœì†Œí™”
```

---

## âœ… ì²´í¬í¬ì¸íŠ¸

- [ ] **BPE ì•Œê³ ë¦¬ì¦˜ì˜ ë‹¨ê³„ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?**
- [ ] **ì •ë³´ ì´ë“ê³¼ì˜ ì—°ê²°ì„ ì´í•´í–ˆë‚˜ìš”?**
- [ ] **BPEì˜ ì¥ì ì„ ì•„ë‚˜ìš”?**
- [ ] **ì‹¤ì œ LLMì—ì„œì˜ í™œìš©ì„ ì´í•´í–ˆë‚˜ìš”?**

---

## ğŸ“ í•µì‹¬ ìš”ì•½

1. **BPE**: ë¹ˆë²ˆí•œ ë°”ì´íŠ¸ ìŒ ë³‘í•©
2. **ëª©í‘œ**: ì••ì¶•, ì—”íŠ¸ë¡œí”¼ ê°ì†Œ
3. **ì¥ì **: íš¨ìœ¨ì , ì–¸ì–´ ë…ë¦½ì 
4. **LLM**: ê±°ì˜ ëª¨ë“  ëª¨ë¸ì´ BPE ì‚¬ìš©

### ë‹¤ìŒ í•™ìŠµ
- **Day 40**: WordPieceì™€ SentencePiece

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** ğŸ‰

**BPEëŠ” ëª¨ë“  LLM í† í¬ë‚˜ì´ì €ì˜ ê¸°ì´ˆì…ë‹ˆë‹¤!**
